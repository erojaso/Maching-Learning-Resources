{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithms Mini-Course\n",
    "by Jason Brownlee on August 12, 2019.[Here](https://machinelearningmastery.com/machine-learning-algorithms-mini-course/) in [Machine Learning Algorithms](https://machinelearningmastery.com/category/machine-learning-algorithms/)\n",
    "\n",
    "Machine learning algorithms are a very large part of machine learning.\n",
    "\n",
    "## Overview\n",
    "\n",
    "### Algorithm Foundations\n",
    "- Lesson 1: How To Talk About Data in Machine Learning\n",
    "- Lesson 2: Principle That Underpins All Algorithms\n",
    "- Lesson 3: Parametric and Nonparametric Algorithms\n",
    "- Lesson 4: Bias, Variance and the Trade-off\n",
    "\n",
    "### Linear Algorithms\n",
    "- Lesson 5: Linear Regression\n",
    "- Lesson 6: Logistic Regression\n",
    "- Lesson 7: Linear Discriminant Analysis\n",
    "\n",
    "### Nonlinear Algorithms\n",
    "- Lesson 8: Classification and Regression Trees\n",
    "- Lesson 9: Naive Bayes\n",
    "- Lesson 10: k-Nearest Neighbors\n",
    "- Lesson 11: Learning Vector Quantization\n",
    "- Lesson 12: Support Vector Machines\n",
    "\n",
    "### Ensemble Algorithms\n",
    "- Lesson 13: Bagging and Random Forest\n",
    "- Lesson 14: Boosting and AdaBoost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Foundations\n",
    "\n",
    "#### Lesson 1: How To Talk About Data in Machine Learning\n",
    "Data plays a big part in machine learning.\n",
    "\n",
    "The statistical perspective of `machine learning frames data in the context of a hypothetical function (f) that the machine learning algorithm aims to learn`. Given some input variables (Input)  the function answer the question as to what is the predicted output variable (Output).\n",
    "\n",
    "Output = f(Input)\n",
    "\n",
    "The inputs and outputs can be referred to as __variables__ or __vectors__.\n",
    "\n",
    "#### Lesson 2: The Principle That Underpins (sustenta) All Algorithms\n",
    "Machine learning algorithms are `described as learning a target function (f) that best maps input variables (X) to an output variable (Y)`.\n",
    "\n",
    "The most common type of machine learning is to learn the mapping $Y = f(X)$ to make predictions of Y for new X. This is called __predictive modeling__ or __predictive analytics__ and our goal is to make the most accurate predictions possible.\n",
    "\n",
    "#### Lesson 3: Parametric and Nonparametric Algorithms\n",
    "Assumptions can greatly simplify the learning process, but can also limit what can be learned. __Algorithms that simplify the function to a known form are called parametric machine learning algorithms__.\n",
    "\n",
    "The algorithms involve two steps:\n",
    "\n",
    "- Select a form for the function.\n",
    "- Learn the coefficients for the function from the training data.\n",
    "\n",
    "Examples of parametric machine learning algorithms\n",
    "- Linear Regression.\n",
    "- Logistic Regression.\n",
    "\n",
    "__Algorithms that do not make strong assumptions about the form of the mapping function are called nonparametric machine learning algorithms__. By not making assumptions, they are free to learn any functional form from the training data.\n",
    "\n",
    "`Non-parametric methods are often more flexible, achieve better accuracy but require a lot more data and training time`.\n",
    "\n",
    "Examples of nonparametric algorithms \n",
    "- Support Vector Machines.\n",
    "- Neural Networks.\n",
    "- Decision Trees.\n",
    "\n",
    "#### Lesson 4: Bias, Variance and the Trade-off\n",
    "Machine learning algorithms can best be understood through the lens of the bias-variance trade-off (__equilibrio de sesgo-varianza__).\n",
    "\n",
    "- Bias are the simplifying assumptions made by a model to make the target function easier to learn.\n",
    "    - `Low bias`: Decision Trees.\n",
    "    - `High-bias`: Linear Regression.\n",
    "\n",
    "- Variance is the amount that the estimate of the target function will change if different training data was used. The target function is estimated from the training data by a machine learning algorithm, so we should expect the algorithm to __have some variance, not zero variance__.\n",
    "    - `High-variance`: k-Nearest Neighbors\n",
    "    - `Low variance`: Linear Discriminant Analysis\n",
    "\n",
    "The goal of any predictive modeling machine learning algorithm is to achieve low bias and low variance.\n",
    "\n",
    "The parameterization of machine learning algorithms is often a battle to balance out bias and variance.\n",
    "- Increasing the bias will decrease the variance.\n",
    "- Increasing the variance will decrease the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Algorithms\n",
    "#### Lesson 5: Linear Regression\n",
    "__Predictive modeling__ is primarily concerned with minimizing the error of a model or making the most accurate predictions possible, at the expense of explainability. \n",
    "\n",
    ">The representation of linear regression is a `equation that describes a line that best fits the relationship between the input variables` (__x__) and the `output variables` (__y__), by `finding specific weightings for the input variables called coefficients` (__B__).\n",
    "\n",
    "- $y = B0 + B1 * x$\n",
    "\n",
    "Linear regression has been around for more than 200 years and has been extensively studied. Some __good rules of thumb when using this technique are to remove variables that are very similar (correlated) and to remove noise from your data__, if possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lesson 6: Logistic Regression\n",
    "Logistic regression is another technique borrowed by machine learning from the field of statistics. It is the go-to method for binary classification problems (problems with two class values).\n",
    "\n",
    "Logistic regression is like linear regression in that the goal is to find the values for the coefficients that weight each input variable.\n",
    "\n",
    "Unlike linear regression, the prediction for the output is transformed using a __non-linear function called the logistic function__.\n",
    "\n",
    "The logistic function looks like a big S and will transform any value into the range 0 to 1. This is useful because we can apply a rule to the output of the logistic function to snap values to 0 and 1 (e.g. IF less than 0.5 then output 1) and predict a class value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lesson 7: Linear Discriminant Analysis\n",
    "Logistic regression is a classification algorithm traditionally limited to only two-class classification problems. If you have `more than two classes then the Linear Discriminant Analysis algorithm is the preferred` linear classification technique.\n",
    "\n",
    "The representation of LDA is pretty straight forward. It consists of statistical properties of your data, calculated for each class. For a single input variable this includes:\n",
    "\n",
    "1. The `mean value` for each class.\n",
    "2. The `variance` calculated across all classes.\n",
    "\n",
    "Predictions are `made by calculating a discriminate value for each class` and making a prediction for the class with the `largest value`.\n",
    "\n",
    "The technique __assumes that the data has a Gaussian distribution (bell curve)__, so it is a good idea to __remove outliers__ from your data before hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear Algorithms\n",
    "#### Lesson 8: Classification and Regression Trees\n",
    "The representation for the decision tree model is a binary tree. This is your binary tree from algorithms and data structures, nothing too fancy. Each node represents a single input variable (x) and a split point on that variable (assuming the variable is numeric).\n",
    "\n",
    "The leaf nodes of the tree contain an output variable (y) which is used to make a prediction.  Predictions are made by walking the splits of the tree until arriving at a leaf node and output the class value at that leaf node.\n",
    "\n",
    "Trees are fast to learn and very fast for making predictions. They are also often accurate for a broad range of problems and do not require any special preparation for your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lesson 9: Naive Bayes\n",
    "The model is comprised of two types of probabilities that can be calculated directly from your training data:\n",
    "\n",
    "- The probability of each class.\n",
    "- The conditional probability for each class given each x value.\n",
    "\n",
    "Once calculated, the probability model can be used to make predictions for new data using Bayes Theorem.\n",
    "\n",
    "When your data is real-valued it is common to __assume a Gaussian distribution (bell curve)__ so that you can easily estimate these probabilities.\n",
    "\n",
    "__Naive Bayes is called naive because it assumes that each input variable is independent__. This is a strong assumption and unrealistic for real data, nevertheless, the technique is very effective on a large range of complex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lesson 10: k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lesson 11: Learning Vector Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lesson 12: Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
