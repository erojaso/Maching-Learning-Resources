{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending Ensemble Machine Learning With Python\n",
    "by Jason Brownlee on November 30, 2020.[Here](https://machinelearningmastery.com/blending-ensemble-machine-learning-with-python/) in [Ensemble Learning](https://machinelearningmastery.com/category/ensemble-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Blending -`mezcla`- is an ensemble machine learning algorithm__\n",
    "\n",
    "It is a colloquial name for `stacked generalization` or `stacking ensemble` where instead of fitting the meta-model on __out-of-fold predictions__ made by the base model, it is fit on predictions made on a __holdout dataset__.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "- Blending ensembles are a type of stacking where the meta-model is fit using predictions on a holdout validation -`validación de retención`- dataset instead of out-of-fold predictions -`predicciones fuera del pliegue`-.\n",
    "- How to develop a blending ensemble, including functions for training the model and making predictions on new data.\n",
    "- How to evaluate blending ensembles for classification and regression predictive modeling problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Overview\n",
    "This tutorial is divided into four parts; they are:\n",
    "\n",
    "1. Blending Ensemble\n",
    "2. Develop a Blending Ensemble\n",
    "3. Blending Ensemble for Classification\n",
    "    - 3.1 Blending Ensemble for Classification - soft voting\n",
    "    - 3.2 Blending Ensemble for Classification - Prediction\n",
    "4. Blending Ensemble for Regression\n",
    "    - 4.1 Blending Ensemble for Regression - Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Blending Ensemble\n",
    "Blending is an ensemble machine learning technique that uses a machine learning model to learn how to best combine the predictions from multiple contributing ensemble member models.\n",
    "\n",
    "As such, blending is the same as `stacked generalization`, known as stacking, broadly conceived. Often, blending and stacking are used interchangeably in the same paper or model description.\n",
    "\n",
    "The __architecture of a stacking model__ involves two or more base models, often `referred to as level-0 models`, and a meta-model that combines the predictions of the base models, `referred to as a level-1 model`. The meta-model is trained on the predictions made by base models on out-of-sample data.\n",
    "\n",
    "- __Level-0 Models__ (Base-Models): Models fit on the training data and whose predictions are compiled.\n",
    "- __Level-1 Model__ (Meta-Model): Model that learns how to best combine the predictions of the base models.\n",
    "\n",
    "Blending may suggest developing a stacking ensemble where the base-models are machine learning models of any type, and the meta-model is a linear model that “blends” the predictions of the base-models.\n",
    "\n",
    "For example, a linear regression model when predicting a numerical value or a logistic regression model when predicting a class label would calculate a weighted sum of the predictions made by base models and would be considered a blending of predictions.\n",
    "\n",
    "- __Blending__: Stacking-type ensemble where the meta-model is trained on predictions made on a holdout dataset.\n",
    "- __Stacking__: Stacking-type ensemble where the meta-model is trained on out-of-fold predictions made during k-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Develop a Blending Ensemble \n",
    "__The scikit-learn library does not natively support blending at the time of writing.__\n",
    "\n",
    "__First__, we need to create a number of base models. These can be any models we like for a regression or classification problem. We can define a function get_models() that returns a list of models where each model is defined as a tuple with a name and the configured classifier or regression object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# get a list of base models\n",
    "def get_models():\n",
    "    models = list()\n",
    "    \n",
    "    models.append(('lr', LogisticRegression()))\n",
    "    models.append(('knn', KNeighborsClassifier()))\n",
    "    models.append(('cart', DecisionTreeClassifier()))\n",
    "    models.append(('svm', SVC(probability=True)))\n",
    "    models.append(('bayes', GaussianNB()))\n",
    "    \n",
    "    return models\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Second__, we need to fit the blending model.\n",
    "\n",
    "Recall that the base models are fit on a training dataset. The meta-model is fit on the predictions made by each base model on a holdout dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "...\n",
    "# fit all models on the training set and predict on hold out set\n",
    "meta_X = list()\n",
    "for name, model in models:\n",
    "\t\n",
    "    # fit in training set\n",
    "\tmodel.fit(X_train, y_train)\n",
    "\t\n",
    "    # predict on hold out set\n",
    "\tyhat = model.predict(X_val)\n",
    "\t\n",
    "    # reshape predictions into a matrix with one column\n",
    "\tyhat = yhat.reshape(len(yhat), 1)\n",
    "\t\n",
    "    # store predictions as input for blending\n",
    "\tmeta_X.append(yhat)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have “meta_X” that represents the input data that can be used to train the meta-model. `Each column or feature represents the output of one base model`.\n",
    "\n",
    "__Third__, Each row represents the one sample from the holdout dataset. We can use the hstack() function to ensure this dataset is a 2D numpy array as expected by a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "...\n",
    "# create 2d array from predictions, each set is an input feature\n",
    "meta_X = hstack(meta_X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train our meta-model. This can be any machine learning model we like, such as logistic regression for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "...\n",
    "# define blending model\n",
    "blender = LogisticRegression()\n",
    "\n",
    "# fit on predictions from base models\n",
    "blender.fit(meta_X, y_val)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tie all of this together into a function named fit_ensemble() that trains the blending model using a training dataset and holdout validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# fit the blending ensemble\n",
    "def fit_ensemble(models, X_train, X_val, y_train, y_val):\n",
    "\t\n",
    "    # fit all models on the training set and predict on hold out set\n",
    "\tmeta_X = list()\n",
    "\tfor name, model in models:\n",
    "\t\t\n",
    "        # fit in training set\n",
    "\t\tmodel.fit(X_train, y_train)\n",
    "\t\t\n",
    "        # predict on hold out set\n",
    "\t\tyhat = model.predict(X_val)\n",
    "\t\t\n",
    "        # reshape predictions into a matrix with one column\n",
    "\t\tyhat = yhat.reshape(len(yhat), 1)\n",
    "\t\t\n",
    "        # store predictions as input for blending\n",
    "\t\tmeta_X.append(yhat)\n",
    "\t\n",
    "    # create 2d array from predictions, each set is an input feature\n",
    "\tmeta_X = hstack(meta_X)\n",
    "\t\n",
    "    # define blending model\n",
    "\tblender = LogisticRegression()\n",
    "\t\n",
    "    # fit on predictions from base models\n",
    "\tblender.fit(meta_X, y_val)\n",
    "    \n",
    "\treturn blender\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Fourth__ The next step is to use the blending ensemble to make predictions on new data.\n",
    "\n",
    "This is a two-step process. The `first step` is to use each base model to make a prediction. These predictions are then gathered together and `second step`, used as input to the blending model to make the final prediction.\n",
    "\n",
    "We can use the same looping structure as we did when training the model. That is, we can collect the predictions from each base model into a training dataset, stack the predictions together, and call predict() on the blender model with this meta-level dataset.\n",
    "\n",
    "The predict_ensemble() function below implements this. Given the list of fit base models, the fit blender ensemble, and a dataset (such as a test dataset or new data), it will return a set of predictions for the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# make a prediction with the blending ensemble\n",
    "def predict_ensemble(models, blender, X_test):\n",
    "\t\n",
    "    # make predictions with base models\n",
    "\tmeta_X = list()\n",
    "\tfor name, model in models:\n",
    "\t\t\n",
    "        # predict with base model\n",
    "\t\tyhat = model.predict(X_test)\n",
    "\t\t\n",
    "        # reshape predictions into a matrix with one column\n",
    "\t\tyhat = yhat.reshape(len(yhat), 1)\n",
    "\t\t\n",
    "        # store prediction\n",
    "\t\tmeta_X.append(yhat)\n",
    "\t\n",
    "    # create 2d array from predictions, each set is an input feature\n",
    "\tmeta_X = hstack(meta_X)\n",
    "\t\n",
    "    # predict\n",
    "\treturn blender.predict(meta_X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Blending Ensemble for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blending ensemble for classification using hard voting\n",
    "from numpy import hstack\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset\n",
    "def get_dataset():\n",
    "    X, y = make_classification(n_samples=10000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of base models\n",
    "def get_models():\n",
    "    models = list()\n",
    "    models.append(('lr', LogisticRegression()))\n",
    "    models.append(('knn', KNeighborsClassifier()))\n",
    "    models.append(('cart', DecisionTreeClassifier()))\n",
    "    models.append(('svm', SVC()))\n",
    "    models.append(('bayes', GaussianNB()))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the blending ensemble\n",
    "def fit_ensemble(models, X_train, X_val, y_train, y_val):\n",
    "    \n",
    "    # fit all models on the training set and predict on hold out set\n",
    "    meta_X = list()\n",
    "    for name, model in models:\n",
    "        \n",
    "        # fit in training set\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # predict on hold out set\n",
    "        yhat = model.predict(X_val)\n",
    "        \n",
    "        # reshape predictions into a matrix with one column\n",
    "        yhat = yhat.reshape(len(yhat), 1)\n",
    "        \n",
    "        # store predictions as input for blending\n",
    "        meta_X.append(yhat)\n",
    "    \n",
    "    # create 2d array from predictions, each set is an input feature\n",
    "    meta_X = hstack(meta_X)\n",
    "    \n",
    "    # define blending model\n",
    "    blender = LogisticRegression()\n",
    "    \n",
    "    # fit on predictions from base models\n",
    "    blender.fit(meta_X, y_val)\n",
    "    \n",
    "    return blender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction with the blending ensemble\n",
    "def predict_ensemble(models, blender, X_test):\n",
    "    \n",
    "    # make predictions with base models\n",
    "    meta_X = list()\n",
    "    for name, model in models:\n",
    "        \n",
    "        # predict with base model\n",
    "        yhat = model.predict(X_test)\n",
    "        \n",
    "        # reshape predictions into a matrix with one column\n",
    "        yhat = yhat.reshape(len(yhat), 1)\n",
    "        \n",
    "        # store prediction\n",
    "        meta_X.append(yhat)\n",
    "    \n",
    "    # create 2d array from predictions, each set is an input feature\n",
    "    meta_X = hstack(meta_X)\n",
    "\n",
    "    # predict\n",
    "    return blender.predict(meta_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 20) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to split the dataset up, `first` into train and test sets, and `then the training` set into a subset used to train the base models and a subset used to train the meta-model.\n",
    "\n",
    "In this case, we will use a 50-50 split for the train and test sets, then use a 67-33 split for train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (3350, 20), Val: (1650, 20), Test: (5000, 20)\n"
     ]
    }
   ],
   "source": [
    "# split dataset into train and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "\n",
    "# split training set into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.33, random_state=1)\n",
    "\n",
    "# summarize data split\n",
    "print('Train: %s, Val: %s, Test: %s' % (X_train.shape, X_val.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the get_models() function from the previous section to create the classification models used in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the base models\n",
    "models = get_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the get_models() function from the previous section to create the classification models used in the ensemble.\n",
    "\n",
    "The fit_ensemble() function can then be called to fit the blending ensemble on the train and validation datasets and the predict_ensemble() function can be used to make predictions on the holdout dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the blending ensemble\n",
    "blender = fit_ensemble(models, X_train, X_val, y_train, y_val)\n",
    "\n",
    "# make predictions on test set\n",
    "yhat = predict_ensemble(models, blender, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can evaluate the performance of the blending model by reporting the classification accuracy on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blending Accuracy: 97.700\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictions\n",
    "score = accuracy_score(y_test, yhat)\n",
    "print('Blending Accuracy: %.3f' % (score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Blending Ensemble for Classification - soft voting\n",
    "In the previous example, crisp class label predictions were combined using the blending model. This is a type of [hard voting](https://machinelearningmastery.com/voting-ensembles-with-python/).\n",
    "\n",
    "An alternative is to have each model predict class probabilities and use the meta-model to blend the probabilities. This is a type of `soft voting` and can result in better performance in some cases.\n",
    "\n",
    "- 1. Configure the models to return probabilities, such as the `SVM model`.\n",
    "- 2. Change the base models to predict probabilities instead of crisp class labels.\n",
    "- 3. Change the predictions made by the base models when using the blending model to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (3350, 20), Val: (1650, 20), Test: (5000, 20)\n",
      "Blending Accuracy: 98.300\n"
     ]
    }
   ],
   "source": [
    "# blending ensemble for classification using soft voting\n",
    "from numpy import hstack\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "    X, y = make_classification(n_samples=10000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "    return X, y\n",
    "\n",
    "# get a list of base models\n",
    "def get_models():\n",
    "    models = list()\n",
    "    models.append(('lr', LogisticRegression()))\n",
    "    models.append(('knn', KNeighborsClassifier()))\n",
    "    models.append(('cart', DecisionTreeClassifier()))\n",
    "    models.append(('svm', SVC(probability=True))) ## (1)\n",
    "    models.append(('bayes', GaussianNB()))\n",
    "    return models\n",
    "\n",
    "# fit the blending ensemble\n",
    "def fit_ensemble(models, X_train, X_val, y_train, y_val):\n",
    "    \n",
    "    # fit all models on the training set and predict on hold out set\n",
    "    meta_X = list()\n",
    "    for name, model in models:\n",
    "        \n",
    "        # fit in training set\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # predict on hold out set\n",
    "        yhat = model.predict_proba(X_val)\n",
    "        \n",
    "        ## reshape predictions into a matrix with one column\n",
    "        ##yhat = yhat.reshape(len(yhat), 1) ## (2)\n",
    "\n",
    "        # store predictions as input for blending\n",
    "        meta_X.append(yhat)\n",
    "    \n",
    "    # create 2d array from predictions, each set is an input feature\n",
    "    meta_X = hstack(meta_X)\n",
    "    \n",
    "    # define blending model\n",
    "    blender = LogisticRegression()\n",
    "    \n",
    "    # fit on predictions from base models\n",
    "    blender.fit(meta_X, y_val)\n",
    "\n",
    "    return blender\n",
    "\n",
    "# make a prediction with the blending ensemble\n",
    "def predict_ensemble(models, blender, X_test):\n",
    "    \n",
    "    # make predictions with base models\n",
    "    meta_X = list()\n",
    "    for name, model in models:\n",
    "        \n",
    "        # predict with base model\n",
    "        yhat = model.predict_proba(X_test)\n",
    "        \n",
    "        ## reshape predictions into a matrix with one column\n",
    "        ##yhat = yhat.reshape(len(yhat), 1) ## (3)        \n",
    "        \n",
    "        # store prediction\n",
    "        meta_X.append(yhat)\n",
    "    \n",
    "    # create 2d array from predictions, each set is an input feature\n",
    "    meta_X = hstack(meta_X)\n",
    "\n",
    "    # predict\n",
    "    return blender.predict(meta_X)\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "\n",
    "# split dataset into train and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "\n",
    "# split training set into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.33, random_state=1)\n",
    "\n",
    "# summarize data split\n",
    "print('Train: %s, Val: %s, Test: %s' % (X_train.shape, X_val.shape, X_test.shape))\n",
    "\n",
    "# create the base models\n",
    "models = get_models()\n",
    "\n",
    "# train the blending ensemble\n",
    "blender = fit_ensemble(models, X_train, X_val, y_train, y_val)\n",
    "\n",
    "# make predictions on test set\n",
    "yhat = predict_ensemble(models, blender, X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "score = accuracy_score(y_test, yhat)\n",
    "print('Blending Accuracy: %.3f' % (score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A blending ensemble is only effective if it is able to out-perform any single contributing model.\n",
    "\n",
    "We can confirm this by evaluating each of the base models in isolation. Each base model can be fit on the entire training dataset (unlike the blending ensemble) and evaluated on the test dataset (just like the blending ensemble).\n",
    "\n",
    "The example below demonstrates this, evaluating each base model in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">lr Accuracy: 87.800\n",
      ">knn Accuracy: 97.380\n",
      ">cart Accuracy: 88.060\n",
      ">svm Accuracy: 98.200\n",
      ">bayes Accuracy: 87.300\n"
     ]
    }
   ],
   "source": [
    "# evaluate standalone model\n",
    "for name, model in models:\n",
    "    # fit the model on the training dataset\n",
    "    model.fit(X_train_full, y_train_full)\n",
    "    \n",
    "    # make a prediction on the test dataset\n",
    "    yhat = model.predict(X_test)\n",
    "    \n",
    "    # evaluate the predictions\n",
    "    score = accuracy_score(y_test, yhat)\n",
    "    \n",
    "    # report the score\n",
    "    print('>%s Accuracy: %.3f' % (name, score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that all models perform worse than the blended ensemble.\n",
    "\n",
    "Interestingly, we can see that the SVM comes very close to achieving an accuracy of 98.200 percent compared to 98.240 achieved with the blending ensemble.\n",
    "\n",
    "We may choose to use a blending ensemble as our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Blending Ensemble for Classification - Prediction\n",
    "\n",
    "This involves fitting the ensemble on the entire training dataset and making predictions on new examples. Specifically, the entire training dataset is split onto train and validation sets to train the base and meta-models respectively, then the ensemble can be used to make a prediction.\n",
    "\n",
    "The complete example of making a prediction on new data with a blending ensemble for classification is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (6700, 20), Val: (3300, 20)\n",
      "Predicted Class: 1\n"
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "\n",
    "# split dataset set into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "# summarize data split\n",
    "print('Train: %s, Val: %s' % (X_train.shape, X_val.shape))\n",
    "\n",
    "# create the base models\n",
    "models = get_models()\n",
    "\n",
    "# train the blending ensemble\n",
    "blender = fit_ensemble(models, X_train, X_val, y_train, y_val)\n",
    "\n",
    "# make a prediction on a new row of data\n",
    "row = [-0.30335011, 2.68066314, 2.07794281, 1.15253537, -2.0583897, -2.51936601, 0.67513028, -3.20651939, -1.60345385, 3.68820714, 0.05370913, 1.35804433, 0.42011397, 1.4732839, 2.89997622, 1.61119399, 7.72630965, -2.84089477, -1.83977415, 1.34381989]\n",
    "yhat = predict_ensemble(models, blender, [row])\n",
    "\n",
    "# summarize prediction\n",
    "print('Predicted Class: %d' % (yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Blending Ensemble for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate blending ensemble for regression\n",
    "from numpy import hstack\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset\n",
    "def get_dataset():\n",
    "    X, y = make_regression(n_samples=10000, n_features=20, n_informative=10, noise=0.3, random_state=7)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of base models\n",
    "def get_models():\n",
    "    models = list()\n",
    "    \n",
    "    models.append(('lr', LinearRegression()))\n",
    "    models.append(('knn', KNeighborsRegressor()))\n",
    "    models.append(('cart', DecisionTreeRegressor()))\n",
    "    models.append(('svm', SVR()))\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the blending ensemble\n",
    "def fit_ensemble(models, X_train, X_val, y_train, y_val):\n",
    "    \n",
    "    # fit all models on the training set and predict on hold out set\n",
    "    meta_X = list()\n",
    "    for name, model in models:\n",
    "        \n",
    "        # fit in training set\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # predict on hold out set\n",
    "        yhat = model.predict(X_val)\n",
    "        \n",
    "        # reshape predictions into a matrix with one column\n",
    "        yhat = yhat.reshape(len(yhat), 1)\n",
    "        \n",
    "        # store predictions as input for blending\n",
    "        meta_X.append(yhat)\n",
    "    \n",
    "    # create 2d array from predictions, each set is an input feature\n",
    "    meta_X = hstack(meta_X)\n",
    "    \n",
    "    # define blending model\n",
    "    blender = LinearRegression()\n",
    "    \n",
    "    # fit on predictions from base models\n",
    "    blender.fit(meta_X, y_val)\n",
    "    \n",
    "    return blender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction with the blending ensemble\n",
    "def predict_ensemble(models, blender, X_test):\n",
    "    \n",
    "    # make predictions with base models\n",
    "    meta_X = list()\n",
    "    for name, model in models:\n",
    "        \n",
    "        # predict with base model\n",
    "        yhat = model.predict(X_test)\n",
    "        \n",
    "        # reshape predictions into a matrix with one column\n",
    "        yhat = yhat.reshape(len(yhat), 1)\n",
    "        \n",
    "        # store prediction\n",
    "        meta_X.append(yhat)\n",
    "    \n",
    "    # create 2d array from predictions, each set is an input feature\n",
    "    meta_X = hstack(meta_X)\n",
    "\n",
    "    # predict\n",
    "    return blender.predict(meta_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (3350, 20), Val: (1650, 20), Test: (5000, 20)\n"
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "\n",
    "# split dataset into train and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "\n",
    "# split training set into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.33, random_state=1)\n",
    "\n",
    "# summarize data split\n",
    "print('Train: %s, Val: %s, Test: %s' % (X_train.shape, X_val.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the base models\n",
    "models = get_models()\n",
    "\n",
    "# train the blending ensemble\n",
    "blender = fit_ensemble(models, X_train, X_val, y_train, y_val)\n",
    "\n",
    "# make predictions on test set\n",
    "yhat = predict_ensemble(models, blender, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blending MAE: 0.237\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictions\n",
    "score = mean_absolute_error(y_test, yhat)\n",
    "print('Blending MAE: %.3f' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with classification, the blending ensemble is only useful if it performs better than any of the base models that contribute to the ensemble.\n",
    "\n",
    "We can check this by evaluating each base model in isolation by first fitting it on the entire training dataset (unlike the blending ensemble) and making predictions on the test dataset (like the blending ensemble).\n",
    "\n",
    "The example below evaluates each of the base models in isolation on the synthetic regression predictive modeling dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (5000, 20), Test: (5000, 20)\n",
      ">lr MAE: 0.236\n",
      ">knn MAE: 100.169\n",
      ">cart MAE: 132.670\n",
      ">svm MAE: 138.195\n"
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "\n",
    "# split dataset into train and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "\n",
    "# summarize data split\n",
    "print('Train: %s, Test: %s' % (X_train_full.shape, X_test.shape))\n",
    "\n",
    "# create the base models\n",
    "models = get_models()\n",
    "\n",
    "# evaluate standalone model\n",
    "for name, model in models:\n",
    "    # fit the model on the training dataset\n",
    "    model.fit(X_train_full, y_train_full)\n",
    "    \n",
    "    # make a prediction on the test dataset\n",
    "    yhat = model.predict(X_test)\n",
    "    \n",
    "    # evaluate the predictions\n",
    "    score = mean_absolute_error(y_test, yhat)\n",
    "    \n",
    "    # report the score\n",
    "    print('>%s MAE: %.3f' % (name, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that indeed the linear regression model has performed slightly better than the blending ensemble, achieving a MAE of 0.236 as compared to 0.237 with the ensemble. This may be because of the way that the synthetic dataset was constructed.\n",
    "\n",
    "Nevertheless, in this case, we would choose to use the linear regression model directly on this problem. This highlights the importance of checking the performance of the contributing models before adopting an ensemble model as the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Blending Ensemble for Regression - Prediction\n",
    "Again, we may choose to use a blending ensemble as our final model for regression.\n",
    "\n",
    "This involves fitting splitting the entire dataset into train and validation sets to fit the base and meta-models respectively, then the ensemble can be used to make a prediction for a new row of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (6700, 20), Val: (3300, 20)\n",
      "Predicted: 359.985\n"
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "\n",
    "# split dataset set into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "# summarize data split\n",
    "print('Train: %s, Val: %s' % (X_train.shape, X_val.shape))\n",
    "\n",
    "# create the base models\n",
    "models = get_models()\n",
    "\n",
    "# train the blending ensemble\n",
    "blender = fit_ensemble(models, X_train, X_val, y_train, y_val)\n",
    "\n",
    "# make a prediction on a new row of data\n",
    "row = [-0.24038754, 0.55423865, -0.48979221, 1.56074459, -1.16007611, 1.10049103, 1.18385406, -1.57344162, 0.97862519, -0.03166643, 1.77099821, 1.98645499, 0.86780193, 2.01534177, 2.51509494, -1.04609004, -0.19428148, -0.05967386, -2.67168985, 1.07182911]\n",
    "yhat = predict_ensemble(models, blender, [row])\n",
    "\n",
    "# summarize prediction\n",
    "print('Predicted: %.3f' % (yhat[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
