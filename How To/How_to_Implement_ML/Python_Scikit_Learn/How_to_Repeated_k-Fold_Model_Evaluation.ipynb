{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeated k-Fold Cross-Validation for Model Evaluation in Python\n",
    "\n",
    "by Jason Brownlee on August 3, 2020.[Here](https://machinelearningmastery.com/repeated-k-fold-cross-validation-with-python/) in [Python Machine Learning](https://machinelearningmastery.com/category/python-machine-learning/)\n",
    "\n",
    "The k-fold cross-validation procedure is a standard method for estimating the performance of a machine learning algorithm or configuration on a dataset.\n",
    "\n",
    "A single run of the k-fold cross-validation procedure may result in a noisy estimate of model performance. Different splits of the data may result in very different results.\n",
    "\n",
    "__Repeated k-fold cross-validation__ provides a way to `improve the estimated performance of a machine learning model. This involves simply repeating the cross-validation procedure multiple times and reporting the mean result across all folds from all runs`. This mean result is expected to be a more accurate estimate of the true unknown underlying mean performance of the model on the dataset, as __calculated using the standard error__.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "- The mean performance reported from a single run of k-fold cross-validation may be noisy.\n",
    "- Repeated k-fold cross-validation provides a way to reduce the error in the estimate of mean model performance.\n",
    "- How to evaluate machine learning models using repeated k-fold cross-validation in Python.\n",
    "\n",
    "## Tutorial Overview\n",
    "This tutorial is divided into three parts; they are:\n",
    "\n",
    "1. k-Fold Cross-Validation\n",
    "2. Repeated k-Fold Cross-Validation\n",
    "3. Repeated k-Fold Cross-Validation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. k-Fold Cross-Validation\n",
    "The k-fold cross-validation `procedure divides a limited dataset into k` __non-overlapping__ `folds`. Each of the k folds is given an opportunity to be used as a held back test set, whilst all other folds collectively are used as a training dataset. A total of k models are fit and evaluated on the k hold-out test sets and the mean performance is reported.\n",
    "\n",
    "Create a synthetic __binary classification__ dataset. We will configure it to generate 1,000 samples each with 20 input features, 15 of which contribute to the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.89 0.82 0.92 0.88 0.91 0.82 0.86 0.86 0.85 0.87]\n",
      "Accuracy: 0.868, STD: (0.032)\n"
     ]
    }
   ],
   "source": [
    "# evaluate a logistic regression model using k-fold cross-validation\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# create dataset\n",
    "X, y = make_classification(n_samples=1000, \n",
    "                           n_features=20, \n",
    "                           n_informative=15, \n",
    "                           n_redundant=5, \n",
    "                           random_state=1)\n",
    "\n",
    "# prepare the cross-validation procedure\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\n",
    "# create model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report performance\n",
    "print('Scores:', scores)\n",
    "print('Accuracy: %.3f, STD: (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Repeated k-Fold Cross-Validation\n",
    "\n",
    "The estimate of model performance via __k-fold cross-validation__ can be noisy.\n",
    "\n",
    "This means that each time the procedure is run, `a different split of the dataset into k-folds can be implemented, and in turn, the distribution of performance scores can be different`, resulting in a different mean estimate of model performance.\n",
    "\n",
    "- One __solution to reduce the noise in the estimated model performance is to increase the k-value__. This will reduce the bias in the model’s estimated performance, although it will increase the variance: e.g. tie the result more to the specific dataset used in the evaluation.\n",
    "- An alternate approach is to repeat the k-fold cross-validation process multiple times and report the mean performance across all folds and all repeats. For example, if 10-fold cross-validation was repeated five times, 50 different held-out sets would be used to estimate model efficacy.\n",
    "\n",
    "Common numbers of repeats include 3, 5, and 10. For example, if 3 repeats of 10-fold cross-validation are used to estimate the model performance, this means that (3 * 10) or 30 different models would need to be fit and evaluated.\n",
    "\n",
    "As such, the approach is suited for small- to modestly-sized datasets and/or models that are not too computationally costly to fit and evaluate. This __suggests that the approach may be appropriate for linear models and not appropriate for slow-to-fit models like deep learning neural networks__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Repeated k-Fold Cross-Validation in Python\n",
    "The scikit-learn Python machine learning library provides an implementation of repeated k-fold cross-validation via the [RepeatedKFold class](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedKFold.html).\n",
    "\n",
    "The main parameters are the number of folds (n_splits), which is the “k” in k-fold cross-validation, and the number of repeats (n_repeats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.89 0.82 0.92 0.88 0.91 0.82 0.86 0.86 0.85 0.87 0.83 0.85 0.8  0.84\n",
      " 0.91 0.87 0.88 0.92 0.91 0.86 0.9  0.85 0.82 0.86 0.86 0.86 0.87 0.87\n",
      " 0.87 0.91]\n",
      "Accuracy: 0.867, STD: (0.031)\n"
     ]
    }
   ],
   "source": [
    "# evaluate a logistic regression model using repeated k-fold cross-validation\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# create dataset\n",
    "X, y = make_classification(n_samples=1000, \n",
    "                           n_features=20, \n",
    "                           n_informative=15, \n",
    "                           n_redundant=5, \n",
    "                           random_state=1)\n",
    "\n",
    "# prepare the cross-validation procedure\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# create model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report performance\n",
    "print('Scores:', scores)\n",
    "print('Accuracy: %.3f, STD: (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may suggest that the single run result may be optimistic and that the result from three repeats might be a better estimate of the true mean model performance.\n",
    "\n",
    "The expectation of repeated k-fold cross-validation is that the repeated mean would be a more reliable estimate of model performance than the result of a single k-fold cross-validation procedure.\n",
    "\n",
    "This may mean less statistical noise.\n",
    "\n",
    "The __standard error__ can provide an indication for a given sample size of the amount of error or the spread of error that may be expected from the sample mean to the underlying and unknown population mean.\n",
    "\n",
    "Standard error can be calculated as follows:\n",
    "\n",
    "- standard_error = sample_standard_deviation / sqrt(number of repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard error: 0.005811865258054234\n"
     ]
    }
   ],
   "source": [
    "# calculate the standard error for a sample using the sem() scipy function\n",
    "from scipy import stats\n",
    "print('Standard error:', stats.sem(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below demonstrates this by reporting model performance with 10-fold cross-validation with 1 to 15 repeats of the procedure.\n",
    "\n",
    "We would expect that more repeats of the procedure would result in a more accurate estimate of the mean model performance, given the [law of large numbers](https://machinelearningmastery.com/a-gentle-introduction-to-the-law-of-large-numbers-in-machine-learning/). Although, the trials are not independent, so the underlying statistical principles become challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1 mean=0.8680 se=0.011\n",
      ">2 mean=0.8675 se=0.008\n",
      ">3 mean=0.8673 se=0.006\n",
      ">4 mean=0.8670 se=0.006\n",
      ">5 mean=0.8658 se=0.005\n",
      ">6 mean=0.8655 se=0.004\n",
      ">7 mean=0.8651 se=0.004\n",
      ">8 mean=0.8651 se=0.004\n",
      ">9 mean=0.8656 se=0.003\n",
      ">10 mean=0.8658 se=0.003\n",
      ">11 mean=0.8655 se=0.003\n",
      ">12 mean=0.8654 se=0.003\n",
      ">13 mean=0.8652 se=0.003\n",
      ">14 mean=0.8651 se=0.003\n",
      ">15 mean=0.8653 se=0.003\n"
     ]
    }
   ],
   "source": [
    "# compare the number of repeats for repeated k-fold cross-validation\n",
    "from scipy.stats import sem\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# evaluate a model with a given number of repeats\n",
    "def evaluate_model(X, y, repeats):\n",
    "    # prepare the cross-validation procedure\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=repeats, random_state=1)\n",
    "    \n",
    "    # create model\n",
    "    model = LogisticRegression()\n",
    "    \n",
    "    # evaluate model\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    "\n",
    "# create dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "\n",
    "# configurations to test\n",
    "repeats = range(1,16)\n",
    "results = list()\n",
    "for r in repeats:\n",
    "    # evaluate using a given number of repeats\n",
    "    scores = evaluate_model(X, y, r)\n",
    "    \n",
    "    # summarize\n",
    "    print('>%d mean=%.4f se=%.3f' % (r, mean(scores), sem(scores)))\n",
    "    \n",
    "    # store\n",
    "    results.append(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that the default of one repeat appears optimistic compared to the other results with an accuracy of about 86.80 percent compared to 86.73 percent and lower with differing numbers of repeats.\n",
    "\n",
    "We can see that the mean seems to coalesce around a value of about 86.5 percent. We might take this as the stable estimate of model performance and in turn, choose 5 or 6 repeats that seem to approximate this value first.\n",
    "\n",
    "Looking at the standard error, we can see that it decreases with an increase in the number of repeats and stabilizes with a value around 0.003 at around 9 or 10 repeats, although 5 repeats achieve a standard error of 0.005, half of that achieved with a single repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAaw0lEQVR4nO3df5Ac9X3m8ffjFTIGZCEZoZMlYSmUSgipYtmeU3znlOOzYluQGBknTkTlfFiHS6YK5SCX+NCZVE4u6qqIDeauCgqdMDoTB0EZA0a4CKBDzmGuyrZW8uo3CmuB0SJZWiwu8h2H9etzf3SLNKPZ3Z6d7tX29vOqmprp7m8/8+3emflMf2e2RxGBmZnVzzvOdgfMzOzscAEwM6spFwAzs5pyATAzqykXADOzmhp3tjvQjosuuihmzZp1trthZlYpW7ZseS0ipjTPr1QBmDVrFt3d3We7G2ZmlSLp563mewjIzKymXADMzGrKBcDMrKZcAMzMasoFwMysplwAzMxqygXAzKymXADMzGqqUv8IZvlJGnBZp78BMVB2WbmdZo+lfTFa9/Fg2VXL7TS7SvvYBWCMyj4gJHX8wGuVXVZu0dneF+XnZrPrvi9GYh8Xle0hIDOzmnIBMDOrKRcAM7OacgEwM6spFwAzs5pyATAzqykXADOzmnIBMDOrKRcAM7OacgEwM6spFwAzs5pyATAzqykXADOzmspVACQtkbRXUq+kVS2WT5L0mKTtkn4iaUE6f6akH0jaI2mXpBsz66yW9KqknvRyZXGbZWZmQxnydNCSuoC7gU8AfcBmSRsiYnem2VeAnoi4WtJlafvFwAngzyNiq6QJwBZJGzPr3hkRtxe5QWZmlk+eI4BFQG9E7IuIY8BDwNKmNpcDzwJExAvALElTI+JgRGxN5/8K2ANML6z3ZmY2bHkKwHRgf2a6jzNfxLcBnwWQtAh4HzAj20DSLOADwI8zs1emw0brJE1qdeeSVkjqltTd39+fo7tmZpZHngLQ6jfImn+G5jZgkqQe4E+Bn5IM/yQB0gXAI8BNEXE0nX0PcCmwEDgI3NHqziNibUQ0IqIxZcqUHN01M7M88vwkZB8wMzM9AziQbZC+qC8HUPKjlS+lFySdQ/Li/0BEPJpZ59Dp25LuBb4/vE0wM7PhyHMEsBmYI2m2pPHAMmBDtoGkC9NlAF8EnouIo2kxuA/YExHfaFpnWmbyamDncDfCzMzaN+QRQESckLQSeBroAtZFxC5J16fL1wDzgL+RdBLYDVyXrv4R4PPAjnR4COArEfEk8DVJC0mGk14GvlTcZpmZ2VBU5C/Wl63RaER3d/eAy5MDjtY63c6BssvKLSI7ex9l/J3Lyi0zu2q5ZWZXLbfM7KrltpstaUtENJrn5/kMoDKyO6PoHX86q6zcMrLNzAbjU0GYmdWUC4CZWU25AJiZ1ZQLgJlZTbkAmJnVlAuAmVlNuQCYmdWUC4CZWU25AJiZ1ZQLgJlZTbkAmJnVlAuAmVlNuQCYmdWUC4CZWU25AJiZ1ZQLgJlZTbkAmJnVlAuAmVlNuQCYmdVUrgIgaYmkvZJ6Ja1qsXySpMckbZf0E0kLhlpX0mRJGyW9mF5PKmaTzMwsjyELgKQu4G7gCuBy4BpJlzc1+wrQExG/Cfwb4L/mWHcV8GxEzAGeTafNzGyE5DkCWAT0RsS+iDgGPAQsbWpzOcmLOBHxAjBL0tQh1l0K3J/evh/4TEdbYmZmbclTAKYD+zPTfem8rG3AZwEkLQLeB8wYYt2pEXEQIL2+uNWdS1ohqVtSd39/f47u1tfkyZORdMYFaDl/8uTJZ7nHZnY25SkAajEvmqZvAyZJ6gH+FPgpcCLnuoOKiLUR0YiIxpQpU9pZtXZef/11IiL35fXXXz/bXTazs2hcjjZ9wMzM9AzgQLZBRBwFlgMoecv5Uno5b5B1D0maFhEHJU0DDg9rC8zMbFjyHAFsBuZImi1pPLAM2JBtIOnCdBnAF4Hn0qIw2LobgGvT29cCj3e2KWZm1o4hjwAi4oSklcDTQBewLiJ2Sbo+Xb4GmAf8jaSTwG7gusHWTaNvA74j6TrgFeBzxW6amZkNRhFtDcmfVY1GI7q7u3O1lUQZ21ZWbhHZ7a4/0vc3GrKrlltmdtVyy8yuWm672ZK2RESjeb7/E3gQrb5VA/5GjZmNDXk+BK6t09+qyeN0cTAzqwofAZiZ1ZQLgJlZTbkAmJnVlAuA5VLWB+LtnL5itHzQXrV9UeYpQrwvqrsvwB8CW05lfSBexQ/aq7Yv2sktM7tquWVmj4Zc8BGAmVltjYkCULXv61dx2KNqfGZUs6GNiSGgqg0jVK2/VVTmob7ZWDEmjgDMzKx9LgBmZjXlAmBmVlMuAGZmNeUCYGZWUy4AZmY15QJgZlZTLgBmZjXlAmBmVlMuAGZmNZWrAEhaImmvpF5Jq1osnyjpCUnbJO2StDydP1dST+ZyVNJN6bLVkl7NLLuy2E0zM7PBDHkuIEldwN3AJ4A+YLOkDRGxO9PsBmB3RHxa0hRgr6QHImIvsDCT8yrwWGa9OyPi9oK2xczM2pDnCGAR0BsR+yLiGPAQsLSpTQATlJxR6wLgCHCiqc1i4GcR8fMO+2xmZgXIUwCmA/sz033pvKy7gHnAAWAHcGNEnGpqswx4sGneSknbJa2TNKnVnUtaIalbUnd/f3+O7pqZWR55CkCr8+Q2n2f3U0AP8F6SIZ+7JL37rQBpPHAV8HBmnXuAS9P2B4E7Wt15RKyNiEZENKZMmZKju2ZmlkeeAtAHzMxMzyB5p5+1HHg0Er3AS8BlmeVXAFsj4tDpGRFxKCJOpkcK95IMNZmZ2QjJUwA2A3MkzU7fyS8DNjS1eYVkjB9JU4G5wL7M8mtoGv6RNC0zeTWws72um5lZJ4b8FlBEnJC0Enga6ALWRcQuSdeny9cAtwLfkrSDZMjo5oh4DUDSeSTfIPpSU/TXJC0kGU56ucVyMzMrUa6fhIyIJ4Enm+atydw+AHxygHXfAN7TYv7n2+ppG/rf6OfLz32Z23/ndi5610Vl3Y2ZWaWpnd9NPdsajUZ0d3efMV/S237/9dYf3crDex/mj+b+EX/54b8ctO1gmtsOVlg6yR0su6zcIrLHSttR04/VE/O1e6v9P5aTW2Z21XLLzB7BXElbIqJxxvyxVgD63+jnikev4Ncnf807u97JU3/wVGEvqGUVlsGyy8otInustB0t/RgNbUdLP0ZD29HSjyLaDlQAxty5gNZsX8Op9F8QTsUp1mxbM8Qa+fS/0c/jvY8TBN/r/R6v/b/XCsktM7vsPn/hqS8Umll2dtVyy8yuWm6Z2VXLLTJ7TBWA0y94x08dB+D4qeOFvfCVVVjKzC67z1sPbS00s+zsquWWmV213DKzq5ZbZPaYKgDZF7zTinjhK7OwlJU9En320ZD3xUjklpldtdyis8dEAYj/9G5YPZFt2//2rRe8046fOk7P9m8nH6Ssnpi0bTN3zX3/nFPH33zbslPH32TNNxsd5ebJLiu3qOzmzHZzm1XhaMj7YmRzy8yuWm7R2WPuQ+ChDKftH274Q/a+vveM5XMnzeW7V323oz4MlV1W7nD7nP2Q/bTmD9uH2+ehssvKHW6fvS+8L1plj8Z9MdCHwLn+D6DuTr9gVim7rNzBhtmav2U0WrKrlltmdtVyy8yuWm4Z2WNiCMhGzrbD21oPsx3uGbXZVcstM7tquWVmVy23jGwPAdW07Wjpx2hoO1r6MRrajpZ+jIa2o6Uf/j8AMzMrnD8DsLPqrW/U5G1bQm672WZjhQuAnVX66tH2Dm9XF5/bbrbZWOEhIDOzmnIBMDOrKRcAM7OacgEwM6spFwAzs5pyATAzqykXADOzmspVACQtkbRXUq+kVS2WT5T0hKRtknZJWp5Z9rKkHZJ6JHVn5k+WtFHSi+n1pE42RFKuy6RJHd2NmdmYMWQBkNQF3A1cAVwOXCPp8qZmNwC7I+L9wMeAOySNzyz/VxGxsOlcFKuAZyNiDvBsOj0sEXHGZaD5R44cGe7dmJmNKXmOABYBvRGxLyKOAQ8BS5vaBDBBkoALgCPAiSFylwL3p7fvBz6Tu9dmZtaxPKeCmA7sz0z3Ab/V1OYuYANwAJgA/HHEWyetDuAZSQH8t4hYm86fGhEHASLioKSLW925pBXACoBLLrkkR3erIamVQ2t3yCpvbpnZo2WYrcx9UZaq7WMbGWU9LvIUgFb33HySlU8BPcDHgUuBjZJ+GBFHgY9ExIH0BX6jpBci4rm8HUwLxlpITgedd73RbKBz1LR7qtqRyh0ou4jcspS5L8pSxT5b+cp8XOQZAuoDZmamZ5C8089aDjwaiV7gJeAygIg4kF4fBh4jGVICOCRpGkB6fXi4G2FmZu3LUwA2A3MkzU4/2F1GMtyT9QqwGEDSVGAusE/S+ZImpPPPBz4J7EzX2QBcm96+Fni8kw0pi79dVL4q7uOq9Tlvf0dTn618Qw4BRcQJSSuBp4EuYF1E7JJ0fbp8DXAr8C1JO0iGjG6OiNck/QbwWDp+NQ5YHxFPpdG3Ad+RdB1JAflcwdvWsaoNe1RRFYc9qva4qOI+tpGR6/cAIuJJ4MmmeWsytw+QvLtvXm8f8P4BMn9JetRgZmYjz/8JbGZWUy4AZmY15QJgZlZT/k1gM+uI/3mtulwAzGzYqvaNKHs7DwGZmdWUjwDMbFTy+bLK5wJgZqOOz5c1MjwEZGZWUy4AZmY15QJgZlZTLgBmZjXlAmBmVlMuAGZmNeUCYGZWUy4AZmY15QJgZlZTLgBmZjXlAmBmVlMuAGZmNZWrAEhaImmvpF5Jq1osnyjpCUnbJO2StDydP1PSDyTtSeffmFlntaRXJfWklyuL2ywzMxvKkGcDldQF3A18AugDNkvaEBG7M81uAHZHxKclTQH2SnoAOAH8eURslTQB2CJpY2bdOyPi9kK3yMzMcslzBLAI6I2IfRFxDHgIWNrUJoAJSk6GfQFwBDgREQcjYitARPwK2ANML6z3ZmY2bHkKwHRgf2a6jzNfxO8C5gEHgB3AjRFxKttA0izgA8CPM7NXStouaZ2klr+QIGmFpG5J3f39/Tm6a2ZmeeQpAK1+4qb5Fw4+BfQA7wUWAndJevdbAdIFwCPATRFxNJ19D3Bp2v4gcEerO4+ItRHRiIjGlClTcnTXzMzyyFMA+oCZmekZJO/0s5YDj0aiF3gJuAxA0jkkL/4PRMSjp1eIiEMRcTI9UriXZKjJzMxGSJ4CsBmYI2m2pPHAMmBDU5tXgMUAkqYCc4F96WcC9wF7IuIb2RUkTctMXg3sHN4mmJnZcAz5LaCIOCFpJfA00AWsi4hdkq5Pl68BbgW+JWkHyZDRzRHxmqTfBj4P7JDUk0Z+JSKeBL4maSHJcNLLwJcK3jYzMxuERvMPFjdrNBrR3d2dq21ZP8Zc5o88V63P3hfl55aZXbXcMrOrlttutqQtEdFonj/kEUCVJCNOraerVOjMzEbCmCoAfpE3M8vP5wIyM6spFwAzs5pyATAzqykXADOzmnIBMDOrKRcAM7OacgEwM6spFwAzs5pyATAzqykXADOzmnIBMDOrKRcAM7OacgEwM6spFwAzs5pyATAzqykXADOzmnIBMDOrKRcAM7OaylUAJC2RtFdSr6RVLZZPlPSEpG2SdklaPtS6kiZL2ijpxfR6UjGbZGZmeQxZACR1AXcDVwCXA9dIuryp2Q3A7oh4P/Ax4A5J44dYdxXwbETMAZ5Np83MbITkOQJYBPRGxL6IOAY8BCxtahPABEkCLgCOACeGWHcpcH96+37gMx1tiZmZtWVcjjbTgf2Z6T7gt5ra3AVsAA4AE4A/johTkgZbd2pEHASIiIOSLm5155JWACsALrnkkhzdLUdS2868HRGF5RaZXVZuc5b3xcjui9G6j5uzisy1f1L04yJPAVCLec339imgB/g4cCmwUdIPc647qIhYC6wFaDQaZ+2RVNaDuGq5ZWZXLbfM7Krllp1tiaL3cZ4hoD5gZmZ6Bsk7/azlwKOR6AVeAi4bYt1DkqYBpNeH2+++mZkNV54CsBmYI2m2pPHAMpLhnqxXgMUAkqYCc4F9Q6y7Abg2vX0t8HgnG2JmZu0ZcggoIk5IWgk8DXQB6yJil6Tr0+VrgFuBb0naQTLsc3NEvAbQat00+jbgO5KuIykgnyt208zMbDCq0rhdo9GI7u7us90NMztLJJXyWUPVcofRjy0R0Wie7/8ENjOrKRcAM7OacgEwM6spFwAzs5pyATAzqykXADOzmnIBMDOrKRcAM7OacgEwM6spFwAzs5pyATAzqykXADOzmnIBMDOrKRcAM7OacgEwM6spFwAzs5pyATAzqykXADOzmnIBMDOrKRcAM7OaylUAJC2RtFdSr6RVLZZ/WVJPetkp6aSkyZLmZub3SDoq6aZ0ndWSXs0su7LojTMzs4GNG6qBpC7gbuATQB+wWdKGiNh9uk1EfB34etr+08CfRcQR4AiwMJPzKvBYJv7OiLi9oG0xM7M25DkCWAT0RsS+iDgGPAQsHaT9NcCDLeYvBn4WET9vv5tmZla0PAVgOrA/M92XzjuDpPOAJcAjLRYv48zCsFLSdknrJE0aIHOFpG5J3f39/Tm6a2ZmeeQpAGoxLwZo+2ngf6XDP/8UII0HrgIezsy+B7iUZIjoIHBHq8CIWBsRjYhoTJkyJUd3zcwsjzwFoA+YmZmeARwYoG2rd/kAVwBbI+LQ6RkRcSgiTkbEKeBekqEmMzMbIXkKwGZgjqTZ6Tv5ZcCG5kaSJgK/AzzeIuOMzwUkTctMXg3szNtpy+fBBx9kwYIFdHV1sWDBAh58sFVtHj25ZWZXLbfM7Krl2tsVup8jYsgLcCXwD8DPgFvSedcD12fafAF4qMW65wG/BCY2zf82sAPYTlJQpg3Vjw996ENh+axfvz5mz54dmzZtimPHjsWmTZti9uzZsX79+lGZW8U+e1+Un9sseckqXlVyh7ufge5o9dreauZovbgA5Dd//vzYtGnT2+Zt2rQp5s+fPypzy8yuWm6Z2VXLbVaVF+qycoe7nwcqAEqWVUOj0Yju7u6z3Y1K6Orq4s033+Scc855a97x48c599xzOXny5KjLrWKfvS/KzwWQWn0PBTp97Root9PssnJh+PtZ0paIaDTP96kgxqh58+bx/PPPv23e888/z7x580ZlbpnZVcstM7tquTDwKEVZuZ1ml5ULJeznwTo72i4eAsqvimO9Veuz90X5ufZ2/gzAclu/fn3Mnz8/3vGOd8T8+fMLezKWlVtmdtVyy8yuWq693XD280AFwJ8BmJmNcf4MwMzM3sYFwMysplwAzMxqygXAzKymXADMzGqqUt8CktQP5P1BmYuA10roRlm5ZWZXLbfM7Krllpldtdwys6uW2272+yLijPPpV6oAtENSd6uvPY3W3DKzq5ZbZnbVcsvMrlpumdlVyy0q20NAZmY15QJgZlZTY7kArK1YbpnZVcstM7tquWVmVy23zOyq5RaSPWY/AzAzs8GN5SMAMzMbhAuAmVlNjbkCIGmdpMOSCv2ReUkzJf1A0h5JuyTdWFDuuZJ+ImlbmvvVInIz+V2Sfirp+wXnvixph6QeSYWdolXShZK+K+mFdF//i4Jy56Z9PX05KummgrL/LP3b7ZT0oKRzC8q9Mc3c1WlfWz0vJE2WtFHSi+n1pIJyP5f2+ZSkYX1NcYDcr6ePi+2SHpN0YYHZt6a5PZKekfTeInIzy/5CUki6qKD+rpb0aubxfGW7uUC1fg8gzwX4KPBBYGfBudOAD6a3JwD/AFxeQK6AC9Lb5wA/Bj5cYL//PbAe+H7B++Nl4KIS/n73A19Mb48HLizhPrqAX5D8c0ynWdOBl4B3pdPfAb5QQO4CYCdwHjAO+B/AnA7yznheAF8DVqW3VwF/XVDuPGAu8PdAo8D+fhIYl97+6+H0d5Dsd2du/ztgTRG56fyZwNMk/8Ta9nNmgP6uBv6i08fZmDsCiIjngCMl5B6MiK3p7V8Be0ie/J3mRkT8n3TynPRSyCfzkmYAvwd8s4i8skl6N8mD/T6AiDgWEf+7hLtaDPwsIvL+V/lQxgHvkjSO5AX7QAGZ84AfRcQbEXEC+J/A1cMNG+B5sZSk4JJef6aI3IjYExF7h9PPIXKfSfcFwI+AGQVmH81Mns8wnoODvPbcCfyH4WQOkduxMVcARoKkWcAHSN6tF5HXJakHOAxsjIhCcoH/QvLAO1VQXlYAz0jaImlFQZm/AfQD/z0dtvqmpPMLys5aBjxYRFBEvArcDrwCHAT+MSKeKSB6J/BRSe+RdB5wJck7ySJNjYiDkLzBAS4uOL9M/xb4uyIDJf1nSfuBPwH+qqDMq4BXI2JbEXlNVqbDVuuGM3wHLgBtk3QB8AhwU9O7hmGLiJMRsZDkHc0iSQs6zZT0+8DhiNjScQdb+0hEfBC4ArhB0kcLyBxHcqh7T0R8APi/JEMThZE0HrgKeLigvEkk76RnA+8Fzpf0rzvNjYg9JMMcG4GngG3AiUFXqglJt5DsiweKzI2IWyJiZpq7stO8tHDfQkHFpMk9wKXAQpI3HncMJ8QFoA2SziF58X8gIh4tOj8d7vh7YEkBcR8BrpL0MvAQ8HFJf1tALgARcSC9Pgw8BiwqILYP6MscAX2XpCAU6Qpga0QcKijvd4GXIqI/Io4DjwL/sojgiLgvIj4YER8lGQJ4sYjcjEOSpgGk14cLzi+cpGuB3wf+JNLB8BKsB/6ggJxLSd4YbEufhzOArZL+WafBEXEofeN4CriXYT7/XABykiSSsek9EfGNAnOnnP42g6R3kbygvNBpbkT8x4iYERGzSIY8NkVEx+9MASSdL2nC6dskH851/K2riPgFsF/S3HTWYmB3p7lNrqGg4Z/UK8CHJZ2XPkYWk3w+1DFJF6fXlwCfpdh+A2wArk1vXws8XnB+oSQtAW4GroqINwrOnpOZvIpinoM7IuLiiJiVPg/7SL5I8otOs08X7tTVDPf51+mnyKPtQvIkOQgcJ9nh1xWU+9sk497bgZ70cmUBub8J/DTN3Qn8VQn75GMU+C0gkrH6bellF3BLgdkLge50f3wPmFRg9nnAL4GJBe/fr5K8YOwEvg28s6DcH5IUwG3A4g6zznheAO8BniU5sngWmFxQ7tXp7V8Dh4CnC8rtBfZnnn9tf1NnkOxH0r/fduAJYHoRuU3LX2Z43wJq1d9vAzvS/m4Apg1nX/hUEGZmNeUhIDOzmnIBMDOrKRcAM7OacgEwM6spFwAzs5pyATAzqykXADOzmvr/vcMvQhBcmLAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the results\n",
    "pyplot.boxplot(results, labels=[str(r) for r in repeats], showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A box and whisker plot is created to summarize the distribution of scores for each number of repeats.\n",
    "\n",
    "The orange line indicates the median of the distribution and the green triangle represents the arithmetic mean. If these symbols (values) coincide, it suggests a reasonable symmetric distribution and that the mean may capture the central tendency well.\n",
    "\n",
    "This might provide an additional heuristic for choosing an appropriate number of repeats for your test harness.\n",
    "\n",
    "Taking this into consideration, using five repeats with this chosen test harness and algorithm appears to be a good choice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
