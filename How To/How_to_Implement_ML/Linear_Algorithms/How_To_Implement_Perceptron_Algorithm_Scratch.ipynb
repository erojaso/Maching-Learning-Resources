{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How To Implement The Perceptron Algorithm From Scratch In Python\n",
    "\n",
    "by Jason Brownlee on August 13, 2019.[Here](https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/) in [Code Algorithms From Scratch](https://machinelearningmastery.com/category/algorithms-from-scratch/)\n",
    "\n",
    "The Perceptron algorithm is the __simplest type of artificial neural network__.\n",
    "\n",
    "It is a model of a single neuron that can be used for two-class classification problems and provides the foundation for later developing much larger networks.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "- How to train the network weights for the Perceptron.\n",
    "- How to make predictions with the Perceptron.\n",
    "- How to implement the Perceptron algorithm for a real-world classification problem.\n",
    "\n",
    "## Tutorial\n",
    "This tutorial is broken down into 3 parts:\n",
    "\n",
    "1. Description\n",
    "    - 1.1 Perceptron Algorithm\n",
    "    - 1.2 Stochastic Gradient Descent\n",
    "    - 1.3 Sonar Dataset\n",
    "2. Making Predictions.\n",
    "3. Training Network Weights.\n",
    "4. Modeling the Sonar Dataset.\n",
    "5. Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Description\n",
    "\n",
    "### 1.1 Perceptron Algorithm\n",
    "\n",
    "__Perceptron__ receives `input signals from examples of training data` that we weight and combined in a `linear equation` called the __activation__.\n",
    "\n",
    "$activation = sum(weight_i * x_i) + bias$\n",
    "\n",
    "The activation is then transformed into an __output value__ or __prediction__ using a transfer function, such as the `step transfer function`.\n",
    "\n",
    "$prediction = 1.0 (if (activation >= 0.0) else (0.0))$\n",
    "\n",
    "In this way, the __Perceptron is a classification algorithm for problems with two classes__ (0 and 1) where a linear equation (like or hyperplane) can be used to separate the two classes.\n",
    "\n",
    "### 1.2 Stochastic Gradient Descent\n",
    "\n",
    "__Gradient Descent__ is the process of minimizing a function by following the gradients of the cost function.\n",
    "\n",
    "In machine learning, we can use a technique that evaluates and updates the weights every iteration called stochastic gradient descent `to minimize the error of a model on our training data`. \n",
    "\n",
    "The model makes a prediction for a training instance, the error is calculated and the model is updated in order to reduce the error for the next prediction.\n",
    "\n",
    "This procedure can be used to find the set of weights in a model that result in the smallest error for the model on the training data.\n",
    "\n",
    "For the Perceptron algorithm, each iteration the weights (__w__) are updated using the equation:\n",
    "\n",
    "$w = w + learning_rate * (expected - predicted) * x$\n",
    "\n",
    "Where w is weight being optimized, __learning_rate__ is a learning rate that you must configure (e.g. 0.01), (__expected__ – __predicted__) is the prediction error for the model on the training data attributed to the weight and x is the input value.\n",
    "\n",
    "\n",
    "### 1.3 Sonar Dataset\n",
    "\n",
    "This is a dataset that describes sonar chirp returns bouncing off different services. The 60 input variables are the strength of the returns at different angles. It is a binary classification problem that requires a model to differentiate rocks from metal cylinders.\n",
    "\n",
    "It is a well-understood dataset. All of the `variables are continuous and generally in the range of 0 to 1`. As such we __will not have to normalize the input data, which is often a good practice with the Perceptron algorithm__. The output variable is a string “__M__” for mine and “__R__” for rock, which will need to be converted to integers __1__ and __0__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Making Predictions.\n",
    "The first step is to develop a function that can make predictions.\n",
    "\n",
    "This will be needed both in the evaluation of candidate weights values in stochastic gradient descent, and after the model is finalized and we wish to start making predictions on test data or new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small dataset to test our prediction function\n",
    "# Make a prediction with weights\n",
    "def predict(row, weights):\n",
    "    activation = weights[0]\n",
    "    for i in range(len(row)-1):\n",
    "        activation += weights[i + 1] * row[i]\n",
    "    return 1.0 if activation >= 0.0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected=0, Predicted=0\n",
      "Expected=0, Predicted=0\n",
      "Expected=0, Predicted=0\n",
      "Expected=0, Predicted=0\n",
      "Expected=0, Predicted=0\n",
      "Expected=1, Predicted=1\n",
      "Expected=1, Predicted=1\n",
      "Expected=1, Predicted=1\n",
      "Expected=1, Predicted=1\n",
      "Expected=1, Predicted=1\n"
     ]
    }
   ],
   "source": [
    "# test predictions\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "           [1.465489372,2.362125076,0],\n",
    "           [3.396561688,4.400293529,0],\n",
    "           [1.38807019,1.850220317,0],\n",
    "           [3.06407232,3.005305973,0],\n",
    "           [7.627531214,2.759262235,1],\n",
    "           [5.332441248,2.088626775,1],\n",
    "           [6.922596716,1.77106367,1],\n",
    "           [8.675418651,-0.242068655,1],\n",
    "           [7.673756466,3.508563011,1]]\n",
    "\n",
    "# first weight is always the bias as it is standalone and \n",
    "# not responsible for a specific input value\n",
    "weights = [-0.1, 0.20653640140000007, -0.23418117710000003]\n",
    "\n",
    "for row in dataset:\n",
    "    prediction = predict(row, weights)\n",
    "    print(\"Expected=%d, Predicted=%d\" % (row[-1], prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two inputs values (__X1__ and __X2__) and three weight values (__bias__, __w1__ and __w2__). The activation equation we have modeled for this problem is:\n",
    "\n",
    "$activation = (w1 * X1) + (w2 * X2) + bias$\n",
    "\n",
    "Or, with the specific weight values we chose by hand as:\n",
    "\n",
    "$activation = (0.206 * X1) + (-0.234 * X2) + -0.1$\n",
    "\n",
    "\n",
    "## 3. Training Network Weights.\n",
    "We can estimate the __weight values__ for our training data using stochastic gradient descent.\n",
    "\n",
    "Stochastic gradient descent requires two parameters:\n",
    "\n",
    "- __Learning Rate__: Used to limit the amount each weight is corrected each time it is updated.\n",
    "- __[Epochs](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)__: The number of times to run through the training data while updating the weight.\n",
    "\n",
    "There are 3 loops we need to perform in the function:\n",
    "\n",
    "- Loop over each epoch.\n",
    "- Loop over each row in the training data for an epoch.\n",
    "- Loop over each weight and update it for a row in an epoch.\n",
    "\n",
    "__we update each weight for each row in the training data, each epoch.__\n",
    "\n",
    "__Weights__ are updated based on the error the model made. The __error__ is calculated as the difference between the expected output value and the prediction made with the candidate weights.\n",
    "\n",
    "There is one weight for each input attribute, and these are updated in a consistent way, for example:\n",
    "\n",
    "$w(t+1)= w(t) + learning_rate * (expected(t) - predicted(t)) * x(t)$\n",
    "\n",
    "The bias is updated in a similar way, except without an input as it is not associated with a specific input value:\n",
    "\n",
    "$bias(t+1) = bias(t) + learning_rate * (expected(t) - predicted(t))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with weights\n",
    "def predict(row, weights):\n",
    "    activation = weights[0]\n",
    "    for i in range(len(row)-1):\n",
    "        activation += weights[i + 1] * row[i]\n",
    "    return 1.0 if activation >= 0.0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "def train_weights(train, l_rate, n_epoch):\n",
    "    weights = [0.0 for i in range(len(train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0.0\n",
    "        for row in train:\n",
    "            prediction = predict(row, weights)\n",
    "            error = row[-1] - prediction\n",
    "            sum_error += error**2\n",
    "            weights[0] = weights[0] + l_rate * error\n",
    "            for i in range(len(row)-1):\n",
    "                weights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.100, error=2.000\n",
      ">epoch=1, lrate=0.100, error=1.000\n",
      ">epoch=2, lrate=0.100, error=0.000\n",
      ">epoch=3, lrate=0.100, error=0.000\n",
      ">epoch=4, lrate=0.100, error=0.000\n",
      "[-0.1, 0.20653640140000007, -0.23418117710000003]\n"
     ]
    }
   ],
   "source": [
    "# Calculate weights\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "           [1.465489372,2.362125076,0],\n",
    "           [3.396561688,4.400293529,0],\n",
    "           [1.38807019,1.850220317,0],\n",
    "           [3.06407232,3.005305973,0],\n",
    "           [7.627531214,2.759262235,1],\n",
    "           [5.332441248,2.088626775,1],\n",
    "           [6.922596716,1.77106367,1],\n",
    "           [8.675418651,-0.242068655,1],\n",
    "           [7.673756466,3.508563011,1]]\n",
    "\n",
    "l_rate = 0.1\n",
    "n_epoch = 5\n",
    "weights = train_weights(dataset, l_rate, n_epoch)\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling the Sonar Dataset.\n",
    "In this section, we will train a Perceptron model using stochastic gradient descent on the Sonar dataset.\n",
    "\n",
    "The dataset is first loaded, the string values converted to numeric and the output column is converted from strings to the integer values of 0 to 1. This is achieved with helper functions __load_csv()__, __str_column_to_float()__ and __str_column_to_int()__ to load and prepare the dataset.\n",
    "\n",
    "We will use k-fold cross validation to estimate the performance of the learned model on unseen data. This means that we will construct and evaluate k models and estimate the performance as the mean model error. Classification accuracy will be used to evaluate each model. These behaviors are provided in the __cross_validation_split()__, __accuracy_metric()__ and __evaluate_algorithm()__ helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron Algorithm on the Sonar Dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with weights\n",
    "def predict(row, weights):\n",
    "    activation = weights[0]\n",
    "    for i in range(len(row)-1):\n",
    "        activation += weights[i + 1] * row[i]\n",
    "    return 1.0 if activation >= 0.0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "def train_weights(train, l_rate, n_epoch):\n",
    "    weights = [0.0 for i in range(len(train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            prediction = predict(row, weights)\n",
    "            error = row[-1] - prediction\n",
    "            weights[0] = weights[0] + l_rate * error\n",
    "            for i in range(len(row)-1):\n",
    "                weights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron Algorithm With Stochastic Gradient Descent\n",
    "def perceptron(train, test, l_rate, n_epoch):\n",
    "    predictions = list()\n",
    "    weights = train_weights(train, l_rate, n_epoch)\n",
    "    for row in test:\n",
    "        prediction = predict(row, weights)\n",
    "        predictions.append(prediction)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [80.76923076923077, 76.92307692307693, 92.3076923076923, 73.07692307692307, 76.92307692307693, 76.92307692307693, 61.53846153846154, 69.23076923076923]\n",
      "Mean Accuracy: 75.962%\n"
     ]
    }
   ],
   "source": [
    "# Test the Perceptron algorithm on the sonar dataset\n",
    "seed(1)\n",
    "\n",
    "# load and prepare data\n",
    "filename = '..\\\\..\\\\..\\\\data\\\\sonar.all.data.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "    str_column_to_float(dataset, i)\n",
    "\n",
    "# convert string class to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "# evaluate algorithm\n",
    "n_folds = 8 # (3)72.947, (5):73.659, (**8**):74.519, (9):72.464, (10):72.000, (12):71.078\n",
    "l_rate = 0.01\n",
    "n_epoch = 300 # (200):73.558, (250):75.962 (**300**):75.962, (400):75.481, (500):74.519, (700):74.519\n",
    "\n",
    "scores = evaluate_algorithm(dataset, perceptron, n_folds, l_rate, n_epoch)\n",
    "\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A k value of 3 was used for cross-validation, giving each fold 208/3 = 69.3 or just under 70 records to be evaluated upon each iteration. A learning rate of 0.1 and 500 training epochs were chosen with a little experimentation.\n",
    "\n",
    "## 5. Extensions\n",
    "This section lists extensions to this tutorial that you may wish to consider exploring.\n",
    "\n",
    "- `Tune The Example`. Tune the learning rate, number of epochs and even data preparation method to get an improved score on the dataset.\n",
    "- `Batch Stochastic Gradient Descent`. Change the stochastic gradient descent algorithm to accumulate updates across each epoch and only update the weights in a batch at the end of the epoch.\n",
    "- `Additional Regression Problems`. Apply the technique to other classification problems on the UCI machine learning repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
