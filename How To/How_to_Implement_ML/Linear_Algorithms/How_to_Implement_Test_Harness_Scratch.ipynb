{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How To Create an Algorithm Test Harness From Scratch With Python\n",
    "\n",
    "by Jason Brownlee on December 11, 2019.[Here](https://machinelearningmastery.com/create-algorithm-test-harness-scratch-python/) in [Code Algorithms From Scratch](https://machinelearningmastery.com/category/algorithms-from-scratch/). [Dataset File](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv)\n",
    "\n",
    "We cannot know which algorithm will be best for a given problem.\n",
    "\n",
    "Therefore, we need to design a test harness that we can use to evaluate different machine learning algorithms.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "- How to implement a train-test algorithm test harness.\n",
    "- How to implement a k-fold cross-validation algorithm test harness.\n",
    "\n",
    "A __test harness__ provides a `consistent way to evaluate machine learning algorithms` on a dataset.\n",
    "\n",
    "It involves 3 elements:\n",
    "\n",
    "1. The __resampling__ method to split-up the dataset.\n",
    "2. The __machine learning algorithm__ to evaluate.\n",
    "3. The __performance measure__ by which to evaluate predictions.\n",
    "\n",
    "__The loading and preparation of a dataset is a prerequisite step that must have been completed prior to using the test harness__.\n",
    "\n",
    "The test harness must allow for different machine learning algorithms to be evaluated, whilst the dataset, resampling method and performance measures are kept constant.\n",
    "\n",
    "The __Zero Rule algorithm__ will be evaluated as part of the tutorial. The Zero Rule algorithm `always predicts the class that has the most observations in the training dataset`.\n",
    "\n",
    "## Tutorial\n",
    "This tutorial is broken down into two main sections:\n",
    "\n",
    "1. Train-Test Algorithm Test Harness.\n",
    "2. Cross-Validation Algorithm Test Harness.\n",
    "3. Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train-Test Algorithm Test Harness.\n",
    "The __train-test split__ is a simple `resampling method` that can be used to evaluate a machine learning algorithm.\n",
    "\n",
    "We need a function that can take a dataset and an algorithm and return a performance score.\n",
    "\n",
    "Below is a function named __evaluate_algorithm()__ that achieves this. It takes __3 fixed arguments__ including the `dataset`, the `algorithm` function and the `split percentage` for the train-test split.\n",
    "\n",
    "1. The dataset is `split into train` and `test` elements. \n",
    "2. `Copy of the test` set is made \n",
    "3. Each `output value is cleared` by setting it to the __None__ value, to prevent the algorithm from cheating accidentally.\n",
    "\n",
    "The algorithm function is expected to return a __list of predictions__, one for each row in the training dataset. These are compared to the actual output values from the unmodified test dataset by the __accuracy_metric()__ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Test Harness\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a CSV file\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset into a train and test set\n",
    "def train_test_split(dataset, split):\n",
    "    train = list()\n",
    "    train_size = split * len(dataset)\n",
    "    dataset_copy = list(dataset)\n",
    "    while len(train) < train_size:\n",
    "        index = randrange(len(dataset_copy))\n",
    "        train.append(dataset_copy.pop(index))\n",
    "    return train, dataset_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean absolute error\n",
    "def mae_metric(actual, predicted):\n",
    "    sum_error = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        sum_error += abs(predicted[i] - actual[i])\n",
    "    return sum_error / float(len(actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate root mean squared error\n",
    "def rmse_metric(actual, predicted):\n",
    "    sum_error = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        prediction_error = predicted[i] - actual[i]\n",
    "        sum_error += (prediction_error ** 2)\n",
    "    mean_error = sum_error / float(len(actual))\n",
    "    return sqrt(mean_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate an algorithm using a train/test split\n",
    "def evaluate_algorithm(dataset, algorithm, split, *args):\n",
    "    train, test = train_test_split(dataset, split)\n",
    "    test_set = list()\n",
    "    for row in test:\n",
    "        row_copy = list(row)\n",
    "        row_copy[-1] = None\n",
    "        test_set.append(row_copy)\n",
    "    predicted = algorithm(train, test_set, *args)\n",
    "    actual = [row[-1] for row in test]\n",
    "\n",
    "    accuracy = accuracy_metric(actual, predicted)\n",
    "    mae = mae_metric(actual, predicted)\n",
    "    rmse = rmse_metric(actual, predicted)\n",
    "    \n",
    "    return accuracy, mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero rule algorithm for classification\n",
    "def zero_rule_algorithm_classification(train, test):\n",
    "    output_values = [row[-1] for row in train]\n",
    "    prediction = max(set(output_values), key=output_values.count)\n",
    "    predicted = [prediction for i in range(len(test))]\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.427%\n",
      "Mean Absolute Error (MAE): 0.326%\n",
      "Root Mean Squared Error (RMSE): 0.571%\n"
     ]
    }
   ],
   "source": [
    "# Test the zero rule algorithm on the diabetes dataset\n",
    "seed(1)\n",
    "\n",
    "# load and prepare data\n",
    "filename = '..\\\\..\\\\..\\\\data\\\\pima-indians-diabetes.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "\n",
    "# evaluate algorithm\n",
    "split = 0.6\n",
    "accuracy, mae, rmse = evaluate_algorithm(dataset, zero_rule_algorithm_classification, split)\n",
    "\n",
    "print('Accuracy: %.3f%%' % (accuracy))\n",
    "print('Mean Absolute Error (MAE): %.3f%%' % (mae))\n",
    "print('Root Mean Squared Error (RMSE): %.3f%%' % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was split into 60% for training the model and 40% for evaluating it.\n",
    "\n",
    "Notice how the name of the Zero Rule algorithm zero_rule_algorithm_classification was passed as an argument to the evaluate_algorithm() function. You can see how this test harness may be used again and again with different algorithms.\n",
    "\n",
    "## 2. Cross-Validation Algorithm Test Harness.\n",
    "\n",
    "__Cross-validation__ is a `resampling technique` that provides more reliable estimates of algorithm performance on unseen data.\n",
    "\n",
    "It requires the creation and evaluation of k models on different subsets of your data, and such is more computationally expensive. Nevertheless, it is the gold standard for evaluating machine learning algorithms.\n",
    "\n",
    "The algorithm must be evaluated on different subsets of the dataset many times. This means we need additional loops within our __evaluate_algorithm()__ function.\n",
    "\n",
    "Below is a function that implements algorithm evaluation with cross-validation.\n",
    "\n",
    "1. The dataset is __split into n_folds groups__ called folds.\n",
    "2. Loop giving each fold an opportunity to be held out of training and used to evaluate the algorithm. \n",
    "3. Copy the list of folds is created and the held out fold is removed from this list. \n",
    "4. Then the list of folds is flattened into one long list of rows to match the algorithms expectation of a training dataset. This is done using the sum() function.\n",
    "5. Once the training dataset is prepared the rest of the function within this loop is as above. A copy of the test dataset (the fold) is made and the output values are cleared to avoid accidental cheating by algorithms. \n",
    "\n",
    "The algorithm is prepared on the train dataset and makes predictions on the test dataset. The predictions are evaluated and stored in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    maes = list()\n",
    "    rmses = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        mae = mae_metric(actual, predicted)\n",
    "        rmse = rmse_metric(actual, predicted)\n",
    "        \n",
    "        scores.append(accuracy)\n",
    "        maes.append(mae)\n",
    "        rmses.append(rmse)\n",
    "    return scores, maes, rmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [62.091503267973856, 64.70588235294117, 64.70588235294117, 64.70588235294117, 69.28104575163398]\n",
      "Mean Accuracy: 65.098%\n",
      "\n",
      "MAE: [0.3790849673202614, 0.35294117647058826, 0.35294117647058826, 0.35294117647058826, 0.30718954248366015]\n",
      "Mean Absolute Error (MAE): 0.349%\n",
      "\n",
      "RMSE: [0.6156987634551992, 0.5940885257860046, 0.5940885257860046, 0.5940885257860046, 0.5542468245138262]\n",
      "Root Mean Squared Error (RMSE): 0.590%\n"
     ]
    }
   ],
   "source": [
    "# Test the zero rule algorithm on the diabetes dataset\n",
    "seed(1)\n",
    "\n",
    "# load and prepare data\n",
    "filename = '..\\\\..\\\\..\\\\data\\\\pima-indians-diabetes.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "scores, maes, rmses = evaluate_algorithm(dataset, zero_rule_algorithm_classification, n_folds)\n",
    "\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/len(scores)))\n",
    "\n",
    "print('\\nMAE: %s' % maes)\n",
    "print('Mean Absolute Error (MAE): %.3f%%' % (sum(maes)/len(maes)))\n",
    "\n",
    "\n",
    "print('\\nRMSE: %s' % rmses)\n",
    "print('Root Mean Squared Error (RMSE): %.3f%%' % (sum(rmses)/len(rmses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extensions\n",
    "This section lists extensions to this tutorial that you may wish to consider.\n",
    "\n",
    "- __Parameterized Evaluation__. Pass in the function used to evaluate predictions, allowing you to seamlessly work with regression problems.\n",
    "- __Parameterized Resampling__. Pass in the function used to calculate resampling splits, allowing you to easily switch between the train-test and cross-validation methods.\n",
    "- __Standard Deviation Scores__. Calculate the standard deviation to get an idea of the spread of scores when evaluating algorithms using cross-validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
