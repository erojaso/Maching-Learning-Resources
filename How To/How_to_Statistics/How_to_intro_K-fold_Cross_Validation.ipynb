{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to k-fold Cross-Validation\n",
    "\n",
    "by Jason Brownlee on August 3, 2020.[Here](https://machinelearningmastery.com/k-fold-cross-validation/) in [Statistics](https://machinelearningmastery.com/category/statistical-methods/)\n",
    "\n",
    "Cross-validation is a statistical method used to estimate the skill of machine learning models.\n",
    "\n",
    "Used in applied machine learning to `compare and select a model` for a given predictive modeling problem because it is easy to understand, easy to implement, and results in skill estimates that generally have a lower bias than other methods.\n",
    "\n",
    "## Tutorial Overview\n",
    "This tutorial is divided into 5 parts; they are:\n",
    "\n",
    "1. k-Fold Cross-Validation\n",
    "    - General procedure\n",
    "2. Configuration of k\n",
    "3. Worked Example\n",
    "4. Cross-Validation API\n",
    "5. Variations on Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. k-Fold Cross-Validation\n",
    "Cross-validation is a __resampling procedure__ used to evaluate machine learning models `on a limited data sample`.\n",
    "\n",
    "The procedure has a `single parameter called k that refers to the number of groups that a given data sample is to be split into`. As such, the procedure is often called k-fold cross-validation.\n",
    "\n",
    "Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.\n",
    "\n",
    "### The general procedure is as follows:\n",
    "\n",
    "1. Shuffle the dataset randomly.\n",
    "2. Split the dataset into k groups. This approach involves randomly dividing the set of observations into k groups, or folds, of approximately equal size. The __first fold__ is treated `as a validation set`, and the method is __fit on the remaining k âˆ’ 1 folds__.\n",
    "3. For each unique group:    \n",
    "    - 3.1 Take the group as a hold out or test data set\n",
    "    - 3.2 Take the remaining groups as a training data set\n",
    "    - 3.3 Fit a model on the training set and evaluate it on the test set\n",
    "    - 3.4 Retain the evaluation score and discard the model\n",
    "4. Summarize the skill of the model using the sample of model evaluation scores\n",
    "\n",
    "__Importantly, each observation in the data sample is assigned to an individual group and stays in that group for the duration of the procedure. This means that each sample is given the opportunity to be used in the hold out set 1 time and used to train the model k-1 times__.\n",
    "\n",
    "It is also __important__ that any `preparation of the data prior to fitting the model occur on the CV-assigned training dataset within the loop rather than on the broader data set`. This also applies to any tuning of hyperparameters. A failure to perform these operations within the loop may result in data leakage and an optimistic estimate of the model skill.\n",
    "\n",
    "The results of a k-fold cross-validation run are often `summarized with the mean of the model skill scores`. It is also good practice to include a measure of the variance of the skill scores, such as the `standard deviation` or `standard error`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration of k\n",
    "The k value must be chosen carefully for your data sample.\n",
    "\n",
    "A __poorly chosen value for k__ may result in a mis-representative idea of the skill of the model, such as a `score with a high variance` (that may change a lot based on the data used to fit the model), or a `high bias`, (such as an overestimate of the skill of the model).\n",
    "\n",
    "Three common tactics for choosing a value for k are as follows:\n",
    "\n",
    "- __Representative__: The value for k is chosen such that each train/test group of data samples is large enough to be statistically representative of the broader dataset.\n",
    "- __k=10__: The value for k is fixed to 10, a value that has been found through experimentation to generally result in a model skill estimate with low bias a modest variance.\n",
    "- __k=n__: The value for k is fixed to n, where n is the size of the dataset to give each test sample an opportunity to be used in the hold out dataset. This approach is called __leave-one-out cross-validation__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Worked Example\n",
    "\n",
    "Imagine we have a data sample with 6 observations:\n",
    "\n",
    "$[0.1, 0.2, 0.3, 0.4, 0.5, 0.6]$\n",
    "\n",
    "The first step is to pick a value for k in order to determine the number of folds used to split the data. Here, we will `use a value of k=3`. That __means we will `shuffle the data` and then split the data into 3 groups__. Because we have 6 observations, each group will have an equal number of 2 observations.\n",
    "For example:\n",
    "\n",
    "- $Fold1: [0.5, 0.2]$\n",
    "- $Fold2: [0.1, 0.3]$\n",
    "- $Fold3: [0.4, 0.6]$\n",
    "\n",
    "Three models are trained and evaluated with each fold given a chance to be the held out test set.\n",
    "For example:\n",
    "\n",
    "- __Model1__: Trained on Fold1 + Fold2, Tested on Fold3\n",
    "- __Model2__: Trained on Fold2 + Fold3, Tested on Fold1\n",
    "- __Model3__: Trained on Fold1 + Fold3, Tested on Fold2\n",
    "\n",
    "The skill scores are collected for each model and summarized for use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-Validation API\n",
    "\n",
    "The scikit-learn library provides an implementation that will split a given data sample up.\n",
    "\n",
    "The KFold() scikit-learn class can be used. It takes as arguments the number of splits, whether or not to shuffle the sample, and the seed for the pseudorandom number generator used prior to the shuffle.\n",
    "\n",
    "Create an instance that splits a dataset into `3 folds`, `shuffles prior` to the split, and uses a value of `1 for the pseudorandom` number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=3, random_state=1, shuffle=True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "kfold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Called repeatedly, the split will return each group of train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0 3 4 5], test: [1 2]\n",
      "train: [1 2 3 5], test: [0 4]\n",
      "train: [0 1 2 4], test: [3 5]\n"
     ]
    }
   ],
   "source": [
    "data = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "# enumerate splits\n",
    "for train, test in kfold.split(data):\n",
    "    print('train: %s, test: %s' % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example prints the specific observations chosen for each train and test set. The indices are used directly on the original data array to retrieve the observation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0.1 0.4 0.5 0.6], test: [0.2 0.3]\n",
      "train: [0.2 0.3 0.4 0.6], test: [0.1 0.5]\n",
      "train: [0.1 0.2 0.3 0.5], test: [0.4 0.6]\n"
     ]
    }
   ],
   "source": [
    "# scikit-learn k-fold cross-validation\n",
    "from numpy import array\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# data sample\n",
    "data = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6])\n",
    "\n",
    "# prepare cross validation\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "# enumerate splits\n",
    "for train, test in kfold.split(data):\n",
    "    print('train: %s, test: %s' % (data[train], data[test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Variations on Cross-Validation\n",
    "There are a number of variations on the k-fold cross validation procedure.\n",
    "\n",
    "Three commonly used variations are as follows:\n",
    "\n",
    "- __Train/Test Split__: Taken to one extreme, `k may be set to 2` (not 1) such that a single train/test split is created to evaluate the model.\n",
    "- __LOOCV__: Taken to another extreme, `k may be set to the total number of observations` in the dataset such that each observation is given a chance to be the held out of the dataset. This is called __leave-one-out cross-validation__, or LOOCV for short.\n",
    "- __Stratified__: The splitting of data into folds may be governed by criteria such as `ensuring that each fold has the same proportion of observations` with a given categorical value, such as the class outcome value. This is called stratified cross-validation.\n",
    "- __Repeated__: This is where the k-fold cross-validation `procedure is repeated n times`, where importantly, the `data sample is shuffled prior to each repetition`, which results in a different split of the sample.\n",
    "- __Nested__: This is where `k-fold cross-validation is performed within each fold of cross-validation, often to perform hyperparameter` tuning during model evaluation. This is called __nested cross-validation__ or __double cross-validation__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Tutorials\n",
    "- [How to Configure k-Fold Cross-Validation](https://machinelearningmastery.com/how-to-configure-k-fold-cross-validation/)\n",
    "- [LOOCV for Evaluating Machine Learning Algorithms](https://machinelearningmastery.com/loocv-for-evaluating-machine-learning-algorithms/)\n",
    "- [Nested Cross-Validation for Machine Learning with Python](https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/)\n",
    "- [Repeated k-Fold Cross-Validation for Model Evaluation in Python](https://machinelearningmastery.com/repeated-k-fold-cross-validation-with-python/)\n",
    "- [How to Fix k-Fold Cross-Validation for Imbalanced Classification](https://machinelearningmastery.com/cross-validation-for-imbalanced-classification/)\n",
    "- [Train-Test Split for Evaluating Machine Learning Algorithms](https://machinelearningmastery.com/train-test-split-for-evaluating-machine-learning-algorithms/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
