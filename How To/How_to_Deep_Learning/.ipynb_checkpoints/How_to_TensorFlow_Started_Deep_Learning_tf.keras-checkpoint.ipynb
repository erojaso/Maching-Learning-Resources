{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 2 Tutorial: Get Started in Deep Learning With tf.keras\n",
    "    \n",
    "by Jason Brownlee on June 21, 2020.[Here](https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/) in [Deep Learning](https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/)\n",
    "\n",
    "Using tf.keras allows you to design, fit, evaluate, and use deep learning models to make predictions in just a few lines of code. It makes common deep learning tasks, such as classification and regression predictive modeling, accessible to average developers looking to get things done.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "- The `difference between Keras and tf.keras` and how to install and confirm TensorFlow is working.\n",
    "- The `5-step life-cycle of tf.keras models` and how to use the sequential and functional APIs.\n",
    "- How to `develop MLP, CNN, and RNN models` with tf.keras for regression, classification, and time series forecasting.\n",
    "- How to use the `advanced features` of the tf.keras API to inspect and diagnose your model.\n",
    "- How to `improve the performance` of your tf.keras model by reducing overfitting and accelerating training.\n",
    "\n",
    "## Overview \n",
    "\n",
    "1. Install TensorFlow and tf.keras\n",
    "    - What Are Keras and tf.keras?\n",
    "    - How to Install TensorFlow\n",
    "     -How to Confirm TensorFlow Is Installed\n",
    "2. Deep Learning Model Life-Cycle\n",
    "    - The 5-Step Model Life-Cycle\n",
    "    - Sequential Model API (Simple)\n",
    "    - Functional Model API (Advanced)\n",
    "3. How to Develop Deep Learning Models\n",
    "    - Develop Multilayer Perceptron Models\n",
    "        - MLP for Binary Classification\n",
    "        - MLP for Multiclass Classification\n",
    "        - MLP for Regression\n",
    "    - Develop Convolutional Neural Network Models\n",
    "    - Develop Recurrent Neural Network Models\n",
    "4. How to Use Advanced Model Features\n",
    "    - How to Visualize a Deep Learning Model\n",
    "    - How to Plot Model Learning Curves\n",
    "    - How to Save and Load Your Model\n",
    "5. How to Get Better Model Performance\n",
    "    - How to Reduce Overfitting With Dropout\n",
    "    - How to Accelerate Training With Batch Normalization\n",
    "    - How to Halt Training at the Right Time With Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install TensorFlow and tf.keras\n",
    "\n",
    "#### 1.1 What Are Keras and tf.keras?\n",
    "\n",
    "In 2019, Google released a new version of their TensorFlow deep learning library (TensorFlow 2) that integrated the Keras API directly and promoted this interface as the default or standard interface for deep learning development on the platform.\n",
    "\n",
    "This integration is commonly referred to as the tf.keras interface or API (“tf” is short for “TensorFlow“). This is to distinguish it from the so-called standalone Keras open source project.\n",
    "\n",
    "- __Standalone Keras__. The standalone open source project that supports TensorFlow, Theano and CNTK backends.\n",
    "- __tf.keras__. The Keras API integrated into TensorFlow 2.\n",
    "\n",
    "The Keras API implementation in Keras is referred to as “tf.keras” because this is the Python idiom used when referencing the API. First, the TensorFlow module is imported and named “tf“; then, Keras API elements are accessed via calls to tf.keras; for example:\n",
    "\n",
    "#### 1.2 How to Install TensorFlow -How to Confirm TensorFlow Is Installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "# validate tf.keras\n",
    "import tensorflow as tf\n",
    "model = tf.keras.Sequential()\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Deep Learning Model Life-Cycle\n",
    "You will discover the life-cycle for a deep learning model and the two tf.keras APIs that you can use to define models.\n",
    "\n",
    "#### 2.1 The 5-Step Model Life-Cycle\n",
    "\n",
    "The five steps in the life-cycle are as follows:\n",
    "\n",
    "- Define the model.\n",
    "- Compile the model.\n",
    "- Fit the model.\n",
    "- Evaluate the model.\n",
    "- Make predictions.\n",
    "\n",
    "##### Define the model.\n",
    "Defining the model requires that you `first select the type of model that you need and then choose the architecture or network topology`.\n",
    "\n",
    "__From an API perspective__, this involves defining the layers of the model, configuring each layer with a number of nodes and activation function, and connecting the layers together into a cohesive model.\n",
    "\n",
    "Models can be defined either with the __Sequential API__ or the __Functional API__, and we will take a look at this in the next section.\n",
    "\n",
    "##### Compile the model.\n",
    "Compiling the model requires that you `first select a loss function that you want to optimize, such as mean squared error or cross-entropy`.\n",
    "\n",
    "It also requires that you select an algorithm to perform the optimization procedure, typically __stochastic gradient descent__, or a modern variation, such as __Adam__. It may also require that you `select any performance metrics to keep track of during the model training process`.\n",
    "\n",
    "__From an API perspective__, this involves calling a function to compile the model with the chosen configuration, which will prepare the appropriate data structures required for the efficient use of the model you have defined.\n",
    "\n",
    "The __optimizer__ can be specified as a string for a known optimizer class, e.g. ‘sgd‘ for stochastic gradient descent, or you can configure an instance of an optimizer class and use that. [Here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n",
    "\n",
    "The three most common loss functions are [Here](https://www.tensorflow.org/api_docs/python/tf/keras/losses):\n",
    "\n",
    "- `binary_crossentropy` for binary classification.\n",
    "- `sparse_categorical_crossentropy` for multi-class classification.\n",
    "- `mse` (mean squared error) for regression.\n",
    "\n",
    "__Metrics__ are defined as a list of strings for known metric functions or a list of functions to call to evaluate predictions. List of supported metrics [Here](https://www.tensorflow.org/api_docs/python/tf/keras/metrics).\n",
    "\n",
    "##### Fit the model.\n",
    "Fitting the model requires that you first `select the training configuration`, such as the __number of epochs__ (loops through the training dataset) and the __batch size__ (number of samples in an epoch used to estimate model error).\n",
    "\n",
    "__From an API perspective__, this `involves calling a function to perform the training process`. This function will block (not return) until the training process has finished.\n",
    "\n",
    "- Batch size Tutorial. [How to Control the Stability of Training Neural Networks With the Batch Size](https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/)\n",
    "\n",
    "##### Evaluate the model.\n",
    "Evaluating the model requires that you `first choose a holdout dataset used to evaluate the model`. This should be data not used in the training process so that we can get an unbiased estimate of the performance of the model when making predictions on new data.\n",
    "\n",
    "__From an API perspective__, this involves calling a function with the holdout dataset and getting a loss and perhaps other metrics that can be reported.\n",
    "\n",
    "##### Make predictions.\n",
    "__From an API perspective__, you simply call a function to `make a prediction of a class label, probability, or numerical value`: whatever you designed your model to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Sequential Model API (Simple)\n",
    "It is referred to as “__sequential__” because `it involves defining a Sequential class and adding layers to the model one by one in a linear manner, from input to output` [Here](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential).\n",
    "\n",
    "The example below defines a __Sequential MLP__ model that accepts __eight inputs__, has __one hidden layer with 10 nodes__ and then an __output layer with 1 node__ to predict a numerical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a model defined with the sequential api\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_shape=(8,)))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the visible layer of the network is defined by the “input_shape” argument on the first hidden layer. That means in the above example, the model expects the input for one sample to be a vector of eight numbers.\n",
    "\n",
    "The sequential API is easy to use because you keep calling model.add() until you have added all of your layers.\n",
    "\n",
    "For example, here is a deep MLP with five hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a model defined with the sequential api\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(8,)))\n",
    "model.add(Dense(80))\n",
    "model.add(Dense(30))\n",
    "model.add(Dense(10))\n",
    "model.add(Dense(5))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Functional Model API (Advanced)\n",
    "\n",
    "It involves explicitly connecting the output of one layer to the input of another layer. Each connection is specified.\n",
    "\n",
    "- First, an input layer must be defined via the Input class, and the shape of an input sample is specified. \n",
    "- Fully connected layer can be connected to the input by calling the layer and passing the input layer.\n",
    "- We can then connect this to an output layer in the same manner.\n",
    "- Once connected, we define a Model object and specify the input and output layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a model defined with the functional api\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# define the input layers\n",
    "x_in = Input(shape=(8,))\n",
    "\n",
    "# calling a leyer\n",
    "x = Dense(10)(x_in)\n",
    "\n",
    "# conect input layer to output\n",
    "x_out = Dense(1)(x)\n",
    "\n",
    "# define the model\n",
    "model = Model(inputs=x_in, outputs=x_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, it allows for more complicated model designs, such as models that may have multiple input paths (separate vectors) and models that have multiple output paths (e.g. a word and a number).\n",
    "\n",
    "The functional API can be a lot of fun when you get used to it. [Here](https://www.tensorflow.org/guide/keras/functional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How to Develop Deep Learning Models\n",
    "\n",
    "Discover how to `develop`, `evaluate`, and `make predictions` with standard deep learning models, including __Multilayer Perceptrons__ (MLP), __Convolutional Neural Networks__ (CNNs), and __Recurrent Neural Networks__ (RNNs).\n",
    "\n",
    "#### 3.1 Develop Multilayer Perceptron Models (MLP)\n",
    "\n",
    "Standard fully connected neural network model.\n",
    "\n",
    "It is comprised of `layers of nodes where each node is connected to all outputs from the previous layer and the output of each node is connected to all inputs` for nodes in the next layer.\n",
    "\n",
    "An MLP is `created by with one or more Dense layers`. This model is __appropriate for tabular data__, that is data as it looks in a table or spreadsheet with one column for each variable and one row for each variable. There are three `predictive modeling problems` you may want to explore with an MLP; they are binary __classification__, __multiclass classification__, and __regression__.\n",
    "\n",
    "#### 3.1.1 MLP for Binary Classification\n",
    "\n",
    "We will use the Ionosphere binary (two-class) classification dataset to demonstrate an MLP for binary classification.\n",
    "\n",
    "The dataset will be downloaded\n",
    "- Ionosphere Dataset ([csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv)).\n",
    "- Ionosphere Dataset Description ([csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.names)).\n",
    "\n",
    "We will use a LabelEncoder to encode the string labels to integer values 0 and 1. The model will be fit on 67 percent of the data, and the remaining 33 percent will be used for evaluation, split using the train_test_split() function.\n",
    "\n",
    "\n",
    "It is a good practice to use ‘__relu__‘ activation with a ‘__he_normal__‘ weight initialization. This `combination goes a long way to overcome the problem of vanishing gradients when training deep neural network models`.\n",
    "\n",
    "- A Gentle Introduction to the Rectified Linear Unit ([ReLU](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/)) \n",
    "\n",
    "The model predicts the probability of class 1 and uses the sigmoid activation function. \n",
    "- The model is optimized using the [adam version of stochastic gradient descent](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/).\n",
    "- The model seeks to minimize the [cross-entropy loss](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(351, 35)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mlp for binary classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# load the dataset\n",
    "# path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'\n",
    "path = '..\\\\..\\\\data\\\\ionosphere.data.csv'\n",
    "df = read_csv(path, header=None)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input and output columns\n",
    "X, y = df.values[:, :-1], df.values[:, -1]\n",
    "\n",
    "# ensure all data are floating point values\n",
    "X = X.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode strings to integer\n",
    "y = LabelEncoder().fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(235, 34) (116, 34) (235,) (116,)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the number of input features\n",
    "n_features = X_train.shape[1]\n",
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "model.add(Dense( 8, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense( 1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ab51774348>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.135\n",
      "Test Accuracy: 0.931\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss: %.3f' % loss)\n",
    "print('Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9805202]]\n",
      "Predicted: 0.981\n"
     ]
    }
   ],
   "source": [
    "# make a prediction\n",
    "row = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n",
    "yhat = model.predict([row])\n",
    "\n",
    "print(yhat)\n",
    "print('Predicted: %.3f' % yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 MLP for Multiclass Classification\n",
    "We will use the Iris flowers multiclass classification dataset to demonstrate an MLP for multiclass classification.\n",
    "\n",
    "he dataset will be downloaded\n",
    "- Iris Dataset ([csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv)).\n",
    "- Iris Dataset Description ([csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.names)).\n",
    "\n",
    "Given that it is a multiclass classification, the model __must have one node for each class in the output layer and use the softmax activation function__. The loss function is the ‘__sparse_categorical_crossentropy__‘, which is appropriate for integer encoded class labels (e.g. 0 for one class, 1 for the next class, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mlp for multiclass classification\n",
    "from numpy import argmax\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# load the dataset\n",
    "##path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv'\n",
    "path = '..\\\\..\\\\data\\\\iris.data.csv'\n",
    "df = read_csv(path, header=None)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
       "       'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
       "       'Iris-setosa', 'Iris-setosa'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into input and output columns\n",
    "X, y = df.values[:, :-1], df.values[:, -1]\n",
    "\n",
    "# ensure all data are floating point values\n",
    "X = X.astype('float32')\n",
    "y[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode strings to integer\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "y[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4) (50, 4) (100,) (50,)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the number of input features\n",
    "n_features = X_train.shape[1]\n",
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "model.add(Dense( 8, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense( 3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ab532483c8>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.624\n",
      "Test Accuracy: 0.760\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print('Test Loss: %.3f' % loss)\n",
    "print('Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[0.78086895 0.21446277 0.00466831]] (class=0)\n"
     ]
    }
   ],
   "source": [
    "# make a prediction\n",
    "row = [5.1,3.5,1.4,0.2]\n",
    "yhat = model.predict([row])\n",
    "\n",
    "print('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 MLP for Regression\n",
    "We will use the Boston housing regression dataset to demonstrate an MLP for regression predictive modeling.\n",
    "\n",
    "The dataset will be downloaded\n",
    "- Boston Housing Dataset ([csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv)).\n",
    "- Boston Housing Dataset Description ([csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.names)).\n",
    "\n",
    "This is a regression problem that involves predicting a single numerical value. As such, the output layer has a single node and uses the default or linear activation function (no activation function). The mean squared error (mse) loss is minimized when fitting the model.\n",
    "\n",
    "__Recall that this is a regression, not classification; therefore, we cannot calculate classification accuracy__.\n",
    "- Tutorial. [Difference Between Classification and Regression in Machine Learning](https://machinelearningmastery.com/classification-versus-regression-in-machine-learning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 14)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mlp for regression\n",
    "from numpy import sqrt\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# load the dataset\n",
    "#path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "path = '..\\\\..\\\\data\\\\housing.data.csv'\n",
    "df = read_csv(path, header=None)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 13) (167, 13) (339,) (167,)\n"
     ]
    }
   ],
   "source": [
    "# split into input and output columns\n",
    "X, y = df.values[:, :-1], df.values[:, -1]\n",
    "\n",
    "# split into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the number of input features\n",
    "n_features = X_train.shape[1]\n",
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "model.add(Dense( 8, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense( 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ab53541ec8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 49.971, RMSE: 7.069\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "error = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('MSE: %.3f, RMSE: %.3f' % (error, sqrt(error)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 29.706\n"
     ]
    }
   ],
   "source": [
    "# make a prediction\n",
    "row = [0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,396.90,4.98]\n",
    "yhat = model.predict([row])\n",
    "print('Predicted: %.3f' % yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that the model achieved an MSE of about 60 which is an RMSE of about `7 (units are thousands of dollars)`. A value of about 26 is then predicted for the single example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Develop Convolutional Neural Network Models (CNN)\n",
    "Convolutional Neural Networks, or CNNs for short, are a type of network `designed for image input`.\n",
    "\n",
    "They are comprised of models with [convolutional layers](https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/) that extract features (called feature maps) and [pooling layers](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/) that distill features down to the most salient elements.\n",
    "\n",
    "A popular image classification task is the __MNIST handwritten digit classification__. It involves tens of thousands of handwritten digits that must be classified as a number between 0 and 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a cnn for image classification\n",
    "from numpy import asarray\n",
    "from numpy import unique\n",
    "from numpy import argmax\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# MNIST handwritten digit classification\n",
    "from tensorflow.keras.datasets.mnist import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1) 10\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "\n",
    "# reshape data to have a single channel\n",
    "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], x_train.shape[2], 1))\n",
    "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], x_test.shape[2], 1))\n",
    "\n",
    "# determine the shape of the input images\n",
    "in_shape = x_train.shape[1:]\n",
    "\n",
    "# determine the number of classes\n",
    "n_classes = len(unique(y_train))\n",
    "print(in_shape, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize pixel values\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', kernel_initializer='he_uniform', input_shape=in_shape))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss and optimizer\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.450\n",
      "Accuracy: 0.128\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Loss: %.3f' % loss)\n",
    "print('Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: class=2\n"
     ]
    }
   ],
   "source": [
    "# make a prediction\n",
    "image = x_train[0]\n",
    "yhat = model.predict(asarray([image]))\n",
    "print('Predicted: class=%d' % argmax(yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that the model achieved a classification accuracy of about 98 percent on the test dataset. We can then see that the model predicted class 5 for the first image in the training set.\n",
    "\n",
    "#### 3.3 Develop Recurrent Neural Network Models (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
