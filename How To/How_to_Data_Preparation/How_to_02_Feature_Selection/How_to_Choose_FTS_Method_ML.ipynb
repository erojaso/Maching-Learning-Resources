{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Choose a Feature Selection Method For Machine Learning\n",
    "by Jason Brownlee on June 30, 2020. [Here](https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/) in [Data Preparation](https://machinelearningmastery.com/category/data-preparation/)\n",
    "\n",
    "Feature selection is the process of `reducing the number of input variables` when developing a predictive model.\n",
    "\n",
    "It is desirable to reduce the number of input variables to both `reduce the computational cost` of modeling and, in some cases, to `improve the performance` of the model.\n",
    "\n",
    "In this post, you will discover how to choose statistical measures for filter-based feature selection with `numerical` and `categorical` data.\n",
    "\n",
    "After reading this post, you will know:\n",
    "\n",
    "- There are two main types of feature selection techniques: `supervised` and `unsupervised`, and supervised methods may be divided into *wrapper*, *filter* and *intrinsic*.\n",
    "- Filter-based feature selection methods use statistical measures to score the correlation or dependence between input variables that can be filtered to choose the most relevant features.\n",
    "- Statistical measures for feature *`selection must be carefully chosen based on the data type of the input variable and the output or response variable`*.\n",
    "\n",
    "## Overview\n",
    "This tutorial is divided into 4 parts; they are:\n",
    "\n",
    "1. Feature Selection Methods\n",
    "2. Statistics for Filter Feature Selection Methods\n",
    "    1. Numerical Input, Numerical Output\n",
    "    2. Numerical Input, Categorical Output\n",
    "    3. Categorical Input, Numerical Output\n",
    "    4. Categorical Input, Categorical Output\n",
    "3. Tips and Tricks for Feature Selection\n",
    "    1. Correlation Statistics\n",
    "    2. Selection Method\n",
    "    3. Transform Variables\n",
    "    4. What Is the Best Method?\n",
    "4. Worked Examples\n",
    "    - 1. Regression Feature Selection (Numerical Input, Numerical Output)\n",
    "    - 2.a Classification Feature Selection (Numerical Input, Categorical Output)\n",
    "    - 2.b Classification Feature Selection (Categorical Input, Categorical Output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Selection Methods\n",
    "Feature selection methods are intended to reduce the number of input variables to those that are believed to be most useful to a model in order to predict the target variable.\n",
    "\n",
    "Many models, especially those based on `regression slopes` and `intercepts`, will estimate parameters for every term in the model. Because of this, the presence of non-informative -*that are not relevant to the target variable*- variables can add uncertainty to the predictions and reduce the overall effectiveness of the model.\n",
    "\n",
    "We can summarize feature selection as follows.\n",
    "\n",
    "- Feature Selection: Select a subset of input features from the dataset.\n",
    "    - Unsupervised: Do not use the target variable (e.g. remove redundant variables).\n",
    "        - Correlation\n",
    "    - Supervised: Use the target variable (e.g. remove irrelevant variables).\n",
    "        - Wrapper: Search for well-performing subsets of features (maximizes model performance).\n",
    "            - RFE\n",
    "        - Filter: Select subsets of features based on their relationship with the target.\n",
    "            - Statistical Methods\n",
    "            - Feature Importance Methods\n",
    "        - Intrinsic: Algorithms that perform automatic feature selection during training (intrinsically conduct feature selection).\n",
    "            - Decision Trees\n",
    "            - Rule-based models, MARS, randon forest, and the lasso, for example. \n",
    "- Dimensionality Reduction: Project input data into a lower-dimensional feature space (create a projection of the data resulting in entirely new input features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistics for Filter Feature Selection Methods\n",
    "It is common to use correlation type statistical measures between input and output variables as the basis for filter feature selection. Statistical measures is highly dependent upon the variable data types.\n",
    "\n",
    "Common input variable data types:\n",
    "\n",
    "- Numerical Variables\n",
    "    - Integer Variables (1, 2, 3).\n",
    "    - Floating Point Variables (0.1, 0.2, 0.3).\n",
    "- Categorical Variables.\n",
    "    - Boolean Variables (dichotomous) (True, False).\n",
    "    - Ordinal Variables (1st, 2nd, 3rd).\n",
    "    - Nominal Variables (r, g, b).\n",
    "\n",
    "`Input variables` are those that are provided as input to a model. In feature selection, it is this group of variables that we wish to reduce in size. `Output variables` are those for which a model is intended to predict, often called the response variable.    \n",
    "\n",
    "The type of response variable typically indicates the type of predictive modeling problem being performed.\n",
    "- `Numerical Output`: Regression predictive modeling problem.\n",
    "- `Categorical Output`: Classification predictive modeling problem.\n",
    "\n",
    "Most of these techniques are `univariate`, *`meaning that they evaluate each predictor in isolation`*. In this case, the existence of correlated predictors makes it possible to select important, but redundant, predictors. The obvious consequences of this issue are that too many predictors are chosen and, as a result, `collinearity problems arise`.\n",
    "\n",
    "Univariate statistical measures used for filter-based feature selection.\n",
    "- Input Variable\n",
    "    - Numerical\n",
    "        - Output Variable\n",
    "            - Numerical (`regression` predictive modeling)\n",
    "                - Pearson’s correlation coefficient (linear).\n",
    "                - Spearman’s rank coefficient (nonlinear)                \n",
    "            - Categorical (`classification` predictive modeling)\n",
    "                - ANOVA correlation coefficient (linear).\n",
    "                - Kendall’s rank coefficient (nonlinear) (assume that the categorical variable is ordinal).\n",
    "    - Categorical\n",
    "        - Output Variable\n",
    "            - Numerical (`regression` predictive modeling - *not encounter it often*)\n",
    "                - ANOVA (use “Numerical Input, Categorical Output” methodsin reverse)\n",
    "                - Kendall’s (use “Numerical Input, Categorical Output” methodsin reverse)\n",
    "            - Categorical (`classification` predictive modeling)\n",
    "                - Chi-Squared (contingency tables)\n",
    "                - Mutual Information (agnostic to the data types, usefull for categorical and numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tips and Tricks for Feature Selection\n",
    "Some additional considerations when using filter-based feature selection\n",
    "\n",
    "### 1. Correlation Statistics\n",
    "The scikit-learn library provides\n",
    "- Pearson’s Correlation Coefficient: [f_regression()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html)\n",
    "- ANOVA: [f_classif()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html)\n",
    "- Chi-Squared: [chi2()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html)\n",
    "- Mutual Information: [mutual_info_classif()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html) and [mutual_info_regression()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html)\n",
    "\n",
    "The SciPy library provides\n",
    "- Kendall’s tau ([kendalltau](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kendalltau.html))\n",
    "- Spearman’s rank correlation ([spearmanr](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html)).\n",
    "\n",
    "### 2. Selection Method\n",
    "The scikit-learn library provides \n",
    "- Select the top k variables: [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)(most usefull)\n",
    "- Select the top percentile variables: [SelectPercentile](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html)\n",
    "\n",
    "### 3. Transform Variables\n",
    "Consider transforming the variables in order to access different statistical methods.\n",
    "- Transform a `categorical` variable to `ordinal`\n",
    "- Transform `numerical` variable discrete (e.g. bins); try `categorical-based` measures\n",
    "- Some statistical measures assume properties of the variables.\n",
    "    - Pearson’s assumes `Gaussian probability distribution` and a linear relationship\n",
    "\n",
    "### 4. What Is the Best Method?\n",
    "- There is no best feature selection method.\n",
    "- Just like there is no best set of input variables or best machine learning algorithm. \n",
    "- Use careful systematic experimentation. Try a range of `different models fit` on `different subsets of features` chosen via `different statistical measures` and discover what works best for your specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Worked Examples\n",
    "This section provides worked examples of feature selection cases that you can use as a starting point.\n",
    "\n",
    "### 1. Regression Feature Selection\n",
    "#### (Numerical Input, Numerical Output)\n",
    "\n",
    "Feature selection is performed using `Pearson’s Correlation Coefficient` via the `f_regression()` function.\n",
    "\n",
    "`Note`. Running the example first creates the regression dataset, then defines the feature selection and applies the feature selection procedure to the dataset, returning a subset of the selected input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1000, 10)\n"
    }
   ],
   "source": [
    "# pearson's correlation feature selection for numeric input and numeric output\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "# generate dataset sample\n",
    "X, y = make_regression(n_samples=1000, n_features=100, n_informative=10)\n",
    "\n",
    "# define feature selection\n",
    "fs = SelectKBest(score_func=f_regression, k=10)\n",
    "\n",
    "# apply feature selection\n",
    "X_selected = fs.fit_transform(X, y)\n",
    "print(X_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.a Classification Feature Selection\n",
    "#### (Numerical Input, Categorical Output)\n",
    "\n",
    "Feature selection is performed using `ANOVA F measure` via the `f_classif()` function.\n",
    "\n",
    "`Note`. Running the example first creates the classification dataset, then defines the feature selection and applies the feature selection procedure to the dataset, returning a subset of the selected input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1000, 2)\n"
    }
   ],
   "source": [
    "# ANOVA feature selection for numeric input and categorical output\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# generate dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2)\n",
    "\n",
    "# define feature selection\n",
    "fs = SelectKBest(score_func=f_classif, k=2)\n",
    "\n",
    "# apply feature selection\n",
    "X_selected = fs.fit_transform(X, y)\n",
    "print(X_selected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.b Classification Feature Selection:\n",
    "#### (Categorical Input, Categorical Output)\n",
    "Feature selection with categorical inputs and categorical outputs\n",
    "\n",
    "- [How to Perform Feature Selection with Categorical Data](https://machinelearningmastery.com/feature-selection-with-categorical-data/).\n",
    "\n",
    "\n",
    "## Tutorials\n",
    "- [How to Calculate Nonparametric Rank Correlation in Python](https://machinelearningmastery.com/how-to-calculate-nonparametric-rank-correlation-in-python/)\n",
    "- [How to Calculate Correlation Between Variables in Python](https://machinelearningmastery.com/how-to-use-correlation-to-understand-the-relationship-between-variables/)\n",
    "- [Feature Selection For Machine Learning in Python](https://machinelearningmastery.com/feature-selection-machine-learning-python/)\n",
    "- [An Introduction to Feature Selection](https://machinelearningmastery.com/an-introduction-to-feature-selection/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594181510535",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}