{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Dimensionality Reduction for Machine Learning\n",
    "\n",
    "by Jason Brownlee on June 30, 2020.[Here](https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/) in [Data Preparation](https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/)\n",
    "\n",
    "The number of input variables or features for a dataset is referred to as its dimensionality.\n",
    "\n",
    "Dimensionality reduction refers to techniques that reduce the number of input variables in a dataset.\n",
    "\n",
    "After reading this post, you will know:\n",
    "\n",
    "- Large numbers of input features can cause poor performance for machine learning algorithms.\n",
    "- Dimensionality reduction is a general field of study concerned with reducing the number of input features.\n",
    "- Dimensionality reduction methods include feature selection, linear algebra methods, projection methods, and autoencoders.\n",
    "\n",
    "## Overview\n",
    "This tutorial is divided into three parts; they are:\n",
    "\n",
    "1. Problem With Many Input Variables\n",
    "2. Dimensionality Reduction\n",
    "3. Techniques for Dimensionality Reduction\n",
    "    3.1. Feature Selection Methods\n",
    "    3.2. Matrix Factorization\n",
    "    3.3. Manifold Learning\n",
    "    3.4. Autoencoder Methods\n",
    "    3.5. Tips for Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem With Many Input Variables\n",
    "\n",
    "The performance of machine learning algorithms can degrade with too many input variables.\n",
    "\n",
    "The input variables `are the columns` that are fed as input to a model to predict the target variable. Input variables are also `called features`.\n",
    "\n",
    "Having a large number of dimensions in the feature space can mean that the volume of that space is very large, and in turn, the points that we have in that space (rows of data) often represent a small and non-representative sample.\n",
    "\n",
    "Therefore, it is often desirable to reduce the number of input features.\n",
    "\n",
    "This reduces the number of dimensions of the feature space, hence the name “dimensionality reduction.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction refers to `techniques for reducing the number of input variables` in training data.\n",
    "\n",
    "*When dealing with high dimensional data, it is often useful to reduce the dimensionality by projecting the data to a lower dimensional subspace which captures the “essence” of the data. This is called dimensionality reduction*.\n",
    "\n",
    "Fewer input dimensions often mean correspondingly fewer parameters or a simpler structure in the machine learning model, referred to as __degrees of freedom__. `A model with too many degrees of freedom is likely to overfit the training dataset and therefore may not perform well on new data`.\n",
    "\n",
    "it is `desirable to have simple models that generalize well`, and in turn, input data with few input variables. This is particularly true for __linear models__ where the `number of inputs and the degrees of freedom of the model are often closely related`.\n",
    "\n",
    "Dimensionality reduction is a data preparation technique performed on data prior to modeling. `It might be performed after data cleaning and data scaling and before training a predictive model`.\n",
    "\n",
    "As such, any __dimensionality reduction performed__ on `training data must also` be performed on `new data`, such as a `test dataset`, `validation dataset`, and `data` when making a prediction with the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Techniques for Dimensionality Reduction\n",
    "### 3.1. Feature Selection Methods\n",
    "\n",
    "Perhaps the most common are so-called feature selection techniques that use scoring or statistical methods to select which features to keep and which features to delete. \n",
    "\n",
    "Perform feature selection, to `remove “irrelevant” features that do not help much with the classification problem`.\n",
    "\n",
    "Two main classes of feature selection techniques include `wrapper methods` and `filter methods`.\n",
    "\n",
    "- __Wrapper methods__. Wrap a machine learning model, fitting and `evaluating the model with different subsets of input features` and selecting the subset the results in the best model performance. __RFE__ is an example of a wrapper feature selection method.\n",
    "\n",
    "- __Filter methods__ `use scoring methods, like correlation between the feature and the target variable`, to select a subset of input features that are most predictive. Examples include __Pearson’s correlation__ and __Chi-Squared__ test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Matrix Factorization\n",
    "\n",
    "__Note__. `assume that all input features have the same scale or distribution`.\n",
    "\n",
    "Techniques from `linear algebra` can be used for dimensionality reduction.\n",
    "\n",
    "Specifically, `matrix factorization methods` can be used to reduce a dataset matrix into its constituent parts.\n",
    "\n",
    "Examples include the __eigendecomposition__ and __singular value decomposition__.\n",
    "\n",
    "- __Matrix factorization__. [A Gentle Introduction to Matrix Factorization for Machine Learning](https://machinelearningmastery.com/introduction-to-matrix-decompositions-for-machine-learning/)\n",
    "\n",
    "The parts can then be `ranked` and a subset of those parts can be selected that best captures the salient structure of the matrix that can be used to represent the dataset.\n",
    "\n",
    "The most common method for ranking the components is __principal components analysis__, or PCA for short.\n",
    "\n",
    "- __PCA__. [How to Calculate Principal Component Analysis (PCA) From Scratch in Python](https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Manifold Learning\n",
    "\n",
    "__Note__. `assume that all input features have the same scale or distribution`.\n",
    "\n",
    "Techniques from `high-dimensionality statistics` can also be used for dimensionality reduction.\n",
    "\n",
    "These techniques are sometimes referred to as “__manifold learning__” and are `used to create a low-dimensional projection of high-dimensional data, often for the purposes of data visualization`.\n",
    "\n",
    "The projection is designed to both create a low-dimensional representation of the dataset whilst best preserving the salient structure or relationships in the data.\n",
    "\n",
    "Examples of manifold learning techniques include:\n",
    "\n",
    "- Kohonen Self-Organizing Map (__SOM__) [Here](https://en.wikipedia.org/wiki/Self-organizing_map).\n",
    "- Sammons Mapping [Here](https://en.wikipedia.org/wiki/Sammon_mapping).\n",
    "- Multidimensional Scaling (__MDS__) [Here]().\n",
    "- t-distributed Stochastic Neighbor Embedding (__t-SNE__) [Here]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Autoencoder Methods\n",
    "\n",
    "Deep learning neural networks can be constructed to perform dimensionality reduction.\n",
    "\n",
    "A popular approach is called __autoencoders__. This `involves framing a self-supervised learning problem where a model must reproduce the input correctly`.\n",
    "\n",
    "A network model is used that seeks to compress the data flow to a bottleneck layer with far fewer dimensions than the original input data. The `part of the model prior to and including the bottleneck` is referred to as the __encoder__, and the `part of the model that reads the bottleneck output and reconstructs the input` is called the __decoder__.\n",
    "\n",
    "After training, the decoder is discarded and the output from the bottleneck is used directly as the reduced dimensionality of the input. Inputs transformed by this encoder can then be fed into another model, not necessarily a neural network model.\n",
    "\n",
    "The output of the __encoder__ `is a type of projection`, and like other projection methods, `there is no direct relationship to the bottleneck output back to the original input variables`, making them challenging to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Tips for Dimensionality Reduction\n",
    "\n",
    "There is no best technique for dimensionality reduction\n",
    "\n",
    "The best approach is to `use systematic controlled experiments to discover what dimensionality reduction techniques`, when paired with your model of choice, result in the best performance on your dataset.\n",
    "\n",
    "Typically, __linear algebra__ and __manifold learning__ methods `assume that all input features have the same scale or distribution`. This suggests that it is __good practice to either normalize or standardize data prior to using these methods__ if the input variables have differing scales or units."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
