{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Handle Missing Data with Python\n",
    "by Jason Brownlee on June 30, 2020. [Reference](https://machinelearningmastery.com/handle-missing-data-python/)\n",
    "\n",
    "Handling missing data is important as many machine learning algorithms do not support data with missing values.\n",
    "\n",
    "After completing this tutorial you will know:\n",
    "\n",
    "- How to marking invalid or corrupt values as missing in your dataset.\n",
    "- How to remove rows with missing data from your dataset.\n",
    "- How to impute missing values with mean values in your dataset.\n",
    "\n",
    "## Overview\n",
    "This tutorial is divided into 6 parts:\n",
    "\n",
    "1. `Diabetes Dataset`: where we look at a dataset that has known missing values.\n",
    "2. `Mark Missing Values`: where we learn how to mark missing values in a dataset.\n",
    "3. `Missing Values Causes Problems`: where we see how a machine learning algorithm can fail when it contains missing values.\n",
    "4. `Remove Rows With Missing Values`: where we see how to remove rows that contain missing values.\n",
    "5. `Impute Missing Values`: where we replace missing values with sensible values.\n",
    "6. `Algorithms that Support Missing Values`: where we learn about algorithms that support missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Diabetes Dataset\n",
    "The Diabetes Dataset involves predicting the onset of diabetes within 5 years in given medical details.\n",
    "\n",
    "- [Dataset File](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv).\n",
    "- [Dataset Details](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names).\n",
    "\n",
    "It is a binary (2-class) classification problem. The number of observations for each class *`is not balanced`*. There are 768 observations with 8 input variables and 1 output variable. The variable names are as follows:\n",
    "\n",
    "- 0. Number of times pregnant.\n",
    "- 1. Plasma glucose concentration a 2 hours in an oral glucose tolerance test.\n",
    "- 2. Diastolic blood pressure (mm Hg).\n",
    "- 3. Triceps skinfold thickness (mm).\n",
    "- 4. 2-Hour serum insulin (mu U/ml).\n",
    "- 5. Body mass index (weight in kg/(height in m)^2).\n",
    "- 6. Diabetes pedigree function.\n",
    "- 7. Age (years).\n",
    "- 8. Class variable (0 or 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mark Missing Values\n",
    "Most data has missing values, and the likelihood of having missing values increases with the size of the dataset.\n",
    "\n",
    "In this section, we will look at how we can identify and mark values as missing.\n",
    "\n",
    "We can use plots and summary statistics to help identify missing or corrupt data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(768, 9)\n                0           1           2           3           4           5  \\\ncount  768.000000  768.000000  768.000000  768.000000  768.000000  768.000000   \nmean     3.845052  120.894531   69.105469   20.536458   79.799479   31.992578   \nstd      3.369578   31.972618   19.355807   15.952218  115.244002    7.884160   \nmin      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   \n50%      3.000000  117.000000   72.000000   23.000000   30.500000   32.000000   \n75%      6.000000  140.250000   80.000000   32.000000  127.250000   36.600000   \nmax     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   \n\n                6           7           8  \ncount  768.000000  768.000000  768.000000  \nmean     0.471876   33.240885    0.348958  \nstd      0.331329   11.760232    0.476951  \nmin      0.078000   21.000000    0.000000  \n25%      0.243750   24.000000    0.000000  \n50%      0.372500   29.000000    0.000000  \n75%      0.626250   41.000000    1.000000  \nmax      2.420000   81.000000    1.000000  \n"
    }
   ],
   "source": [
    "# load and summarize the dataset\n",
    "from pandas import read_csv\n",
    "\n",
    "# load the dataset\n",
    "dataset = read_csv('..//..//..//data/pima-indians-diabetes.csv', header=None)\n",
    "\n",
    "# summarize the dataset\n",
    "print(dataset.shape)\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are columns that have a minimum value of zero (0). On some columns, a value of zero does not make sense and indicates an invalid or missing value.\n",
    "\n",
    "Specifically, the following columns have an invalid zero minimum value:\n",
    "\n",
    "- 1: Plasma glucose concentration\n",
    "- 2: Diastolic blood pressure\n",
    "- 3: Triceps skinfold thickness\n",
    "- 4: 2-Hour serum insulin\n",
    "- 5: Body mass index\n",
    "\n",
    "Confirm printing first 20 rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0    1   2   3    4     5      6   7  8\n0    6  148  72  35    0  33.6  0.627  50  1\n1    1   85  66  29    0  26.6  0.351  31  0\n2    8  183  64   0    0  23.3  0.672  32  1\n3    1   89  66  23   94  28.1  0.167  21  0\n4    0  137  40  35  168  43.1  2.288  33  1\n5    5  116  74   0    0  25.6  0.201  30  0\n6    3   78  50  32   88  31.0  0.248  26  1\n7   10  115   0   0    0  35.3  0.134  29  0\n8    2  197  70  45  543  30.5  0.158  53  1\n9    8  125  96   0    0   0.0  0.232  54  1\n10   4  110  92   0    0  37.6  0.191  30  0\n11  10  168  74   0    0  38.0  0.537  34  1\n12  10  139  80   0    0  27.1  1.441  57  0\n13   1  189  60  23  846  30.1  0.398  59  1\n14   5  166  72  19  175  25.8  0.587  51  1\n15   7  100   0   0    0  30.0  0.484  32  1\n16   0  118  84  47  230  45.8  0.551  31  1\n17   7  107  74   0    0  29.6  0.254  31  1\n18   1  103  30  38   83  43.3  0.183  33  0\n19   1  115  70  30   96  34.6  0.529  32  1\n"
    }
   ],
   "source": [
    "# print the first 20 rows of data\n",
    "print(dataset.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a count of the number of missing values on each of these columns. We can do this my marking all of the values in the subset of the DataFrame we are interested in that have zero values as True. We can then count the number of true values in each column.\n",
    "\n",
    "We can do this my marking all of the values in the subset of the DataFrame we are interested in that have zero values as True. We can then count the number of true values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1      5\n2     35\n3    227\n4    374\n5     11\ndtype: int64\n1     0.651042\n2     4.557292\n3    29.557292\n4    48.697917\n5     1.432292\ndtype: float64\n"
    }
   ],
   "source": [
    "# count the number of missing values for each column\n",
    "num_missing = (dataset[[1,2,3,4,5]] == 0).sum()\n",
    "\n",
    "# get % of missing values for each column\n",
    "num_missing_prc = ((dataset[[1,2,3,4,5]] == 0).sum()/768) * 100\n",
    "\n",
    "# report the results\n",
    "print(num_missing)\n",
    "print(num_missing_prc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This highlights that different “missing value” strategies may be needed for different columns, e.g. to ensure that there are still a sufficient number of records left to train a predictive model.\n",
    "\n",
    "In Python, specifically Pandas, NumPy and Scikit-Learn, `we mark missing values as NaN`.\n",
    "\n",
    "Values with a NaN value are ignored from operations like sum, count, etc.\n",
    "\n",
    "After we have marked the missing values, we can use the isnull() function to mark all of the NaN values in the dataset as True and get a count of the missing values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0      0\n1      5\n2     35\n3    227\n4    374\n5     11\n6      0\n7      0\n8      0\ndtype: int64\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   0      1     2     3      4     5      6   7  8\n0  6  148.0  72.0  35.0    NaN  33.6  0.627  50  1\n1  1   85.0  66.0  29.0    NaN  26.6  0.351  31  0\n2  8  183.0  64.0   NaN    NaN  23.3  0.672  32  1\n3  1   89.0  66.0  23.0   94.0  28.1  0.167  21  0\n4  0  137.0  40.0  35.0  168.0  43.1  2.288  33  1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>6</td>\n      <td>148.0</td>\n      <td>72.0</td>\n      <td>35.0</td>\n      <td>NaN</td>\n      <td>33.6</td>\n      <td>0.627</td>\n      <td>50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1</td>\n      <td>85.0</td>\n      <td>66.0</td>\n      <td>29.0</td>\n      <td>NaN</td>\n      <td>26.6</td>\n      <td>0.351</td>\n      <td>31</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>8</td>\n      <td>183.0</td>\n      <td>64.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>23.3</td>\n      <td>0.672</td>\n      <td>32</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1</td>\n      <td>89.0</td>\n      <td>66.0</td>\n      <td>23.0</td>\n      <td>94.0</td>\n      <td>28.1</td>\n      <td>0.167</td>\n      <td>21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0</td>\n      <td>137.0</td>\n      <td>40.0</td>\n      <td>35.0</td>\n      <td>168.0</td>\n      <td>43.1</td>\n      <td>2.288</td>\n      <td>33</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# example of marking missing values with nan values\n",
    "from numpy import nan\n",
    "\n",
    "# replace '0' values with 'nan'\n",
    "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, nan)\n",
    "\n",
    "# count the number of nan values in each column\n",
    "print(dataset.isnull().sum())\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Missing Values Causes Problems\n",
    "Missing values are common occurrences in data. Unfortunately, most predictive modeling techniques cannot handle any missing values. Therefore, this problem must be addressed prior to modeling.\n",
    "\n",
    "In this section, we will try to evaluate a the `Linear Discriminant Analysis (LDA)` algorithm on the dataset with missing values.\n",
    "\n",
    "This is an algorithm that does not work when there are missing values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: nan\n"
    }
   ],
   "source": [
    "# example where missing values cause errors\n",
    "from numpy import nan\n",
    "from pandas import read_csv\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# load the dataset\n",
    "dataset = read_csv('..//..//..//data/pima-indians-diabetes.csv', header=None)\n",
    "\n",
    "# replace '0' values with 'nan'\n",
    "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, nan)\n",
    "\n",
    "# split dataset into inputs and outputs\n",
    "values = dataset.values\n",
    "X = values[:,0:8] # input\n",
    "y = values[:,8] # output\n",
    "\n",
    "# define the model\n",
    "model = LinearDiscriminantAnalysis()\n",
    "\n",
    "# define the model evaluation procedure\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "# evaluate the model\n",
    "result = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "\n",
    "# report the mean performance\n",
    "print('Accuracy: %.3f' % result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example results in an error, as follows:\n",
    "\n",
    "`Accuracy: nan`\n",
    "\n",
    "We are prevented from evaluating an `*LDA algorithm*` (and other algorithms) on the dataset with missing values.\n",
    "\n",
    "Many popular predictive models such as *`support vector`* machines, the `*glmnet*`, and `*neural networks*`, cannot tolerate any amount of missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Remove Rows With Missing Values\n",
    "he simplest approach for dealing with missing values is to remove entire predictor(s) and/or sample(s) that contain missing values.\n",
    "\n",
    "We can do this by creating a new Pandas DataFrame with the rows containing missing values removed. Use dropna() to remove all rows with missing data, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Before: (768, 9)\nAfter: (392, 9)\n"
    }
   ],
   "source": [
    "# example of removing rows that contain missing values\n",
    "from numpy import nan\n",
    "from pandas import read_csv\n",
    "\n",
    "# load the dataset\n",
    "dataset = read_csv('..//..//..//data/pima-indians-diabetes.csv', header=None)\n",
    "\n",
    "# summarize the shape of the raw data\n",
    "print('Before: ' + str(dataset.shape))\n",
    "\n",
    "# replace '0' values with 'nan'\n",
    "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, nan)\n",
    "\n",
    "# drop rows with missing values\n",
    "dataset.dropna(inplace=True)\n",
    "\n",
    "# summarize the shape of the data with missing rows removed\n",
    "print('After: ' + str(dataset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: 0.781\n"
    }
   ],
   "source": [
    "# example where missing values cause errors\n",
    "from numpy import nan\n",
    "from pandas import read_csv\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# split dataset into inputs and outputs\n",
    "values = dataset.values\n",
    "X = values[:,0:8] # input\n",
    "y = values[:,8] # output\n",
    "\n",
    "# define the model\n",
    "model = LinearDiscriminantAnalysis()\n",
    "\n",
    "# define the model evaluation procedure\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "# evaluate the model\n",
    "result = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "\n",
    "# report the mean performance\n",
    "print('Accuracy: %.3f' % result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Impute Missing Values\n",
    "Missing data can be imputed. In this case, we can use information in the training set predictors to, in essence, estimate the values of other predictors.\n",
    "\n",
    "There are many options we could consider when replacing a missing value, for example:\n",
    "\n",
    "- A `constant value` that has meaning within the domain, such as 0, distinct from all other values.\n",
    "- A value from another `randomly selected` record.\n",
    "- A `mean`, `median` or `mode` value for the column.\n",
    "- A `value estimated` by `another predictive model`.\n",
    "\n",
    "*`Any imputing performed on the training dataset will have to be performed on new data in the future when predictions are needed from the finalized model. This needs to be taken into consideration when choosing how to impute the missing values`*.\n",
    "\n",
    "Note. *For example, if you choose to impute with mean column values, these mean column values will need to be stored to file for later use on new data that has missing values*.\n",
    "\n",
    "### - fillna() function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0    0\n1    0\n2    0\n3    0\n4    0\n5    0\n6    0\n7    0\n8    0\ndtype: int64\n    0      1          2         3           4          5      6   7  8\n0   6  148.0  72.000000  35.00000  155.548223  33.600000  0.627  50  1\n1   1   85.0  66.000000  29.00000  155.548223  26.600000  0.351  31  0\n2   8  183.0  64.000000  29.15342  155.548223  23.300000  0.672  32  1\n3   1   89.0  66.000000  23.00000   94.000000  28.100000  0.167  21  0\n4   0  137.0  40.000000  35.00000  168.000000  43.100000  2.288  33  1\n5   5  116.0  74.000000  29.15342  155.548223  25.600000  0.201  30  0\n6   3   78.0  50.000000  32.00000   88.000000  31.000000  0.248  26  1\n7  10  115.0  72.405184  29.15342  155.548223  35.300000  0.134  29  0\n8   2  197.0  70.000000  45.00000  543.000000  30.500000  0.158  53  1\n9   8  125.0  96.000000  29.15342  155.548223  32.457464  0.232  54  1\n"
    }
   ],
   "source": [
    "# manually impute missing values with numpy\n",
    "from pandas import read_csv\n",
    "from numpy import nan\n",
    "\n",
    "# load the dataset\n",
    "dataset = read_csv('..//..//..//data/pima-indians-diabetes.csv', header=None)\n",
    "\n",
    "# mark zero values as missing or NaN\n",
    "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, nan)\n",
    "\n",
    "# fill missing values with mean column values\n",
    "dataset.fillna(dataset.mean(), inplace=True)\n",
    "\n",
    "# count the number of NaN values in each column\n",
    "print(dataset.isnull().sum())\n",
    "print(dataset.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -  SimpleImputer pre-processing\n",
    "\n",
    "The example below uses the SimpleImputer class to replace missing values with the mean of each column then prints the number of NaN values in the transformed matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Missing: 0\n[[6.0e+00 1.5e+02 7.2e+01 3.5e+01 1.6e+02 3.4e+01 6.3e-01 5.0e+01 1.0e+00]\n [1.0e+00 8.5e+01 6.6e+01 2.9e+01 1.6e+02 2.7e+01 3.5e-01 3.1e+01 0.0e+00]\n [8.0e+00 1.8e+02 6.4e+01 2.9e+01 1.6e+02 2.3e+01 6.7e-01 3.2e+01 1.0e+00]\n [1.0e+00 8.9e+01 6.6e+01 2.3e+01 9.4e+01 2.8e+01 1.7e-01 2.1e+01 0.0e+00]\n [0.0e+00 1.4e+02 4.0e+01 3.5e+01 1.7e+02 4.3e+01 2.3e+00 3.3e+01 1.0e+00]\n [5.0e+00 1.2e+02 7.4e+01 2.9e+01 1.6e+02 2.6e+01 2.0e-01 3.0e+01 0.0e+00]\n [3.0e+00 7.8e+01 5.0e+01 3.2e+01 8.8e+01 3.1e+01 2.5e-01 2.6e+01 1.0e+00]\n [1.0e+01 1.2e+02 7.2e+01 2.9e+01 1.6e+02 3.5e+01 1.3e-01 2.9e+01 0.0e+00]\n [2.0e+00 2.0e+02 7.0e+01 4.5e+01 5.4e+02 3.0e+01 1.6e-01 5.3e+01 1.0e+00]\n [8.0e+00 1.2e+02 9.6e+01 2.9e+01 1.6e+02 3.2e+01 2.3e-01 5.4e+01 1.0e+00]]\n"
    }
   ],
   "source": [
    "# example of imputing missing values using scikit-learn\n",
    "from numpy import nan\n",
    "from numpy import isnan\n",
    "from pandas import read_csv\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# load the dataset\n",
    "dataset = read_csv('..//..//..//data/pima-indians-diabetes.csv', header=None)\n",
    "\n",
    "# mark zero values as missing or NaN\n",
    "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, nan)\n",
    "\n",
    "# retrieve the numpy array\n",
    "values = dataset.values\n",
    "\n",
    "# define the imputer\n",
    "imputer = SimpleImputer(missing_values=nan, strategy='mean')\n",
    "\n",
    "# transform the dataset\n",
    "transformed_values = imputer.fit_transform(values)\n",
    "\n",
    "# count the number of NaN values in each column\n",
    "print('Missing: %d' % isnan(transformed_values).sum())\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=1)\n",
    "print(transformed_values[0:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below shows the LDA algorithm trained in the SimpleImputer transformed dataset.\n",
    "\n",
    "We use a Pipeline to define the modeling pipeline, where data is first passed through the imputer transform, then provided to the model. `This ensures that the imputer and model are both fit only on the training dataset and evaluated on the test dataset within each cross-validation fold`. This is important to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: 0.762\n"
    }
   ],
   "source": [
    "# example of evaluating a model after an imputer transform\n",
    "from numpy import nan\n",
    "from pandas import read_csv\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# load the dataset\n",
    "dataset = read_csv('..//..//..//data/pima-indians-diabetes.csv', header=None)\n",
    "\n",
    "# mark zero values as missing or NaN\n",
    "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, nan)\n",
    "\n",
    "# split dataset into inputs and outputs\n",
    "values = dataset.values\n",
    "X = values[:,0:8]\n",
    "y = values[:,8]\n",
    "\n",
    "# define the imputer\n",
    "imputer = SimpleImputer(missing_values=nan, strategy='mean')\n",
    "#imputer = SimpleImputer(missing_values=nan, strategy='median')\n",
    "#imputer = SimpleImputer(missing_values=nan, strategy='most_frequent')\n",
    "#imputer = SimpleImputer(missing_values=nan, strategy='constant')\n",
    "#imputer = SimpleImputer(missing_values=nan, strategy='constant', fill_value=10)\n",
    "\n",
    "# define the model\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# define the modeling pipeline\n",
    "pipeline = Pipeline(steps=[('imputer', imputer), ('model', lda)])\n",
    "\n",
    "# define the cross validation procedure\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "# evaluate the model\n",
    "result = cross_val_score(pipeline, X, y, cv=kfold, scoring='accuracy')\n",
    "\n",
    "# report the mean performance\n",
    "print('Accuracy: %.3f' % result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results with several imputer values\n",
    "\n",
    "- Accuracy (mean): 0.762\n",
    "- Accuracy (median): 0.760\n",
    "- Accuracy (most_frequent): 0.760\n",
    "- Accuracy (constant): 0.763\n",
    "- Accuracy (constant, fill_value=10): 0.767"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Algorithms that Support Missing Values\n",
    "\n",
    "There are algorithms that can be made robust to missing data, such as `k-Nearest Neighbors` that can ignore a column from a distance measure when a value is missing. `Naive Bayes` can also support missing values when making a prediction.\n",
    "\n",
    "There are also algorithms that can use the missing value as a unique and different value when building the predictive model, such as classification and regression trees; especially `tree-based techniques`, can specifically account for missing data.\n",
    "\n",
    "`Note`. *Sadly, the scikit-learn implementations of naive bayes, decision trees and k-Nearest Neighbors are not robust to missing values. Although it is being considered* [Here](https://github.com/scikit-learn/scikit-learn/issues/5870)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this tutorial, you discovered how to handle machine learning data that contains missing values.\n",
    "\n",
    "Specifically, you learned:\n",
    "\n",
    "- How to mark missing values in a dataset as numpy.nan.\n",
    "- How to remove rows from the dataset that contain missing values.\n",
    "- How to replace missing values with sensible values."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}