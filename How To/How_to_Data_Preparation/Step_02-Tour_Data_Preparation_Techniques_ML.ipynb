{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tour of Data Preparation Techniques for Machine Learning\n",
    "\n",
    "by Jason Brownlee - [Reference](https://machinelearningmastery.com/data-preparation-techniques-for-machine-learning/)- June 19, 2020 in [Data Preparation](https://machinelearningmastery.com/category/data-preparation/)\n",
    "\n",
    "\n",
    "## Tutorial Overview\n",
    "This tutorial is divided into six parts; they are:\n",
    "\n",
    "- Common Data Preparation Tasks\n",
    "- Data Cleaning\n",
    "- Feature Selection\n",
    "- Data Transforms\n",
    "- Feature Engineering\n",
    "- Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Data Preparation Tasks\n",
    "We can define data preparation as the transformation of raw data into a form that is more suitable for modeling.\n",
    "\n",
    "The process of applied machine learning consists of a sequence of steps.\n",
    "\n",
    "- Step 1: Define Problem.\n",
    "- Step 2: `Prepare Data`.\n",
    "- Step 3: Evaluate Models.\n",
    "- Step 4: Finalize Model.\n",
    "\n",
    "Concerned with the data preparation step (step 2), you work through multiple predictive modeling projects, you see and require the same types of data preparation tasks again and again.\n",
    "\n",
    "These tasks include:\n",
    "\n",
    "1. `Data Cleaning`: Identifying and correcting mistakes or errors in the data.\n",
    "2. `Feature Selection`: Identifying those input variables that are most relevant to the task.\n",
    "3. `Data Transforms`: Changing the scale or distribution of variables.\n",
    "4. `Feature Engineering`: Deriving new variables from available data.\n",
    "5. `Dimensionality Reduction`: Creating compact projections of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. `Data Cleaning`\n",
    "Data cleaning involves fixing systematic problems or errors in “messy” data. \n",
    "\n",
    "There are many reasons data may have incorrect values, such as being mistyped, corrupted, duplicated, and so on\n",
    "\n",
    "Once messy, noisy, corrupt, or erroneous observations are identified, they can be addressed. This might involve `removing a row` or a `column`. Alternately, it might involve `replacing observations` with new values.\n",
    "\n",
    "there are general data cleaning operations that can be performed, such as:\n",
    "\n",
    "- Using statistics to `define normal data` and identify `outliers` (or `anomalous data`).\n",
    "- Identifying columns that have the `same value` or `no variance` and removing them.\n",
    "- Identifying `duplicate rows` of data and removing them.\n",
    "- `Marking empty values` as missing.\n",
    "- `Imputing missing values` using `statistics` or a `learned model`.\n",
    "\n",
    "Data cleaning is an operation that is typically performed first, prior to other data preparation operations.\n",
    "\n",
    "Data cleaning tutorial:\n",
    "\n",
    "1. [How to Perform Data Cleaning for Machine Learning with Python](https://machinelearningmastery.com/basic-data-cleaning-for-machine-learning/)\n",
    "2. How to delete Duplicate Rows and Useless Features\n",
    "3. [How to identify and Delete Outliers](https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/)\n",
    "4. [How to impute Missing Values](https://machinelearningmastery.com/statistical-imputation-for-missing-values-in-machine-learning/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. `Feature Selection`\n",
    "Feature selection refers to techniques for selecting a subset of input features that are most relevant to the target variable that is being predicted.\n",
    "\n",
    "This is important as irrelevant and redundant input variables can distract or mislead learning algorithms possibly resulting in lower predictive performance. Additionally, it is desirable to develop models only using the data that is required to make a prediction, e.g. to favor the simplest possible well performing model.\n",
    "\n",
    "Statistical methods are popular for scoring input features, such as correlation. The features can then be ranked by their scores and a subset with the largest scores used as input to a model. The choice of statistical measure depends on the data types of the input variables and a review of different statistical measures that can be used.\n",
    "\n",
    "For an overview of how to select statistical feature selection methods based on data type, see the tutorial:\n",
    "\n",
    "1. [How to Choose a Feature Selection Method For Machine Learning](https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/)\n",
    "\n",
    "Additionally, there are different common feature selection use cases we may encounter in a predictive modeling project, such as:\n",
    "\n",
    "- [`Categorical` inputs for a `classification` target variable](https://machinelearningmastery.com/feature-selection-with-categorical-data/).\n",
    "- `Numerical` inputs for a `classification` target variable.\n",
    "- `Numerical` inputs for a `regression` target variable.\n",
    "\n",
    "Feature importance, see the tutorial:\n",
    "\n",
    "2. [How to Calculate Feature Importance With Python](https://machinelearningmastery.com/calculate-feature-importance-with-python/).\n",
    "3. [Recursive Feature Elimination (RFE) for Feature Selection in Python](https://machinelearningmastery.com/rfe-feature-selection-in-python/).\n",
    "3. [How to Use Feature Selection for Regression](https://machinelearningmastery.com/feature-selection-for-regression-data/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. `Data Transforms`\n",
    "Data transforms are used to change the type or distribution of data variables.\n",
    "\n",
    "Recall that data may have one of a few types, such as numeric or categorical, with subtypes for each, such as integer and real-valued for numeric, and nominal, ordinal, and boolean for categorical.\n",
    "\n",
    "- `Numeric Data Type`: Number values.\n",
    "    - `Integer`: Integers with no fractional part.\n",
    "    - `Real`: Floating point values.\n",
    "- `Categorical Data Type`: Label values.\n",
    "    - `Ordinal`: Labels with a rank ordering.\n",
    "    - `Nominal`: Labels with no rank ordering.\n",
    "    - `Boolean`: Values True and False.\n",
    "\n",
    "We may wish to convert a *`numeric variable`* to an *`ordinal variable`* in a process called **discretization**. Alternatively, we may *`encode a categorical variable`* as *`integers or boolean`* variables, required on most **classification tasks**.\n",
    "\n",
    "- **Discretization Transform**: Encode a numeric variable as an ordinal variable.\n",
    "- **Ordinal Transform**: Encode a categorical variable into an integer variable.\n",
    "- **One-Hot Transform**: Encode a categorical variable into binary variables.\n",
    "\n",
    "For real-valued numeric variables, the way they are represented in a computer means there is dramatically more resolution in the range 0-1 than in the broader range of the data type. As such, it may be desirable to scale variables to this range, called normalization. If the data has a Gaussian probability distribution, it may be more useful to shift the data to a standard Gaussian with a mean of zero and a standard deviation of one.\n",
    "\n",
    "- **Normalization Transform**: Scale a variable to the range 0 and 1.\n",
    "- **Standardization Transform**: Scale a variable to a standard Gaussian.\n",
    "\n",
    "If the distribution is nearly Gaussian, but is skewed or shifted, it can be made more Gaussian using a power transform. Alternatively, quantile transforms can be used to force a probability distribution, such as a uniform or Gaussian on a variable with an unusual natural distribution.\n",
    "\n",
    "- **Power Transform**: Change the distribution of a variable to be more Gaussian.\n",
    "- **Quantile Transform**: Impose a probability distribution such as uniform or Gaussian.\n",
    "\n",
    "An important consideration with data transforms is that the operations are generally performed separately for each variable. As such, we may want to perform different operations on different variable types.\n",
    "\n",
    "- Data Transforms\n",
    "    - Numerical Type\n",
    "        - Change Scale\n",
    "            - Normalize\n",
    "            - Standardize\n",
    "            - Robust\n",
    "        - Change Distribucion\n",
    "            - Power\n",
    "            - Quantile\n",
    "            - Descretize\n",
    "        - Engineer\n",
    "            - Polynomial\n",
    "    - Categorical Type\n",
    "        - Nominal Type\n",
    "            - One Hot Encode\n",
    "            - Dummy Encode\n",
    "        - Ordinal Type\n",
    "            - Label Encode\n",
    "\n",
    "1. [4 Common Machine Learning Data Transforms for Time Series Forecasting](https://machinelearningmastery.com/machine-learning-data-transforms-for-time-series-forecasting/).\n",
    "2. [How to use Normalization and Standardization](https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python/)\n",
    "3. [How to use Ordinal and One Hot Encoding](https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/)\n",
    "4. [How to use Power Transforms](https://machinelearningmastery.com/power-transforms-with-scikit-learn/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. `Feature Engineering`\n",
    "Feature engineering refers to the process of creating new input variables from the available data.\n",
    "\n",
    "Engineering new features is highly specific to your data and data types. As such, it often requires the collaboration of a subject matter expert to help identify new features.\n",
    "\n",
    "This specialization makes it a challenging topic to generalize to general methods. There are some techniques that can be reused, such as:\n",
    "\n",
    "- Adding a boolean flag variable for some state.\n",
    "- Adding a group or global summary statistic, such as a mean.\n",
    "- Adding new variables for each component of a compound variable, such as a date-time.\n",
    "\n",
    "A popular approach drawn from statistics is to create copies of numerical input variables that have been changed with a simple mathematical operation, such as raising them to a power or multiplied with other input variables, referred to as polynomial features.\n",
    "\n",
    "- `Polynomial Transform`: Create copies of numerical input variables that are raised to a power [here](https://machinelearningmastery.com/polynomial-features-transforms-for-machine-learning/).\n",
    "\n",
    "1. [How to Use Polynomial Feature Transforms for Machine Learning](https://machinelearningmastery.com/polynomial-features-transforms-for-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. `Dimensionality Reduction`\n",
    "The number of input features for a dataset may be considered the dimensionality of the data.\n",
    "\n",
    "The problem is, the more dimensions this space has (e.g. the more input variables), the more likely it is that the dataset represents a very sparse and likely unrepresentative sampling of that space. This is referred to as the curse of dimensionality.\n",
    "\n",
    "This motivates feature selection, although an alternative to feature selection is to create a projection of the data into a lower-dimensional space that still preserves the most important properties of the original data.\n",
    "\n",
    "This is referred to generally as dimensionality reduction and provides an alternative to feature selection. Unlike feature selection, the variables in the projected data are not directly related to the original input variables, making the projection difficult to interpret.\n",
    "\n",
    "The most common approach to dimensionality reduction is to use a matrix factorization technique:\n",
    "\n",
    "- Principal Component Analysis (PCA)\n",
    "- Singular Value Decomposition (SVD)\n",
    "- `Factor Analysis` together with PCA for dimension\n",
    "\n",
    "Other approaches exist that discover a lower dimensionality reduction. We might refer to these as model-based methods such as LDA and perhaps autoencoders.\n",
    "\n",
    "- Linear Discriminant Analysis (LDA)\n",
    "\n",
    "Sometimes manifold learning algorithms can also be used, such as `Kohonen self-organizing maps` and `t-SNE`.\n",
    "\n",
    "- Dimensionality Reduction\n",
    "    - Manifold Learning\n",
    "        - SOM\n",
    "        - tSNE\n",
    "    - Model Based\n",
    "        - LDA\n",
    "    - Matrix Factorizacion\n",
    "        - PCA\n",
    "        - SVD\n",
    "\n",
    "Dimensionality reduction, see the tutorial:\n",
    "\n",
    "1. [Introduction to Dimensionality Reduction for Machine Learning](https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/).\n",
    "2. [How to use PCA for Dimensionality Reduction](https://machinelearningmastery.com/principal-components-analysis-for-dimensionality-reduction-in-python/)\n",
    "3. [How to use LDA for Dimensionality Reduction](https://machinelearningmastery.com/linear-discriminant-analysis-for-dimensionality-reduction-in-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "This section provides more resources on the topic if you are looking to go deeper.\n",
    "\n",
    "Tutorials\n",
    "- [How to Prepare Data For Machine Learning](https://machinelearningmastery.com/how-to-prepare-data-for-machine-learning/)\n",
    "- [Applied Machine Learning Process](https://machinelearningmastery.com/process-for-working-through-machine-learning-problems/)\n",
    "- [How to Perform Data Cleaning for Machine Learning with Python](https://machinelearningmastery.com/basic-data-cleaning-for-machine-learning/)\n",
    "- [How to Choose a Feature Selection Method For Machine Learning](https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/)\n",
    "- [Introduction to Dimensionality Reduction for Machine Learning](https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/)\n",
    "\n",
    "Books\n",
    "- Feature Engineering and Selection: A Practical Approach for Predictive Models, 2019.[Here](https://bookdown.org/max/FES/).\n",
    "- Applied Predictive Modeling, 2013.[Here](https://amzn.to/2VMhnat).\n",
    "- Data Mining: Practical Machine Learning Tools and Techniques, 4th edition, 2016.[Here](https://amzn.to/2Kk6tn0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
