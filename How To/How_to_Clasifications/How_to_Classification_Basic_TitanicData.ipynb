{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Python: Classification (complete tutorial)\n",
    "Reference. By Mauro Di Pietro. [Here](https://getpocket.com/redirect?url=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-with-python-classification-complete-tutorial-d2c99dc524ec). [Code](https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/machine_learning/example_classification.ipynb)\n",
    "\n",
    "## Content\n",
    "- 1 Environment setup: import libraries and read data\n",
    "- 2 Data Analysis: understand the meaning and the predictive power of the variables\n",
    "- 3 Feature engineering: extract features from raw data\n",
    "- 4 Preprocessing: data partitioning, handle missing values, encode categorical variables, scale\n",
    "- Baseline (xgboost)\n",
    "    - 5 Feature Selection: keep only the most relevant variables\n",
    "    - 6 Model design: train, tune hyperparameters, validation, test\n",
    "    - 7 Performance evaluation: read the metrics\n",
    "    - 8 Explainability: understand how the model produces results\n",
    "\n",
    "## 1 Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# data libraries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "# plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# statistical tests libraries\n",
    "import scipy\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sum\n",
    "\n",
    "# machine learning libraries\n",
    "from sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition\n",
    "\n",
    "# explainer libraries\n",
    "from lime import lime_tabular\n",
    "\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'models' from 'tensorflow_core.keras' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2080c04a3150>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# custom python package\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutility\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml_Classification_utilities\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive\\Projects\\Maching-Learning-Resources\\How To\\How_to_Clasifications\\main\\utility\\ml_Classification_utilities.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m## for machine learning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimpute\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_selection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecomposition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminant_analysis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensemble\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m## for explainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'models' from 'tensorflow_core.keras' (unknown location)"
     ]
    }
   ],
   "source": [
    "# custom python package \n",
    "from main.utility.ml_Classification_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf = pd.read_csv('..//..//data//kaggle-titanic-train.csv')\n",
    "dtf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data Analysis\n",
    "\n",
    "In statistics, exploratory data analysis is the process of summarizing the main characteristics of a dataset to understand what the data can tell us beyond the formal modeling or hypothesis testing task.\n",
    "\n",
    "I always start by getting an overview of the whole dataset, in particular I want to know how many categorical and numerical variables there are and the proportion of missing data. Recognizing a variable’s type sometimes can be tricky because categories can be expressed as numbers (the Survived column is made of 1s and 0s). To this end, I am going to write a simple function that will do that for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf_overview(dtf, max_cat=20, figsize=(10, 5))\n",
    "# dtf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 885 rows and 12 columns:\n",
    "\n",
    "- each row of the table represents a specific passenger (or observation) identified by PassengerId, so I’ll set it as index (or primary key of the table for SQL lovers).\n",
    "- Survived is the phenomenon that we want to understand and predict (or target variable), so I’ll rename the column as “Y”. It contains two classes: 1 if the passenger survived and 0 otherwise, therefore this use case is a binary classification problem.\n",
    "- Age and Fare are numerical variables while the others are categorical.\n",
    "- Only Age and Cabin contain missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf = dtf.set_index(\"PassengerId\")\n",
    "dtf = dtf.rename(columns={\"Survived\": \"Y\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe visualization is the best tool for data analysis, but you need to know what kind of plots are more suitable for the different types of variable. Therefore, I’ll provide the code to plot the appropriate visualization for different examples.\n",
    "\n",
    "First, let’s have a look at the univariate distributions (probability distribution of just one variable). A bar plot is appropriate to understand labels frequency for a single categorical variable. For example, let’s plot the target variable:\n",
    "\n",
    "Group variables by info\n",
    "- who: Sex, Age, Embarked (which port C=Cherbourg, Q=Queenstown, S=Southampton)\n",
    "- wealth: Pclass, Ticket, Fare\n",
    "- where: Cabin\n",
    "- how many: SibSp (with siblings/spouse), Parch (with parent/children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "# target variable\n",
    "# Up to 300 passengers survived and about 550 didn’t, in other words the survival rate (or the population mean) is 38%.\n",
    "freqdist_plot(dtf, \"Y\", figsize=(5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-> Population mean: 38% of the passengers survived\n",
    "\n",
    "corr = corr_matrix(dtf, method=\"pearson\", negative=False, lst_filters=[\"Y\"], figsize=(15,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictive Power Score\n",
    "# reference: https://www.kaggle.com/frtgnn/predictive-power-score-vs-correlation\n",
    "pps = pps_matrix(dtf, lst_filters=[\"Y\"], figsize=(15,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Who? Sex, Age, Embarked\n",
    "\n",
    "#--- Sex ----#\n",
    "bivariate_plot(dtf, x=\"Age\", y=\"Y\", figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Pclass ----#\n",
    "bivariate_plot(dtf, x=\"Age\", y=\"Pclass\", figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Pclass ----#\n",
    "bivariate_plot(dtf, x=\"Age\", y=\"Sex\", figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When not convinced by the “eye intuition”, you can always resort to good old statistics and run a test. In this case of categorical (Y) vs numerical (Age), I would use a one-way ANOVA test. Basically, it tests whether the means of two or more independent samples are significantly different, so if the p-value is small enough (<0.05) the null hypothesis of samples means equality can be rejected.\n",
    "\n",
    "Apparently the passengers age contributed to determine their survival. That makes sense as the lives of women and children were to be saved first in a life-threatening situation, typically abandoning ship, when survival resources such as lifeboats were limited (the “women and children first” code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coeff, p = test_corr(dtf, x=\"Fare\", y=\"Embarked\")\n",
    "#coeff, p = test_corr(dtf, x=\"Age\", y=\"Y\")\n",
    "coeff, p = test_corr(dtf, x=\"Sex\", y=\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-> Sex is Predictive: the surviving rate of females is higher.\n",
    "features.append(\"Sex\")\n",
    "\n",
    "#--- Age ---#\n",
    "nan_analysis(dtf, na_x=\"Age\", y=\"Y\", max_cat=20, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check the validity of this first conclusion, I will have to analyze the behavior of the Sex variable with respect to the target variable. This is a case of categorical (Y) vs categorical (Sex), so I’ll plot 2 bar plots, one with the amount of 1s and 0s among the two categories of Sex (male and female) and the other with the percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqdist_plot(dtf, \"Age\", box_logscale=True, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The passengers were, on average, pretty young: the distribution is skewed towards the left side (the mean is 30 y.o and the 75th percentile is 38 y.o.). Coupled with the outliers in the box plot, the first spike in the left tail says that there was a significant amount of children.\n",
    "\n",
    "I’ll take the analysis to the next level and look into the bivariate distribution to understand if Age has predictive power to predict Y. This would be the case of categorical (Y) vs numerical (Age), therefore I shall proceed like this:\n",
    "\n",
    "- split the population (the whole set of observations) into 2 samples: the portion of passengers with Y = 1 (Survived) and Y = 0 (Not Survived).\n",
    "- Plot and compare densities of the two samples, if the distributions are different then the variable is predictive because the two groups have different patterns.\n",
    "- Group the numerical variable (Age) in bins (subsamples) and plot the composition of each bin, if the proportion of 1s is similar in all of them then the variable is not predictive.\n",
    "- Plot and compare the box plots of the two samples to spot different behaviors of the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bivariate_plot(dtf, x=\"Age\", y=\"Y\", figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff, p = test_corr(dtf, x=\"Age\", y=\"Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 3 plots are just different perspectives of the conclusion that Age is predictive. The survival rate is higher for younger passengers: there is a spike in the left tail of 1s distribution and the first bin (0–16 y.o.) contains the highest percentage of survived passengers.\n",
    "\n",
    "When not convinced by the “eye intuition”, you can always resort to good old statistics and run a test. In this case of categorical (Y) vs numerical (Age), I would use a one-way ANOVA test. Basically, it tests whether the means of two or more independent samples are significantly different, so if the p-value is small enough (<0.05) the null hypothesis of samples means equality can be rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-> Age is Predictive: the Surviving rate is higher for younger passengers, there is a spike in the left tail of Y=1 \n",
    "# distribution and the first bin of Age (0-16) contains the highest percentage of survived people.\n",
    "features.append(\"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It appears that they applied the \"Save women and children first\" code. Let's check:\n",
    "cross_distributions(dtf, x1=\"Sex\", x2=\"Age\", y=\"Y\", figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Embarked ---#\n",
    "bivariate_plot(dtf, x=\"Embarked\", y=\"Y\", figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oeff, p = test_corr(dtf, x=\"Embarked\", y=\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-> Embarked is Predictive: People from port C tend to survive better (that can be because they stayed in a fortunate area\n",
    "# of the ship or just because they're smarter). Since there aren't many observations, I tested the significance \n",
    "# of the correlation (Cramer cat vs cat), it passed.\n",
    "features.append(\"Embarked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than 200 female passengers (75% of the total amount of women onboard) and about 100 male passengers (less than 20%) survived. To put it another way, among women the survival rate is 75% and among men is 20%, therefore Sex is predictive. Moreover, this confirms that they gave priority to women and children.\n",
    "\n",
    "Just like before, we can test the correlation of these 2 variables. Since they are both categorical, I’d use a Chi-Square test: assuming that two variables are independent (null hypothesis), it tests whether the values of the contingency table for these variables are uniformly distributed. If the p-value is small enough (<0.05), the null hypothesis can be rejected and we can say that the two variables are probably dependent. It’s possible to calculate Cramer’s V that is a measure of correlation that follows from this test, which is symmetrical (like traditional Pearson’s correlation) and ranges between 0 and 1 (unlike traditional Pearson’s correlation there are no negative values).\n",
    "\n",
    "Age and Sex are examples of predictive features, but not all of the columns in the dataset are like that. For instance, Cabin seems to be an useless variable as it doesn’t provide any useful information, there are too many missing values and categories.\n",
    "\n",
    "This kind of analysis should be carried on for each variable in the dataset to decide what should be kept as potential feature and what can be dropped because not predictive (check out the link to the full code).\n",
    "\n",
    "### Wealth? Pclass, Ticket, Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Pclass ---#\n",
    "bivariate_plot(dtf, x=\"Pclass\", y=\"Y\", figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-> Pclass is Predctive: the richer the higher the probability of surviving.\n",
    "features.append(\"Pclass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Ticket ---#\n",
    "#-> Ticket is Useless\n",
    "freqdist_plot(dtf, \"Ticket\", top=10, figsize=(5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Fare ---#\n",
    "bivariate_plot(dtf, x=\"Fare\", y=\"Y\", figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bivariate_plot(dtf, x=\"Fare\", y=\"Pclass\", figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_distributions(dtf, x1=\"Pclass\", x2=\"Fare\", y=\"Y\", figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-> Looks there is more information in the first class: who paid higher price survived better.\n",
    "# I will keep it for now and exclude one of the two in the Features Selection section.\n",
    "features.append(\"Fare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where? Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Cabin ---#\n",
    "freqdist_plot(dtf, \"Cabin\", top=10, figsize=(5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useless like this, let's see if the variable can be clustered using the first letter of the cabin:\n",
    "dtf[\"Cabin_section\"] = dtf[\"Cabin\"].apply(lambda x: str(x)[0])\n",
    "freqdist_plot(dtf, \"Cabin_section\", top=10, figsize=(5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3 Feature engineering:\n",
    "\n",
    "It’s time to create new features from raw data using domain knowledge. I will provide one example: I’ll try to create a useful feature by extracting information from the Cabin column. I’m assuming that the letter at the beginning of each cabin number (i.e. “B96”) indicates some kind of section, maybe there were some lucky sections near to lifeboats. I will summarize the observations in clusters by extracting the section of each cabin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_distributions(dtf, x1=\"Cabin_section\", x2=\"Pclass\", y=\"Y\", figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows how survivors are distributed among cabin sections and classes (7 survivors are in section A, 35 in B, …). Most of the sections are assigned to the 1st and the 2nd classes, while the majority of missing sections (“n”) belongs to the 3rd class. I am going to keep this new feature instead of the column Cabin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff, p = test_corr(dtf, x=\"Cabin_section\", y=\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-> Cabin_section is predictive: for now.\n",
    "features.append(\"Cabin_section\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many? SibSp, Parch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff, p = test_corr(dtf, x=\"SibSp\", y=\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-> SibSp is predictive: for now.\n",
    "features.append(\"SibSp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Parch ---#\n",
    "bivariate_plot(dtf, x=\"Parch\", y=\"Y\", figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff, p = test_corr(dtf, x=\"Parch\", y=\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-> Parch is predictive: for now.\n",
    "features.append(\"Parch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf = dtf[features+[\"Y\"]]\n",
    "dtf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Preprocessing:\n",
    "\n",
    "To do:\n",
    "- Dataset partitioning\n",
    "- Resample\n",
    "- Treat missings\n",
    "- Encode categorical data\n",
    "- Scaling\n",
    "- Preprocess Test data\n",
    "\n",
    "Data preprocessing is the phase of preparing the raw data to make it suitable for a machine learning model. In particular:\n",
    "\n",
    "1. each observation must be represented by a single raw, in other words you can’t have two rows describing the same passenger because they will be processed separately by the model (the dataset is already in such form, so ✅). Moreover, each column should be a feature, so you shouldn’t use PassengerId as a predictor, that’s why this kind of table is called “feature matrix”.\n",
    "\n",
    "2. The dataset must be partitioned into at least two sets: the model shall be trained on a significant portion of your dataset (so-called “train set”) and tested on a smaller set (“test set”).\n",
    "\n",
    "3. Missing values should be replaced with something, otherwise your model may freak out.\n",
    "\n",
    "4. Categorical data must be encoded, that means converting labels into integers, because machine learning expects numbers not strings.\n",
    "\n",
    "5. It’s good practice to scale the data, it helps to normalize the data within a particular range and speed up the calculations in an algorithm.\n",
    "\n",
    "Alright, let’s begin by partitioning the dataset. When splitting data into train and test sets you must follow 1 basic rule: rows in the train set shouldn’t appear in the test set as well. That’s because the model sees the target values during training and uses it to understand the phenomenon. In other words, the model already knows the right answer for the training observations and testing it on those would be like cheating. I’ve seen a lot of people pitching their machine learning models claiming 99.99% of accuracy that did in fact ignore this rule. Luckily, the Scikit-learn package knows that:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = data_preprocessing(dtf, y=\"Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf_train, dtf_test = dtf_partitioning(dtf, y=\"Y\", test_size=0.3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf_test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample\n",
    "\n",
    "`no need to resample.`\n",
    "\n",
    "### Missing values\n",
    "\n",
    "Next step: the Age column contains some missing data (19%) that need to be handled. In practice, you can replace missing data with a specific value, like 9999, that keeps trace of the missing information but changes the variable distribution. Alternatively, you can use the average of the column, like I’m going to do. I’d like to underline that from a Machine Learning perspective, it’s correct to first split into train and test and then replace NAs with the average of the training set only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf_train, age_mean = fill_na(dtf_train, x=\"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf_train, embarked_mode = fill_na(dtf_train, x=\"Embarked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Encoding\n",
    "\n",
    "There are still some categorical data that should be encoded. The two most common encoders are the Label-Encoder (each unique label is mapped to an integer) and the One-Hot-Encoder (each label is mapped to a binary vector). The first one is suited for data with ordinality only. If applied to a column with no ordinality, like Sex, it would turn the vector [male, female, female, male, …] into [1, 2, 2, 1, …] and we would have that female > male and with an average of 1.5 which makes no sense. On the other hand, the One-Hot-Encoder would transform the previous example into two dummy variables (dichotomous quantitative variables): Male [1, 0, 0, 1, …] and Female [0, 1, 1, 0, …]. It has the advantage that the result is binary rather than ordinal and that everything sits in an orthogonal vector space, but features with high cardinality can lead to a dimensionality issue. I shall use the One-Hot-Encoding method, transforming 1 categorical column with n unique values into n-1 dummies. Let’s encode Sex as example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf_train = add_dummies(dtf_train, x=\"Sex\", dropx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf_train = add_dummies(dtf_train, x=\"Embarked\", dropx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf_train = add_dummies(dtf_train, x=\"Pclass\", dropx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf_train = add_dummies(dtf_train, x=\"Cabin_section\", dropx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf_train = pop_columns(dtf_train, [\"Y\"], where=\"end\")\n",
    "dtf_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "Last but not least, I’m going to scale the features. There are several different ways to do that, I’ll present just the most used ones: the Standard-Scaler and the MinMax-Scaler. The first one assumes data is normally distributed and rescales it such that the distribution centres around 0 with a standard deviation of 1. However, the outliers have an influence when computing the empirical mean and standard deviation which shrink the range of the feature values, therefore this scaler can’t guarantee balanced feature scales in the presence of outliers. On the other hand, the MinMax-Scaler rescales the data set such that all feature values are in the same range (0–1). It is less affected by outliers but compresses all inliers in a narrow range. Since my data is not normally distributed, I’ll go with the MinMax-Scaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler(feature_range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf_train, scaler = scaling(dtf_train, y=\"Y\", scalerX=scaler, task=\"classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf_overview(dtf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Na\n",
    "dtf_test = fill_na(dtf_test, x=\"Age\", value=age_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Categorical\n",
    "dtf_test = add_dummies(dtf_test, x=\"Sex\", dropx=True)\n",
    "dtf_test = add_dummies(dtf_test, x=\"Embarked\", dropx=True)\n",
    "dtf_test = add_dummies(dtf_test, x=\"Pclass\", dropx=True)\n",
    "dtf_test = add_dummies(dtf_test, x=\"Cabin_section\", dropx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## There are two columns less in the Test set: Cabin_section_T, Cabin_section_G \n",
    "dtf_test[\"Cabin_section_T\"], dtf_test[\"Cabin_section_G\"] = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf_test = dtf_test[dtf_train.columns]\n",
    "dtf_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scale\n",
    "dtf_test, _ = scaling(dtf_test, y=\"Y\", scalerX=scaler, fitted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf_overview(dtf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline (xgboost)\n",
    "Plan:\n",
    "- Feature Selection: by correlation, by p-value, by importance\n",
    "- Model Design\n",
    "- Train / Test\n",
    "- Evaluate\n",
    "- Visualize model\n",
    "- Explainability\n",
    "\n",
    "### 5 Feature Selection:\n",
    "\n",
    "Feature selection is the process of selecting a subset of relevant variables to build the machine learning model. It makes the model easier to interpret and reduces overfitting (when the model adapts too much to the training data and performs badly outside the train set).\n",
    "\n",
    "I already did a first “manual” feature selection during data analysis by excluding irrelevant columns. Now it’s going to be a bit different because we assume that all the features in the matrix are relevant and we want to drop the unnecessary ones. When a feature is not necessary? Well, the answer is easy: when there is a better equivalent, or one that does the same job but better.\n",
    "\n",
    "I’ll explain with an example: Pclass is highly correlated with Cabin_section because, as we’ve seen before, certain sections were located in 1st class and others in the 2nd. Let’s compute the correlation matrix to see it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- correlation ---#\n",
    "corr = corr_matrix(dtf_train, method=\"pearson\", negative=False, annotation=True, figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pps = pps_matrix(dtf_train, annotation=True, figsize=(15,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One among Pclass and Cabin_section could be unnecessary and we may decide to drop it and keep the most useful one (i.e. the one with lowest p-value or the one that most reduces entropy).\n",
    "\n",
    "I will show two different ways to perform automatic feature selection: first I will use a regularization method and compare it with the ANOVA test already mentioned before, then I will show how to get feature importance from ensemble methods.\n",
    "\n",
    "LASSO regularization is a regression analysis method that performs both variable selection and regularization in order to enhance accuracy and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- p values ---#\n",
    "dic_feat_sel = features_selection(dtf_train, y=\"Y\", task=\"classification\", top=10, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue features are the ones selected by both ANOVA and LASSO, the others are selected by just one of the two methods.\n",
    "\n",
    "Random forest is an ensemble method that consists of a number of decision trees in which every node is a condition on a single feature, designed to split the dataset into two so that similar response values end up in the same set. Features importance is computed from how much each feature decreases the entropy in a tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- importance ---#\n",
    "model = ensemble.RandomForestClassifier(n_estimators=100, criterion=\"entropy\", random_state=0)\n",
    "\n",
    "feat_imp = features_importance(X=dtf_train.drop(\"Y\",axis=1).values, y=dtf_train[\"Y\"].values, \n",
    "                               X_names=dtf_train.drop(\"Y\",axis=1).columns.tolist(), \n",
    "                              model=model, task=\"classification\", figsize=(15,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s really interesting that Age and Fare, that are the most important features this time, weren’t the top features before and that on the contrary Cabin_section E, F and D don’t appear really useful here.\n",
    "\n",
    "Personally, I always try to use less features as possible, so here I select the following ones and proceed with the design, train, test and evaluation of the machine learning model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> selection\n",
    "X_names = [\"Age\", \"Fare\", \"Sex_male\", \"SibSp\", \"Pclass_3\", \"Parch\", \"Cabin_section_n\", \"Embarked_S\", \"Pclass_2\",\n",
    "           \"Cabin_section_F\", \"Cabin_section_E\", \"Cabin_section_D\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that before using test data for prediction you have to preprocess it just like we did for the train data.\n",
    "\n",
    "### 6 Model design\n",
    "\n",
    "Finally, it’s time to build the machine learning model. First, we need to choose an algorithm that is able to learn from training data how to recognize the two classes of the target variable by minimizing some error function.\n",
    "\n",
    "`'images/scikit-learn-algoritm-cheat-sheet.jpg'`\n",
    "\n",
    "I suggest to always try a gradient boosting algorithm (like XGBoost). It’s a machine learning technique which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. Basically it’s similar to a Random Forest with the difference that every tree is fitted on the error of the previous one.\n",
    "\n",
    "There a lot of hyperparameters and there is no general rule about what is best, so you just have to find the right combination that fits your data better. You could do different tries manually or you can let the computer do this tedious job with a GridSearch (tries every possible combination but takes time) or with a RandomSearch (tries randomly a fixed number of iterations). I’ll try a RandonSearch for my hyperparameter tuning: the machine will iterate n times (1000) through training data to find the combination of parameters (specified in the code below) that maximizes a scoring function used as KPI (accuracy, the ratio of number of correct predictions to the total number of input samples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dtf_train[X_names].values\n",
    "y_train = dtf_train[\"Y\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ensemble.GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dic = {'learning_rate':[0.15,0.1,0.05,0.01,0.005,0.001],      #weighting factor for the corrections by new trees when added to the model\n",
    "             'n_estimators':[100,250,500,750,1000,1250,1500,1750],  #number of trees added to the model\n",
    "             'max_depth':[2,3,4,5,6,7],                             #maximum depth of the tree\n",
    "             'min_samples_split':[2,4,6,8,10,20,40,60,100],         #sets the minimum number of samples to split\n",
    "             'min_samples_leaf':[1,3,5,7,9],                        #the minimum number of samples to form a leaf\n",
    "             'max_features':[2,3,4,5,6,7],                          #square root of features is usually a good starting point\n",
    "             'subsample':[0.7,0.75,0.8,0.85,0.9,0.95,1]}            #the fraction of samples to be used for fitting the individual base learners. Values lower than 1 generally lead to a reduction of variance and an increase in bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, that’s the best model, with a mean accuracy of 0.85, so probably 85% of predictions on the test set will be correct.\n",
    "\n",
    "We can also validate this model using a k-fold cross-validation, a procedure that consists in splitting the data k times into train and validation sets and for each split the model is trained and tested. It’s used to check how well the model is able to get trained by some data and predict unseen data.\n",
    "\n",
    "I’d like to clarify that I call validation set a set of examples used to tune the hyperparameters of a classifier, extracted from splitting training data. On the other end, a test set is a simulation of how the model would perform in production when it’s asked to predict observations never seen before.\n",
    "\n",
    "It’s common to plot a ROC cruve for every fold, a plot that illustrates how the ability of a binary classifier changes as its discrimination threshold is varied. It is created by plotting the true positive rate (1s predicted correctly) against the false positive rate (1s predicted that are actually 0s) at various threshold settings. The AUC (area under the ROC curve) indicates the probability that the classifier will rank a randomly chosen positive observation (Y=1) higher than a randomly chosen negative one (Y=0).\n",
    "\n",
    "Now I’ll show an example of with 10 folds (k=10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes a while\n",
    "model = tune_classif_model(X_train, y_train, model, param_dic, scoring=\"accuracy\", \n",
    "                           searchtype=\"RandomSearch\", n_iter=1000, cv=10, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this validation, we should expect an AUC score around 0.84 when making predictions on the test.\n",
    "\n",
    "For the purpose of this tutorial I’d say that the performance is fine and we can proceed with the model selected by the RandomSearch. Once that the right model is selected, it can be trained on the whole train set and then tested on the test set.\n",
    "\n",
    "### Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = dtf_test[X_names].values\n",
    "y_test = dtf_test[\"Y\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, predicted_prob, predicted = fit_classif_model(model, X_train, y_train, X_test, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above I made two kinds of predictions: the first one is the probability that an observation is a 1, and the second is the prediction of the label (1 or 0). To get the latter you have to decide a probability threshold for which an observation can be considered as 1, I used the default threshold of 0.5.\n",
    "\n",
    "### 7 Performance evaluation\n",
    "\n",
    "Moment of truth, we’re about to see if all this hard work is worth. The whole point is to study how many correct predictions and error types the model makes.\n",
    "\n",
    "I’ll evaluate the model using the following common metrics: Accuracy, AUC, Precision and Recall. I already mentioned the first two, but I reckon that the others are way more important. Precision is the fraction of 1s (or 0s) that the model predicted correctly among all predicted 1s (or 0s), so it can be seen as a sort of confidence level when predicting a 1 (or a 0). Recall is the portion of 1s (or 0s) that the model predicted correctly among all 1s (or 0s) in the test set, basically it’s the true 1 rate. Combining Precision and Recall with an armonic mean, you get the F1-score.\n",
    "\n",
    "Let’s see how the model did on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classif_model(y_test, predicted, predicted_prob, figsize=(20,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the general accuracy of the model is around 85%. It predicted 71% of 1s correctly with a precision of 84% and 92% of 0s with a precision of 85%. In order to understand these metrics better, I’ll break down the results in a confusion matrix:\n",
    "\n",
    "We can see that the model predicted 85 (70+15) 1s of which 70 are true positives and 15 are false positives, so it has a Precision of 70/85 = 0.82 when predicting 1s. On the other hand, the model got 70 1s right of all the 96 (70+26) 1s in the test set, so its Recall is 70/96 = 0.73.\n",
    "\n",
    "Choosing a threshold of 0.5 to decide whether a prediction is a 1 or 0 led to this result. Would it be different with another one? Definitely yes, but there is no threshold that would bring the top score on both precision and recall, choosing a threshold means to make a compromise between these two metrics. I’ll show what I mean by plotting the ROC curve and the precision-recall curve of the test result:\n",
    "\n",
    "Every point of these curves represents a confusion matrix obtained with a different threshold (the numbers printed on the curves). I could use a threshold of 0.1 and gain a recall of 0.9, meaning that the model would predict 90% of 1s correctly, but the precision would drop to 0.4, meaning that the model would predict a lot of false positives. So it really depends on the type of use case and in particular whether a false positive has an higher cost of a false negative.\n",
    "\n",
    "When the dataset is balanced and metrics aren’t specified by project stakeholder, I usually choose the threshold that maximize the F1-score. Here’s how:\n",
    "\n",
    "Before moving forward with the last section of this long tutorial, I’d like to say that we can’t say that the model is good or bad yet. The accuracy is 0.85, is it high? Compared to what? You need a baseline to compare your model with. Maybe the project you’re working on is about building a new model to replace an old one that can be used as baseline, or you can train different machine learning models on the same train set and compare the performance on a test set.\n",
    "\n",
    "### Visualize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2d = ensemble.GradientBoostingClassifier()\n",
    "model2d.set_params(**{'subsample':1, 'n_estimators':1750, 'min_samples_split':6, 'min_samples_leaf':1, 'max_depth':4, 'learning_rate':0.001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot2d_classif_model(X_train, y_train, X_test, y_test, model2d, annotate=False, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 Explainability\n",
    "\n",
    "You analyzed and understood the data, you trained a model and tested it, you’re even satisfied with the performance. You think you’re done? Wrong. High chance that the project stakeholder doesn’t care about your metrics and doesn’t understand your algorithm, so you have to show that your machine learning model is not a black box.\n",
    "\n",
    "The Lime package can help us to build an explainer. To give an illustration I will take a random observation from the test set and see what the model predicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4\n",
    "print(\"True:\", y_test[i], \"--> Pred:\", predicted[i], \"| Prob:\", np.max(predicted_prob[i]))\n",
    "\n",
    "exp = explainer(X_train, X_names, model, y_train,  X_test_instance=X_test[i], top=10, task=\"classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "The main factors for this particular prediction are that the passenger is female (Sex_male = 0), young (Age ≤ 22) and traveling in 1st class (Pclass_3 = 0 and Pclass_2 = 0).\n",
    "\n",
    "The confusion matrix is a great tool to show how the testing went, but I also plot the classification regions to give a visual aid of what observations the model predicted correctly and what it missed. In order to plot the data in 2 dimensions some dimensionality reduction is required (the process of reducing the number of features by obtaining a set of principal variables). I will give an example using the PCA algorithm to summarize the data into 2 variables obtained with linear combinations of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCA    \n",
    "pca = decomposition.PCA(n_components=2)    \n",
    "X_train_2d = pca.fit_transform(X_train)    \n",
    "X_test_2d = pca.transform(X_test)## train 2d model    \n",
    "model_2d = ensemble.GradientBoostingClassifier()    \n",
    "model_2d.fit(X_train, y_train)\n",
    "\n",
    "## plot classification regions    \n",
    "from matplotlib.colors import ListedColormap    \n",
    "colors = {np.unique(y_test)[0]:\"black\", np.unique(y_test)[1]:\"green\"}    \n",
    "X1, X2 = np.meshgrid(np.arange(start=X_test[:,0].min()-1, stop=X_test[:,0].max()+1, step=0.01),    \n",
    "np.arange(start=X_test[:,1].min()-1, stop=X_test[:,1].max()+1, step=0.01))    \n",
    "fig, ax = plt.subplots()    \n",
    "Y = model_2d.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape)    \n",
    "ax.contourf(X1, X2, Y, alpha=0.5, cmap=ListedColormap(list(colors.values())))    \n",
    "ax.set(xlim=[X1.min(),X1.max()], ylim=[X2.min(),X2.max()], title=\"Classification regions\")    \n",
    "\n",
    "for i in np.unique(y_test):    \n",
    "    ax.scatter(X_test[y_test==i, 0], X_test[y_test==i, 1],     \n",
    "               c=colors[i], label=\"true \"+str(i))      \n",
    "plt.legend()    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This article has been a tutorial to demonstrate how to approach a classification use case with data science. I used the Titanic dataset as an example, going through every step from data analysis to the machine learning model.\n",
    "\n",
    "In the exploratory section, I analyzed the case of a single categorical variable, a single numerical variable and how they interact together. I gave an example of feature engineering extracting a feature from raw data. Regarding preprocessing, I explained how to handle missing values and categorical data. I showed different ways to select the right features, how to use them to build a machine learning classifier and how to assess the performance. In the final section, I gave some suggestions on how to improve the explainability of your machine learning model.\n",
    "\n",
    "An important note is that I haven’t covered what happens after your model is approved for deployment. Just keep in mind that you need to build a pipeline to automatically process new data that you will get periodically.\n",
    "\n",
    "Now that you know how to approach a data science use case, you can apply this code and method to any kind of binary classification problem, carry out your own analysis, build your own model and even explain it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
