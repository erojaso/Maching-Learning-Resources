{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Get Started With Deep Learning for Computer Vision (7-Day Mini-Course)\n",
    "by Jason Brownlee on April 9, 2019. [Here](https://machinelearningmastery.com/how-to-get-started-with-deep-learning-for-computer-vision-7-day-mini-course/) in [Deep Learning for Computer Vision](https://machinelearningmastery.com/category/deep-learning-for-computer-vision/)\n",
    "\n",
    "\n",
    "## Crash-Course Overview\n",
    "\n",
    "- __Lesson 01__: Deep Learning and Computer Vision\n",
    "- __Lesson 02__: Preparing Image Data\n",
    "- __Lesson 03__: Convolutional Neural Networks\n",
    "- __Lesson 04__: Image Classification\n",
    "- __Lesson 05__: Train Image Classification Model\n",
    "- __Lesson 06__: Image Augmentation\n",
    "- __Lesson 07__: Face Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 02: Preparing Image Data\n",
    "\n",
    "Images are comprised of matrices of pixel values.\n",
    "\n",
    "Pixel values are often unsigned integers in the range between 0 and 255. Although these pixel values can be presented directly to neural network models in their raw format, this can result in challenges during modeling, such as slower than expected training of the model.\n",
    "\n",
    "Instead, there can be great benefit in preparing the image pixel values prior to modeling, such as simply scaling pixel values to the range 0-1 to centering and even standardizing the values.\n",
    "\n",
    "This is __called normalization__ and can be performed directly on a loaded image. The example below uses the PIL library (the standard image handling library in Python) to load an image and normalize its pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type: uint8\n",
      "Min: 0.000, Max: 255.000\n",
      "Min: 0.000, Max: 1.000\n"
     ]
    }
   ],
   "source": [
    "# example of pixel normalization\n",
    "from numpy import asarray\n",
    "from PIL import Image\n",
    "\n",
    "# load image\n",
    "image = Image.open('.\\\\images\\\\bondi_beach.jpg')\n",
    "#image = Image.open('.\\\\images\\\\opencv-ocr-sample.jpg')\n",
    "pixels = asarray(image)\n",
    "\n",
    "# confirm pixel range is 0-255\n",
    "print('Data Type: %s' % pixels.dtype)\n",
    "print('Min: %.3f, Max: %.3f' % (pixels.min(), pixels.max()))\n",
    "\n",
    "# convert from integers to floats\n",
    "pixels = pixels.astype('float32')\n",
    "\n",
    "# normalize to the range 0-1\n",
    "pixels /= 255.0\n",
    "\n",
    "# confirm the normalization\n",
    "print('Min: %.3f, Max: %.3f' % (pixels.min(), pixels.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 03: Convolutional Neural Networks\n",
    "\n",
    "### Convolutional Layers\n",
    "A [convolution](https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/) is the simple application of a filter to an input that results in an activation. Repeated application of the same filter to an input results in a map of activations called a feature map, indicating the locations and strength of a detected feature in an input, such as an image.\n",
    "\n",
    "### Pooling Layers\n",
    "[Pooling layers](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/) provide an approach to downsampling feature maps by summarizing the presence of features in patches of the feature map.\n",
    "\n",
    "Maximum pooling, or max pooling, is a pooling operation that calculates the maximum, or largest, value in each patch of each feature map.\n",
    "\n",
    "### Classifier Layer\n",
    "Once the features have been extracted, they can be interpreted and used to make a prediction, such as classifying the type of object in a photograph.\n",
    "\n",
    "This can be achieved by first flattening the two-dimensional feature maps, and then adding a fully connected output layer. For a binary classification problem, the output layer would have one node that would predict a value between 0 and 1 for the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 254, 254, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 127, 127, 32)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 516128)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 516129    \n",
      "=================================================================\n",
      "Total params: 516,449\n",
      "Trainable params: 516,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# cnn with single convolutional, pooling and output layer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "\n",
    "# add convolutional layer\n",
    "model.add(Conv2D(32, (3,3), input_shape=(256, 256, 1)))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below creates a convolutional neural network that expects grayscale images with the square size of 256×256 pixels, with one convolutional layer with 32 filters, each with the size of 3×3 pixels, a max pooling layer, and a binary classification output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 04: Image Classification\n",
    "\n",
    "In this lesson, you will discover how to use a pre-trained model to classify photographs of objects.\n",
    "\n",
    "Deep convolutional neural network models may take days, or even weeks, to train on very large datasets.\n",
    "\n",
    "A way to short-cut this process is to re-use the model weights from pre-trained models that were developed for standard computer vision benchmark datasets, such as the ImageNet image recognition tasks.\n",
    "\n",
    "The example below uses the VGG-16 pre-trained model to classify photographs of objects into one of 1,000 known classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of using a pre-trained model as a classifier\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doberman (33.59%)\n"
     ]
    }
   ],
   "source": [
    "# load an image from file\n",
    "image = load_img('.\\\\images\\\\dog.jpg', target_size=(224, 224))\n",
    "## Doberman (33.59%)\n",
    "\n",
    "# image = load_img('.\\\\images\\\\dog-chiguagua.jpg', target_size=(224, 224))\n",
    "## toy_terrier (82.54%)\n",
    "\n",
    "# image = load_img('.\\\\images\\\\cat.jpg', target_size=(224, 224))\n",
    "## Egyptian_cat (61.94%)\n",
    "\n",
    "# convert the image pixels to a numpy array\n",
    "image = img_to_array(image)\n",
    "\n",
    "# reshape data for the model\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\n",
    "# prepare the image for the VGG model\n",
    "image = preprocess_input(image)\n",
    "\n",
    "# load the model\n",
    "model = VGG16()\n",
    "\n",
    "# predict the probability across all output classes\n",
    "yhat = model.predict(image)\n",
    "\n",
    "# convert the probabilities to class labels\n",
    "label = decode_predictions(yhat)\n",
    "\n",
    "# retrieve the most likely result, e.g. highest probability\n",
    "label = label[0][0]\n",
    "\n",
    "# print the classification\n",
    "print('%s (%.2f%%)' % (label[1], label[2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 05: Train Image Classification Model\n",
    "\n",
    "In this lesson, you will discover how to train and evaluate a convolutional neural network for image classification.\n",
    "\n",
    "The example below loads the dataset, scales the pixel values, then fits a convolutional neural network on the training dataset and evaluates the performance of the network on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 10us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 7s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 2s 1us/step\n",
      "Epoch 1/10\n",
      "1875/1875 - 29s - loss: 0.3756 - accuracy: 0.8663\n",
      "Epoch 2/10\n",
      "1875/1875 - 28s - loss: 0.2513 - accuracy: 0.9087\n",
      "Epoch 3/10\n",
      "1875/1875 - 28s - loss: 0.2062 - accuracy: 0.9241\n",
      "Epoch 4/10\n",
      "1875/1875 - 28s - loss: 0.1751 - accuracy: 0.9346\n",
      "Epoch 5/10\n",
      "1875/1875 - 29s - loss: 0.1494 - accuracy: 0.9444\n",
      "Epoch 6/10\n",
      "1875/1875 - 28s - loss: 0.1273 - accuracy: 0.9526\n",
      "Epoch 7/10\n",
      "1875/1875 - 28s - loss: 0.1078 - accuracy: 0.9599\n",
      "Epoch 8/10\n",
      "1875/1875 - 30s - loss: 0.0927 - accuracy: 0.9658\n",
      "Epoch 9/10\n",
      "1875/1875 - 28s - loss: 0.0807 - accuracy: 0.9704\n",
      "Epoch 10/10\n",
      "1875/1875 - 29s - loss: 0.0665 - accuracy: 0.9759\n",
      "0.34654513001441956 0.9135000109672546\n"
     ]
    }
   ],
   "source": [
    "# fit a cnn on the fashion mnist dataset\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "# load dataset\n",
    "(trainX, trainY), (testX, testY) = fashion_mnist.load_data()\n",
    "\n",
    "# reshape dataset to have a single channel\n",
    "trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
    "testX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
    "\n",
    "# convert from integers to floats\n",
    "trainX, testX = trainX.astype('float32'), testX.astype('float32')\n",
    "\n",
    "# normalize to range 0-1\n",
    "trainX,testX  = trainX / 255.0, testX / 255.0\n",
    "\n",
    "# one hot encode target values\n",
    "trainY, testY = to_categorical(trainY), to_categorical(testY)\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "model.fit(trainX, trainY, epochs=10, batch_size=32, verbose=2)\n",
    "\n",
    "# evaluate model\n",
    "loss, acc = model.evaluate(testX, testY, verbose=0)\n",
    "print(loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
