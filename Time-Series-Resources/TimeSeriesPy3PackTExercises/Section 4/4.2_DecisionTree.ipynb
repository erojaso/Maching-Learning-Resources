{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A building block to Random Forest is a Decision Tree. Decision trees start with a root node and end with a leaf node. For numeric feature, tree split on each unique value of each data.  Tree-based models may poorly handle trends in data, compared to linear models, so you have to detrend your series first, which was done in the previous part for three of the five datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1:  Vacation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df1 = pd.read_csv('~/Desktop/section_4/vacation_lags_12months_features.csv', header=0)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "\n",
    "vacat = df1.values\n",
    "# split into lagged variables and original time series\n",
    "X1= vacat[:, 0:-1]  # slice all rows and start with column 0 and go up to but not including the last column\n",
    "y1 = vacat[:,-1]  # slice all rows and last column, essentially separating out 't' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns t-1 to t-12, which are the lagged variables\n",
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column t, which is the original time series\n",
    "y1[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you can alter the splits as 50-50, 60-40, 70-30, 75-25, 80-20, and 85-15, etc. So, 0.80 is a 80-20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Train-Test split\n",
    "from pandas import read_csv\n",
    "\n",
    "Y1 = y1\n",
    "traintarget_size = int(len(Y1) * 0.80)   # Set split\n",
    "train_target, test_target = Y1[0:traintarget_size], Y1[traintarget_size:len(Y1)]\n",
    "\n",
    "print('Observations for Target: %d' % (len(Y1)))\n",
    "print('Training Observations for Target: %d' % (len(train_target)))\n",
    "print('Testing Observations for Target: %d' % (len(test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features Train-Test split\n",
    "\n",
    "trainfeature_size = int(len(X1) * 0.80)\n",
    "train_feature, test_feature = X1[0:trainfeature_size], X1[trainfeature_size:len(X1)]\n",
    "print('Observations for feature: %d' % (len(X1)))\n",
    "print('Training Observations for feature: %d' % (len(train_feature)))\n",
    "print('Testing Observations for feature: %d' % (len(test_feature)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regresion Model\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create a decision tree regression model with default arguments\n",
    "decision_tree_vacat = DecisionTreeRegressor()  # max-depth not set\n",
    "\n",
    "# Fit the model to the training features and targets\n",
    "decision_tree_vacat.fit(train_feature, train_target)\n",
    "\n",
    "# Check the score on train and test\n",
    "print(decision_tree_vacat.score(train_feature, train_target))\n",
    "print(decision_tree_vacat.score(test_feature,test_target))  # predictions are horrible if negative value, no relationship if 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best Max Depth\n",
    "\n",
    "# Loop through a few different max depths and check the performance\n",
    "# Try different max depths. We want to optimize our ML models to make the best predictions possible.\n",
    "# For regular decision trees, max_depth, which is a hyperparameter, limits the number of splits in a tree.\n",
    "# You can find the best value of max_depth based on the R-squared score of the model on the test set.\n",
    "\n",
    "for d in [2, 3, 4, 5,7,8,10]:\n",
    "    # Create the tree and fit it\n",
    "    decision_tree_vacat = DecisionTreeRegressor(max_depth=d)\n",
    "    decision_tree_vacat.fit(train_feature, train_target)\n",
    "\n",
    "    # Print out the scores on train and test\n",
    "    print('max_depth=', str(d))\n",
    "    print(decision_tree_vacat.score(train_feature, train_target))\n",
    "    print(decision_tree_vacat.score(test_feature, test_target), '\\n')  # You want the test score to be positive and high\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision-tree learners can create over-complex trees that do not generalize the data well. This is called overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted against actual values\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Use the best max_depth \n",
    "decision_tree_vacat = DecisionTreeRegressor(max_depth=5)  # fill in best max depth here\n",
    "decision_tree_vacat.fit(train_feature, train_target)\n",
    "\n",
    "# Predict values for train and test\n",
    "train_prediction = decision_tree_vacat.predict(train_feature)\n",
    "test_prediction = decision_tree_vacat.predict(test_feature)\n",
    "\n",
    "# Scatter the predictions vs actual values\n",
    "plt.scatter(train_prediction, train_target, label='train')  # blue\n",
    "plt.scatter(test_prediction, test_target, label='test')  # orange\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Furniture Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, this data has been stationarized\n",
    "df2 = pd.read_csv('~/Desktop/section_4/furniture_lags_12months_features.csv', header=0)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n",
    "\n",
    "furn = df2.values\n",
    "# split into lagged variables (features) and original time series data (target)\n",
    "X2= furn[:,0:-1]  # slice all rows and start with column 0 and go up to but not including the last column\n",
    "y2 = furn[:,-1]  # slice all rows and last column, essentially separating out 't' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns t-1 to t-12, which are the lagged variables\n",
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column t, which is the original time series\n",
    "# Give first 10 values of target variable, time series\n",
    "y2[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you can alter the splits as 50-50, 60-40, 70-30, 75-25, 80-20, and 85-15, etc. Here we are using a 75-25 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Train-Test split\n",
    "from pandas import read_csv\n",
    "\n",
    "Y2 = y2\n",
    "traintarget_size = int(len(Y2) * 0.75)   # Set split\n",
    "train_target, test_target = Y2[0:traintarget_size], Y2[traintarget_size:len(Y2)]\n",
    "\n",
    "print('Observations for Target: %d' % (len(Y2)))\n",
    "print('Training Observations for Target: %d' % (len(train_target)))\n",
    "print('Testing Observations for Target: %d' % (len(test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features Train-Test split\n",
    "\n",
    "trainfeature_size = int(len(X2) * 0.75)\n",
    "train_feature, test_feature = X2[0:trainfeature_size], X2[trainfeature_size:len(X2)]\n",
    "print('Observations for feature: %d' % (len(X2)))\n",
    "print('Training Observations for feature: %d' % (len(train_feature)))\n",
    "print('Testing Observations for feature: %d' % (len(test_feature)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regression Model\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create a decision tree regression model with default arguments\n",
    "decision_tree_furn = DecisionTreeRegressor()  # max_depth not set\n",
    "\n",
    "# Fit the model to the training features and targets\n",
    "decision_tree_furn.fit(train_feature, train_target)\n",
    "\n",
    "# Check the score on train and test\n",
    "print(decision_tree_furn.score(train_feature, train_target))\n",
    "print(decision_tree_furn.score(test_feature,test_target))  # predictions are horrible if negative value, no relationship if 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Best Max Depth\n",
    "\n",
    "# Loop through a few different max depths and check the performance\n",
    "# Try different max depths. We want to optimize our ML models to make the best predictions possible.\n",
    "# For regular decision trees, max_depth, which is a hyperparameter, limits the number of splits in a tree.\n",
    "# You can find the best value of max_depth based on the R-squared score of the model on the test set.\n",
    "\n",
    "for d in [2, 3,4, 5,7,8,10]:\n",
    "    # Create the tree and fit it\n",
    "    decision_tree_furn = DecisionTreeRegressor(max_depth=d)\n",
    "    decision_tree_furn.fit(train_feature, train_target)\n",
    "\n",
    "    # Print out the scores on train and test\n",
    "    print('max_depth=', str(d))\n",
    "    print(decision_tree_furn.score(train_feature, train_target))\n",
    "    print(decision_tree_furn.score(test_feature, test_target), '\\n')  # You want the test score to be positive\n",
    "    \n",
    "# R-square for train and test scores are below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best max_depth is max_depth is the one that gives the best test score (positive and high).  Decision-tree learners can create over-complex trees that do not generalize the data well. This is called overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted against actual values\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Use the best max_depth \n",
    "decision_tree_furn = DecisionTreeRegressor(max_depth=5) # Fill in best max depth score here\n",
    "decision_tree_furn.fit(train_feature, train_target)\n",
    "\n",
    "# Predict values for train and test\n",
    "train_prediction = decision_tree_furn.predict(train_feature)\n",
    "test_prediction = decision_tree_furn.predict(test_feature)\n",
    "\n",
    "# Scatter the predictions vs actual values, orange is predicted\n",
    "plt.scatter(train_prediction, train_target, label='train')  # blue \n",
    "plt.scatter(test_prediction, test_target, label='test')  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3:  Bank of America Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, this data has been stationarized\n",
    "df3 = pd.read_csv('~/Desktop/section_4/bac_lags_12months_features.csv', header=0)\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n",
    "\n",
    "bac = df3.values\n",
    "# split into lagged variables (features) and original time series data (target)\n",
    "X3= bac[:,0:-1]  # slice all rows and start with column 0 and go up to but not including the last column\n",
    "y3 = bac[:,-1]  # slice all rows and last column, essentially separating out 't' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns t-1 to t-12, which are the lagged variables\n",
    "X3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column t, which is the original time series\n",
    "# Give first 10 values of target variable, time series\n",
    "y3[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Train-Test split\n",
    "from pandas import read_csv\n",
    "\n",
    "Y3 = y3\n",
    "traintarget_size = int(len(Y3) * 0.75)   # Set split\n",
    "train_target, test_target = Y3[0:traintarget_size], Y3[traintarget_size:len(Y3)]\n",
    "\n",
    "print('Observations for Target: %d' % (len(Y3)))\n",
    "print('Training Observations for Target: %d' % (len(train_target)))\n",
    "print('Testing Observations for Target: %d' % (len(test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features Train-Test split\n",
    "\n",
    "trainfeature_size = int(len(X3) * 0.75)\n",
    "train_feature, test_feature = X3[0:trainfeature_size], X3[trainfeature_size:len(X3)]\n",
    "print('Observations for feature: %d' % (len(X3)))\n",
    "print('Training Observations for feature: %d' % (len(train_feature)))\n",
    "print('Testing Observations for feature: %d' % (len(test_feature)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regression Model\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create a decision tree regression model with default arguments\n",
    "decision_tree_bac = DecisionTreeRegressor()  # max_depth not set\n",
    "\n",
    "# Fit the model to the training features and targets\n",
    "decision_tree_bac.fit(train_feature, train_target)\n",
    "\n",
    "# Check the score on train and test\n",
    "print(decision_tree_bac.score(train_feature, train_target))\n",
    "print(decision_tree_bac.score(test_feature,test_target))  # predictions are horrible if negative value, no relationship if 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Best Max Depth\n",
    "\n",
    "# Loop through a few different max depths and check the performance\n",
    "# Try different max depths. We want to optimize our ML models to make the best predictions possible.\n",
    "# For regular decision trees, max_depth, which is a hyperparameter, limits the number of splits in a tree.\n",
    "# You can find the best value of max_depth based on the R-squared score of the model on the test set.\n",
    "\n",
    "for d in [2, 3,4, 5,7,8,10]:\n",
    "    # Create the tree and fit it\n",
    "    decision_tree_bac = DecisionTreeRegressor(max_depth=d)\n",
    "    decision_tree_bac.fit(train_feature, train_target)\n",
    "\n",
    "    # Print out the scores on train and test\n",
    "    print('max_depth=', str(d))\n",
    "    print(decision_tree_bac.score(train_feature, train_target))\n",
    "    print(decision_tree_bac.score(test_feature, test_target), '\\n')  # You want the test score to be positive\n",
    "    \n",
    "# R-square for train and test scores are below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted against actual values\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Use the best max_depth \n",
    "decision_tree_bac = DecisionTreeRegressor(max_depth=4) \n",
    "decision_tree_bac.fit(train_feature, train_target)\n",
    "\n",
    "# Predict values for train and test\n",
    "train_prediction = decision_tree_bac.predict(train_feature)\n",
    "test_prediction = decision_tree_bac.predict(test_feature)\n",
    "\n",
    "# Scatter the predictions vs actual values\n",
    "plt.scatter(train_prediction, train_target, label='train')\n",
    "plt.scatter(test_prediction, test_target, label='test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4:  J.P. Morgan Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df4 = pd.read_csv('~/Desktop/section_4/jpm_lags_12months_features.csv', header=0)\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "\n",
    "jpm = df4.values\n",
    "# split into lagged variables and original time series\n",
    "X4= jpm[:, 0:-1]  # slice all rows and start with column 0 and go up to but not including the last column\n",
    "y4 = jpm[:,-1]  # slice all rows and last column, essentially separating out 't' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns t-1 to t-12, which are the lagged variables\n",
    "X4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column t, which is the original time series\n",
    "y4[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Train-Test split\n",
    "from pandas import read_csv\n",
    "\n",
    "Y4 = y4\n",
    "traintarget_size = int(len(Y4) * 0.50)   # Set split\n",
    "train_target, test_target = Y4[0:traintarget_size], Y4[traintarget_size:len(Y4)]\n",
    "\n",
    "print('Observations for Target: %d' % (len(Y4)))\n",
    "print('Training Observations for Target: %d' % (len(train_target)))\n",
    "print('Testing Observations for Target: %d' % (len(test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features Train-Test split\n",
    "\n",
    "trainfeature_size = int(len(X4) * 0.50)\n",
    "train_feature, test_feature = X4[0:trainfeature_size], X4[trainfeature_size:len(X4)]\n",
    "print('Observations for feature: %d' % (len(X4)))\n",
    "print('Training Observations for feature: %d' % (len(train_feature)))\n",
    "print('Testing Observations for feature: %d' % (len(test_feature)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regression Model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create a decision tree regression model with default arguments\n",
    "decision_tree_jpm = DecisionTreeRegressor()  # max-depth not set\n",
    "\n",
    "# Fit the model to the training features and targets\n",
    "decision_tree_jpm.fit(train_feature, train_target)\n",
    "\n",
    "# Check the score on train and test\n",
    "print(decision_tree_jpm.score(train_feature, train_target))\n",
    "print(decision_tree_jpm.score(test_feature,test_target))  # predictions are horrible if negative value, no relationship if 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Best max depth\n",
    "\n",
    "# Loop through a few different max depths and check the performance\n",
    "# Try different max depths. We want to optimize our ML models to make the best predictions possible.\n",
    "# For regular decision trees, max_depth, which is a hyperparameter, limits the number of splits in a tree.\n",
    "# You can find the best value of max_depth based on the R-squared score of the model on the test set.\n",
    "\n",
    "for d in [2, 3,4, 5,7,8,10]:\n",
    "    # Create the tree and fit it\n",
    "    decision_tree_jpm = DecisionTreeRegressor(max_depth=d)\n",
    "    decision_tree_jpm.fit(train_feature, train_target)\n",
    "\n",
    "    # Print out the scores on train and test\n",
    "    print('max_depth=', str(d))\n",
    "    print(decision_tree_jpm.score(train_feature, train_target))\n",
    "    print(decision_tree_jpm.score(test_feature, test_target), '\\n')  \n",
    "    # You want the test score to be positive and high\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted against actual values\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Use the best max_depth \n",
    "decision_tree_jpm = DecisionTreeRegressor(max_depth=4)\n",
    "decision_tree_jpm.fit(train_feature, train_target)\n",
    "\n",
    "# Predict values for train and test\n",
    "train_prediction = decision_tree_jpm.predict(train_feature)\n",
    "test_prediction = decision_tree_jpm.predict(test_feature)\n",
    "\n",
    "# Scatter the predictions vs actual values\n",
    "plt.scatter(train_prediction, train_target, label='train')\n",
    "plt.scatter(test_prediction, test_target, label='test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5:  Average Temperature Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df5 = pd.read_csv('~/Desktop/section_4/temp_lags_12months_features.csv', header=0)\n",
    "df5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df5.values\n",
    "# split into lagged variables and original time series\n",
    "X5= temp[:,0:-1]  # slice all rows and start with column 0 and go up to but not including the last column\n",
    "y5 = temp[:,-1]  # slice all rows and last column, essentially separating out 't' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns t-1 to t-12, which are the lagged variables\n",
    "X5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column t, which is the original time series\n",
    "y5[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Train-Test split\n",
    "from pandas import read_csv\n",
    "\n",
    "Y5 = y5\n",
    "traintarget_size = int(len(Y5) * 0.80)   # Set split\n",
    "train_target, test_target = Y5[0:traintarget_size], Y5[traintarget_size:len(Y5)]\n",
    "\n",
    "print('Observations for Target: %d' % (len(Y5)))\n",
    "print('Training Observations for Target: %d' % (len(train_target)))\n",
    "print('Testing Observations for Target: %d' % (len(test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features Train-Test split\n",
    "\n",
    "trainfeature_size = int(len(X5) * 0.80)\n",
    "train_feature, test_feature = X5[0:trainfeature_size], X5[trainfeature_size:len(X5)]\n",
    "print('Observations for feature: %d' % (len(X5)))\n",
    "print('Training Observations for feature: %d' % (len(train_feature)))\n",
    "print('Testing Observations for feature: %d' % (len(test_feature)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regression Model\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create a decision tree regression model with default arguments\n",
    "decision_tree_temp = DecisionTreeRegressor()  # max-depth not set\n",
    "\n",
    "# Fit the model to the training features and targets\n",
    "decision_tree_temp.fit(train_feature, train_target)\n",
    "\n",
    "# Check the score on train and test\n",
    "print(decision_tree_temp.score(train_feature, train_target))\n",
    "print(decision_tree_temp.score(test_feature,test_target))  # predictions are horrible if negative value, no relationship if 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best max depth\n",
    "\n",
    "# Loop through a few different max depths and check the performance\n",
    "# Try different max depths. We want to optimize our ML models to make the best predictions possible.\n",
    "# For regular decision trees, max_depth, which is a hyperparameter, limits the number of splits in a tree.\n",
    "# You can find the best value of max_depth based on the R-squared score of the model on the test set.\n",
    "\n",
    "for d in [2, 3, 4, 5,7,8,10]:\n",
    "    # Create the tree and fit it\n",
    "    decision_tree_temp = DecisionTreeRegressor(max_depth=d)\n",
    "    decision_tree_temp.fit(train_feature, train_target)\n",
    "\n",
    "    # Print out the scores on train and test\n",
    "    print('max_depth=', str(d))\n",
    "    print(decision_tree_temp.score(train_feature, train_target))\n",
    "    print(decision_tree_temp.score(test_feature, test_target), '\\n')  # You want the test score to be positive and high\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Use the best max_depth \n",
    "decision_tree_temp = DecisionTreeRegressor(max_depth=4)\n",
    "decision_tree_temp.fit(train_feature, train_target)\n",
    "\n",
    "# Predict values for train and test\n",
    "train_prediction = decision_tree_temp.predict(train_feature)\n",
    "test_prediction = decision_tree_temp.predict(test_feature)\n",
    "\n",
    "# Scatter the predictions vs actual values\n",
    "plt.scatter(train_prediction, train_target, label='train')\n",
    "plt.scatter(test_prediction, test_target, label='test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we looked at a decision tree model on both datasets that consisted of 12 lagged variables.  The datasets were split into features and target. It was further split according to a training and test datasets. We determined that the best max_depth based on the best R-squared score on the test dataset model, which was max_depth with the highest and positive score. The max_depth determines the number of splits and is, essentially, the height of the tree. The scatterplot displays both the predicted and actual values. The decision tree models predicted training data better than test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
